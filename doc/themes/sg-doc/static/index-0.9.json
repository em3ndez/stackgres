[
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/prerequisites/object-storage/aws-s3/",
	"title": "AWS S3",
	"tags": [],
	"description": "",
	"content": "First let\u0026rsquo;s create the IAM policy that would allow the appropriate level of access to the S3 bucket:\nexport S3_BACKUP_BUCKET=YOUR_BUCKET_NAME read -d \u0026#39;\u0026#39; policy \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${S3_BACKUP_BUCKET}\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${S3_BACKUP_BUCKET}/*\u0026#34; ] } ] } EOF Let\u0026rsquo;s then create an IAM user and attach the above policy:\nexport AWS_PROFILE= # optional export AWS_REGION=us-east-2 export S3_BACKUP_BUCKET_USER=stackgres-s3-user aws iam create-user --region $AWS_REGION --user-name $S3_BACKUP_BUCKET_USER aws iam put-user-policy --region $AWS_REGION --user-name $S3_BACKUP_BUCKET_USER \\ \t--policy-name ${S3_BACKUP_BUCKET_USER}-policy --policy-document $policy Then let\u0026rsquo;s create an access key, the credentials that will be used to access this bucket. The following command will output them, consider redirecting the command below to a file or non-printable command if working on a non-private environment:\naws --output json iam create-access-key --region $AWS_REGION --user-name $S3_BACKUP_BUCKET_USER Finally, create the bucket:\naws s3 mb s3://$S3_BACKUP_BUCKET --region $AWS_REGION "
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/complete-cluster/backup-configuration/aws-s3/",
	"title": "AWS S3",
	"tags": [],
	"description": "",
	"content": "To proceed, a Kubernetes Secret with the folling shape needs to be created:\napiVersion: v1 kind: Secret metadata: name: aws-creds-secret type: Opaque data: accessKeyId: ${accessKey} secretAccessKey: ${secretKey} If you have created the S3 bucket following the instructions in the AWS S3 Prerequisites, you should already have the generated bucket credentials stored on a local file in JSON format. In this case, we can even script the creation of the above secret:\nexport CLUSTER_NAMESPACE=demo export CREDENTIALS_FILE= # your credentials file accessKeyId=$(jq -r \u0026#39;.AccessKey.AccessKeyId\u0026#39; \u0026#34;$CREDENTIALS_FILE\u0026#34;) secretAccessKey=$(jq -r \u0026#39;.AccessKey.SecretAccessKey\u0026#39; \u0026#34;$CREDENTIALS_FILE\u0026#34;) kubectl --namespace $CLUSTER_NAMESPACE create secret generic s3-backup-bucket-secret \\  --from-literal=\u0026#34;accessKeyId=$accessKeyId\u0026#34; \\  --from-literal=\u0026#34;secretAccessKey=$secretAccessKey\u0026#34; Having the credentials secret created, we just need to create now a backup configuration. It is governed by the CRD SGBackupConfig. This CRD allows to specify, among others, the retention window for the automated backups, when base backups are performed, performance parameters of the backup process, the object storage technology and parameters required and a reference to the above secret.\nCreate the file sgbackupconfig-backupconfig1.yaml:\napiVersion: stackgres.io/v1 kind: SGBackupConfig metadata: namespace: demo name: backupconfig1 spec: baseBackups: cronSchedule: \u0026#39;*/5 * * * *\u0026#39; retention: 6 storage: type: \u0026#39;s3\u0026#39; s3: bucket: \u0026#39;YOUR_BUCKET_NAME\u0026#39; awsCredentials: secretKeySelectors: accessKeyId: {name: \u0026#39;s3-backup-bucket-secret\u0026#39;, key: \u0026#39;accessKeyId\u0026#39;} secretAccessKey: {name: \u0026#39;s3-backup-bucket-secret\u0026#39;, key: \u0026#39;secretAccessKey\u0026#39;} and deploy to Kubernetes:\nkubectl apply -f sgbackupconfig-backupconfig1.yaml Note that for this tutorial and demo purposes, backups are created every 5 minutes. Modify the .spec.baseBackups.cronSchedule parameter above to adjust to your own needs.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/install/prerequisites/backups/",
	"title": "Backups",
	"tags": [],
	"description": "Details about how to setup and configure the backups",
	"content": "All the configuration for this matter can be found at Backup Configuration documentation. By default, backups are scheduled daily (config.backup.fullSchedule) at 05:00 UTC and with a retention policy (config.backup.retention) of 5 full-backups removed on rotation. You will have to find out the correct time window and retention policy that fit your needs.\nIn the next section, you\u0026rsquo;ll be able to see how to done this via Helm, with more explicit examples.\nStorage StackGres support Backups with the following storage options:\n AWS S3 Google CLoud Storage Azure Blob Storage   Examples are using MinIO service as a S3 compatible service for quick setups on local Kubernetes Cluster. Although, for production setups, StackGres Team recommends emphatically to pick a Storage as a Service for this purpose.\n All the related configuration for the storage, is under configurations.backupconfig.storage section in your Stackgres Cluster configuration file.\nconfigurations: backupconfig: # fill the preferred storage method with # specific credentials and configurations storage: s3: {} s3Compatible: {} gcs: {} azureBlob: {} To extend the CRD for the backups, all the reference can be found at CRD Reference Documentation.\nRestore StackGres can perform a database restoration from a StackGres backup by just setting the UID of the backup CR that represents the backup that we want to restore. Like this:\ncluster: initialData: restore: fromBackup: #the backup UID to restore "
},
{
	"uri": "https://stackgres.io/doc/0.9/install/prerequisites/storage/",
	"title": "Data Storage",
	"tags": [],
	"description": "Details about how to setup and configure the storage classes. Storage classes are used by the database clusters and will impact performance and availability of the cluster.",
	"content": "When setting up a K8s environment the Storage Class by default is created with one main restriction and this is represented with the parameter allowVolumeExpansion: false this will not allow you to expand your disk when these are filling up. It is recommended to create a new Storage Class with at least these next parameters:\n reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true  Here is an example working in a AWS environment:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: io1 provisioner: kubernetes.io/aws-ebs parameters: type: io1 iopsPerGB: \u0026quot;50\u0026quot; reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true and if you\u0026rsquo;re using GKE:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ssd provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true Check the Storage Class documentation for more details and other providers.\nDo not forget using your custom Storage Class when you create a cluster, check the required parameters in Cluster Parameters\nImportant note: Make sure you include these parameters in order to avoid some of the next errors:\n  Autoscaler not working as expected: cluster-autoscaler pod didn't trigger scale-up (it wouldn't fit if a new node is added)\n  Volumes not assigned: N node(s) had no available volume zone\n  Losing data by accidentally removing a volume: reclaimPolicy: Retain will guarantee the volume is not delete when a claim no longer exist.\n  "
},
{
	"uri": "https://stackgres.io/doc/0.9/demo/setenv/eks/",
	"title": "EKS",
	"tags": [],
	"description": "",
	"content": "This section will illustrate how to create an AWS Elastic Kubernetes Service. Assuming you have already installed the aws CLI and the eksctl CLI you can proceed by creating the kubernetes cluster with following characteristics (that you may change):\n Cluster name: stackgres Kubernetes version: 1.13 Zone: us-west-2 Machine type: m5.large Number of nodes: 3 Disk size 20GB  eksctl create cluster --name stackgres \\  --region us-west-2 \\  --node-type m5.large \\  --node-volume-size 20 \\  --nodes 3 \\  --version 1.13 [ℹ] eksctl version 0.13.0 [ℹ] using region us-west-2 [ℹ] setting availability zones to [us-west-2a us-west-2c us-west-2b] [ℹ] subnets for us-west-2a - public:192.168.0.0/19 private:192.168.96.0/19 [ℹ] subnets for us-west-2c - public:192.168.32.0/19 private:192.168.128.0/19 [ℹ] subnets for us-west-2b - public:192.168.64.0/19 private:192.168.160.0/19 [ℹ] nodegroup \u0026#34;ng-308f6134\u0026#34; will use \u0026#34;ami-09bcf0b1f5b446c5d\u0026#34; [AmazonLinux2/1.13] [ℹ] using Kubernetes version 1.13 [ℹ] creating EKS cluster \u0026#34;stackgres\u0026#34; in \u0026#34;us-west-2\u0026#34; region with un-managed nodes [ℹ] will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup [ℹ] if you encounter any issues, check CloudFormation console or try \u0026#39;eksctl utils describe-stacks --region=us-west-2 --cluster=stackgres\u0026#39; [ℹ] CloudWatch logging will not be enabled for cluster \u0026#34;stackgres\u0026#34; in \u0026#34;us-west-2\u0026#34; [ℹ] you can enable it with \u0026#39;eksctl utils update-cluster-logging --region=us-west-2 --cluster=stackgres\u0026#39; [ℹ] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \u0026#34;stackgres\u0026#34; in \u0026#34;us-west-2\u0026#34; [ℹ] 2 sequential tasks: { create cluster control plane \u0026#34;stackgres\u0026#34;, create nodegroup \u0026#34;ng-308f6134\u0026#34; } [ℹ] building cluster stack \u0026#34;eksctl-stackgres-cluster\u0026#34; [ℹ] deploying stack \u0026#34;eksctl-stackgres-cluster\u0026#34; [ℹ] building nodegroup stack \u0026#34;eksctl-stackgres-nodegroup-ng-308f6134\u0026#34; [ℹ] --nodes-min=3 was set automatically for nodegroup ng-308f6134 [ℹ] --nodes-max=3 was set automatically for nodegroup ng-308f6134 [ℹ] deploying stack \u0026#34;eksctl-stackgres-nodegroup-ng-308f6134\u0026#34; [✔] all EKS cluster resources for \u0026#34;stackgres\u0026#34; have been created [✔] saved kubeconfig as \u0026#34;/home/matteom/.kube/config-aws\u0026#34; [ℹ] adding identity \u0026#34;arn:aws:iam::661392101474:role/eksctl-stackgres-nodegroup-ng-NodeInstanceRole-C8R84QGP5UYX\u0026#34; to auth ConfigMap [ℹ] nodegroup \u0026#34;ng-308f6134\u0026#34; has 1 node(s) [ℹ] node \u0026#34;ip-192-168-66-45.us-west-2.compute.internal\u0026#34; is not ready [ℹ] waiting for at least 3 node(s) to become ready in \u0026#34;ng-308f6134\u0026#34; [ℹ] nodegroup \u0026#34;ng-308f6134\u0026#34; has 3 node(s) [ℹ] node \u0026#34;ip-192-168-2-185.us-west-2.compute.internal\u0026#34; is ready [ℹ] node \u0026#34;ip-192-168-58-166.us-west-2.compute.internal\u0026#34; is ready [ℹ] node \u0026#34;ip-192-168-66-45.us-west-2.compute.internal\u0026#34; is ready [ℹ] kubectl command should work with \u0026#34;/home/matteom/.kube/config-aws\u0026#34;, try \u0026#39;kubectl --kubeconfig=/home/matteom/.kube/config-aws get nodes\u0026#39; [✔] EKS cluster \u0026#34;stackgres\u0026#34; in \u0026#34;us-west-2\u0026#34; region is ready To cleanup the kubernetes cluster you may issue following command:\neksctl delete cluster --name stackgres \\  --region us-west-2 \\  --wait You may also want to cleanup EBS used by persistence volumes that may have been created:\naws ec2 describe-volumes --region us-west-2 --filters Name=tag-key,Values=kubernetes.io/cluster/stackgres \\  | jq -r \u0026#39;.Volumes[].VolumeId\u0026#39; | xargs -r -n 1 -I % aws ec2 delete-volume --region us-west-2 --volume-id % "
},
{
	"uri": "https://stackgres.io/doc/0.9/monitoring/metrics/envoy/",
	"title": "Envoy",
	"tags": [],
	"description": "Contains details about the metrics collected by the envoy proxy with the Postgres filter.",
	"content": "The list below contains details about the metrics enabled by the envoy proxy:\n   item metric group metric type description     1 envoy_postgres_ingress        errors Counter Number of times the server replied with ERROR message     errors_error Counter Number of times the server replied with ERROR message with ERROR severity     errors_fatal Counter Number of times the server replied with ERROR message with FATAL severity     errors_panic Counter Number of times the server replied with ERROR message with PANIC severity     errors_unknown Counter Number of times the server replied with ERROR message but the decoder could not parse it     messages Counter Total number of messages processed by the filter     messages_backend Counter Total number of backend messages detected by the filter     messages_frontend Counter Number of frontend messages detected by the filter     messages_unknown Counter Number of times the filter successfully decoded a message but did not know what to do with it     sessions Counter Total number of successful logins     sessions_encrypted Counter Number of times the filter detected encrypted sessions     sessions_unencrypted Counter Number of messages indicating unencrypted successful login     statements Counter Total number of SQL statements     statements_delete Counter Number of DELETE statements     statements_insert Counter Number of INSERT statements     statements_select Counter Number of SELECT statements     statements_update Counter Number of UPDATE statements     statements_other Counter Number of statements other than DELETE, INSERT, SELECT or UPDATE     statements_parsed Counter Number of SQL queries parsed successfully     statements_parse_error Counter Number of SQL queries not parsed successfully     transactions Counter Total number of SQL transactions     transactions_commit Counter Number of COMMIT transactions     transactions_rollback Counter Number of ROLLBACK transactions     notices Counter Total number of NOTICE messages     notices_notice Counter Number of NOTICE messages with NOTICE subtype     notices_log Counter Number of NOTICE messages with LOG subtype     notices_warning Counter Number ofr NOTICE messags with WARNING severity     notices_debug Counter Number of NOTICE messages with DEBUG severity     notices_info Counter Number of NOTICE messages with INFO severity     notices_unknown Counter Number of NOTICE messages which could not be recognized    Check the filter documentation page for more details.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/api/responses/error/",
	"title": "Error Responses",
	"tags": [],
	"description": "",
	"content": "The operator error responses follows the RFC 7807.\nThat means that all of error messages follows the following structure:\n{ \u0026#34;type\u0026#34;: \u0026#34;https://StackGres.io/doc/\u0026lt;operator-version\u0026gt;/07-operator-api/01-error-types/#\u0026lt;error-type\u0026gt;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;The title of the error message\u0026#34;, \u0026#34;detail\u0026#34;: \u0026#34;A human readable description of what is the problem\u0026#34;, \u0026#34;field\u0026#34;: \u0026#34;If applicable the field that is causing the issue\u0026#34; } Error types    Type Summary     postgres-blacklist The postgres configuration that is trying to be created or update contains blacklisted parameters   postgres-major-version-mismatch The postgres configuration that you are using is targeted to a different major version that the one that your cluster has.   invalid-configuration-reference The StackGres cluster you are trying to create or update holds a reference to a custom resource that don\u0026rsquo;t exists   default-configuration An attempt to update or delete a default configuration has been detected   forbidden-configuration-deletion You are attempting to delete a custom resource that cluster depends on it   forbidden-configuration-update You are attempting to update a custom resource that cluster depends on it   forbidden-cluster-update You are trying to update a cluster property that should not be updated   invalid-storage-class You are trying to create a cluster using a storage class that doesn\u0026rsquo;t exists   constraint-violation One of the properties of the CR that you are creating or updating violates its syntactic rules.   forbidden-authorization You don\u0026rsquo;t have the permisions to access the Kubernetes resource based on the RBAC rules.    Postgres Blacklist Some postgres configuration properties are managed automatically by StackGres, therefore you cannot include them.\nThe blacklisted configuration properties are:\n   Parameters     listen_addresses   port   cluster_name   hot_standby   fsync   full_page_writes   log_destination   logging_collector   max_replication_slots   max_wal_senders   wal_keep_segments   wal_level   wal_log_hints   archive_mode   archive_command    Invalid Configuration Reference This error means that you are trying to create or update a StackGres cluster using a reference to a custom resource that doesn\u0026rsquo;t exists in the same namespace.\nFor example:\nSupose that we are trying to create a StackGres cluster with the following json.\n{ \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;StackGres\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;instances\u0026#34;: 1, \u0026#34;postgresVersion\u0026#34;: \u0026#34;11.6\u0026#34;, \u0026#34;pods\u0026#34;: { \u0026#34;persistentVolume\u0026#34;: { \u0026#34;size\u0026#34;: \u0026#34;5Gi\u0026#34;, } }, \u0026#34;configurations\u0026#34;: { \u0026#34;sgPostgresConfig\u0026#34;: \u0026#34;postgresconf\u0026#34; } } } In order to create the cluster successfully, a postgres configuration with the name \u0026ldquo;postgresconf\u0026rdquo; must exists in the same namespace of the cluster that is being created.\nThe same principle applies for the properties: sgPoolingConfig, sgInstanceProfile, sgBackupConfig.\nDefault configuration When the operator is first installed a set of default configurations objects that are created in the namespace in which the operator is installed.\nIf you try to update or delete any of those configuraions, you will get this error.\nForbidden Configuration Deletion A StackGres cluster configuration is composed in several configuration objects. When you create a postgres, connection pooling, resource profile or backup configuration, you can delete them if you loke to, until you create a cluster that references one of these objets.\nOnce a StackGres cluster references any of the above mentioned objects those become protected against deletion.\nSuppose that you send a the following request:\nuri: /stackgres/pgconfig method: POST payload: { \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;postgresconf\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;postgresVersion\u0026#34;: \u0026#34;12\u0026#34;, \u0026#34;postgresql.conf\u0026#34;: \u0026#34;password_encryption: \u0026#39;scram-sha-256\u0026#39;\\nrandom_page_cost: \u0026#39;1.5\u0026#39;\u0026#34; } } This will create a postgres configuration object with the name postgresconf. At this point, you can delete the created object without any issue.\nNonetheless, if you send the request to the path /stackgres/cluster:\nuri: /stackgres/cluster method: POST payload: { \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;StackGres\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;instances\u0026#34;: 1, \u0026#34;postgresVersion\u0026#34;: \u0026#34;12.1\u0026#34;, \u0026#34;pods\u0026#34;: { \u0026#34;persistentVolume\u0026#34;: { \u0026#34;size\u0026#34;: \u0026#34;5Gi\u0026#34;, } }, \u0026#34;sgPostgresConfig\u0026#34;: \u0026#34;postgresconf\u0026#34; } } The postgresconf object becomes protected against deletion, and if you try to delete it you will get an error of this type to prevent deletion of postgresql configuration used by an existing cluster.\nForbidden Configuration Update Whole or parts of some objects cannot be updated. This is because changing it would require some particular handling that is not possible or not supported at this time.\nIn future versions we expect to do these types of operation automatically, and planned. But, since we are not there yet, must of out configuration object cannot be updated.\nForbidden Cluster Update After a StackGres cluster is created some of it\u0026rsquo;s properties cannot be updated.\nThese properties are:\n postgresVersion size configurations storageClass pods restore  If you try to update any of these properties, you will receive a error of this type.\nInvalid Storage Class If you specify a storage class in the cluster creation, that storage have to be already configured.\nIf it doesn\u0026rsquo;t you will get an error.\nConstraint Violations All fields of all StackGres objects have some limitations regarding of the value type, maximum, minimum values, etc. All these of limitations are described in the documentation of each object.\nAny violation of these limitations will trigger an error of these.\nThe details of the error should indicate which configuration limitation are you violating.\nPostgres Major Version Mismatch When you create a StackGres cluster you have to specify the postgres version do you want to use. Also you can specify which postgres configuration do you want to use.\nPostgres configurations are targeted to a specific postgres major version. Therefore in order to use a postgres configuration, cluster\u0026rsquo;s postgres version and the postgres configuration\u0026rsquo;s target version should match.\nSuppose that create a postgres configuration with the following request:\nuri: /stackgres/pgconfig method: POST payload: { \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;postgresconf\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;postgresVersion\u0026#34;: \u0026#34;12\u0026#34;, \u0026#34;postgresql.conf\u0026#34;: \u0026#34;password_encryption: \u0026#39;scram-sha-256\u0026#39;\\nrandom_page_cost: \u0026#39;1.5\u0026#39;\u0026#34; } } Notice that the postgresVersion property says \u0026ldquo;12\u0026rdquo;. This means that this configuration is targeted for postgresql versions 12.x.\nIn order to use that postgres configuration, your StackGres cluster should have postgres version 12, like the following:\n{ \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;StackGres\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;instances\u0026#34;: 1, \u0026#34;postgresVersion\u0026#34;: \u0026#34;12.1\u0026#34;, \u0026#34;pods\u0026#34;: { \u0026#34;persistentVolume\u0026#34;: { \u0026#34;size\u0026#34;: \u0026#34;5Gi\u0026#34;, } }, \u0026#34;sgPostgresConfig\u0026#34;: \u0026#34;postgresconf\u0026#34; } } Notice that the cluster postgresVersion says 12.1. Therefore, you will be able to install a cluster like the above.\nAlso if instead of using the above payload, you try to create a cluster with the following request (notice the postgresVersion change):\nuri: /stackgres/cluster method: POST payload: { \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;StackGres\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;instances\u0026#34;: 1, \u0026#34;postgresVersion\u0026#34;: \u0026#34;12.1\u0026#34;, \u0026#34;pods\u0026#34;: { \u0026#34;persistentVolume\u0026#34;: { \u0026#34;size\u0026#34;: \u0026#34;5Gi\u0026#34;, } }, \u0026#34;sgPostgresConfig\u0026#34;: \u0026#34;postgresconf\u0026#34; } } You will receive an error. This is because the cluster wants to use postgres 11 and your the postgres configuration is targeted for postgres 12.\nForbidden Authorization Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization.\nThe REST API uses the RBAC Authorization from Kubernetes, so you should define correctly the subject User in the RoleBinding or ClusterRoleBinding with the correct set of permisions in a Role or ClusterRole\nThis error means that you either don\u0026rsquo;t have access to the corresponding resource or that your permisions are not set correctly.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/demo/setenv/gke/",
	"title": "GKE",
	"tags": [],
	"description": "",
	"content": "To create a Google Kubernetes Engine you have to do so in a Google Cloud Project. Assuming you have already do so and that you have installed the gcloud CLI you can proceed by creating the kubernetes cluster with following characteristics (that you may change):\n Project: my-project Cluster name: stackgres GKE version: 1.13.11-gke.23 Zone: us-west1 Node locations: us-west1-a,us-west1-b,us-west1-c Machine type: n1-standard-1 Number of nodes: 3 Disk size 20GB Node auto upgrade disabled  gcloud -q beta container \\ --project my-project \\ clusters create stackgres \\ --cluster-version 1.13.11-gke.23 \\ --region us-west1 \\ --node-locations us-west1-a,us-west1-b,us-west1-c \\ --machine-type n1-standard-1 \\ --disk-size \u0026quot;20\u0026quot; \\ --num-nodes 3 \\ --no-enable-autoupgrade WARNING: Currently VPC-native is not the default mode during cluster creation. In the future, this will become the default mode and can be disabled using `--no-enable-ip-alias` flag. Use `--[no-]enable-ip-alias` flag to suppress this warning. WARNING: Starting in 1.12, default node pools in new clusters will have their legacy Compute Engine instance metadata endpoints disabled by default. To create a cluster with legacy instance metadata endpoints disabled in the default node pool, run `clusters create` with the flag `--metadata disable-legacy-endpoints=true`. WARNING: Your Pod address range (`--cluster-ipv4-cidr`) can accommodate at most 1008 node(s). This will enable the autorepair feature for nodes. Please see https://cloud.google.com/kubernetes-engine/docs/node-auto-repair for more information on node autorepairs. Creating cluster stackgres in us-west1... Cluster is being health-checked...done. Created [https://container.googleapis.com/v1beta1/projects/my-project/zones/us-west1/clusters/stackgres]. To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-west1/stackgres?project=my-project kubeconfig entry generated for stackgres. NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS stackgres us-west1 1.13.11-gke.23 35.233.239.208 n1-standard-1 1.13.11-gke.23 9 RUNNING To cleanup the kubernetes cluster you may issue following command:\ngcloud -q beta container \\ --project my-project \\ clusters delete stackgres \\ --region us-west1 \\ You may also want to cleanup compute disks used by persistence volumes that may have been created:\ngcloud -q compute disks list --project my-project --filter \u0026#34;zone:us-west1\u0026#34; | tail -n+2 | sed \u0026#39;s/ \\+/|/g\u0026#39; | cut -d \u0026#39;|\u0026#39; -f 1-2 \\  | grep \u0026#39;^gke-stackgres-[0-9a-f]\\{4\\}-pvc-[0-9a-f]\\{8\\}-[0-9a-f]\\{4\\}-[0-9a-f]\\{4\\}-[0-9a-f]\\{4\\}-[0-9a-f]\\{12\\}|\u0026#39; \\  | xargs -r -n 1 -I % sh -ec \u0026#34;gcloud -q compute disks delete --project my-project --zone \\\u0026#34;\\$(echo \u0026#39;%\u0026#39; | cut -d \u0026#39;|\u0026#39; -f 2)\\\u0026#34; \\\u0026#34;\\$(echo \u0026#39;%\u0026#39; | cut -d \u0026#39;|\u0026#39; -f 1)\\\u0026#34;\u0026#34; "
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/complete-cluster/instance-profile/",
	"title": "Instance Profile",
	"tags": [],
	"description": "",
	"content": "An Instance Profile is an abstraction over the resource characteristics of an instance (basically, as of today, CPU \u0026ldquo;cores\u0026rdquo; and RAM). It is represented in StackGres with the CRD SGInstanceProfile. You can think of instance profiles as \u0026ldquo;t-shirt sizes\u0026rdquo;, a way to create named t-shirt sizes, that you will reference when you create your clusters. It is a way to enforce best practices by using standardized instance sizes.\nCreate the following file: sginstanceprofile-small.yaml:\napiVersion: stackgres.io/v1 kind: SGInstanceProfile metadata: namespace: demo name: size-small spec: cpu: \u0026#34;4\u0026#34; memory: \u0026#34;8Gi\u0026#34; and deploy to Kubernetes:\nkubectl apply -f sginstanceprofile-small.yaml You may create other instance profiles with other sizes if you wish.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/install/integrations/nutanix-ahv/",
	"title": "Integrations - Nutanix AHV",
	"tags": [],
	"description": "",
	"content": "Integrations - Nutanix AHV StackGres installation documentation for production environment.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/install/integrations/nutanix-karbon/",
	"title": "Integrations - Nutanix Karbon",
	"tags": [],
	"description": "",
	"content": "Integrations - Nutanix Karbon This document give the guide for StackGres installation Nutanix Karbon Cluster production environment.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/intro/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Chapter 1 Introduction Discover what StackGres is all about and the core-concepts behind it.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/install/prerequisites/services-mesh-integration/istio/",
	"title": "Istio",
	"tags": [],
	"description": "Details about how to work in a k8s cluster with Istio",
	"content": "StackGres already has an implementation of Envoy, the sidecar injected by Istio (istio-proxy) it\u0026rsquo;s not compatible at the moment. In a k8s cluster with Istio installed you just only need to Annotate the StackGres cluster to avoid the sidecar injection from Istio.\nAnnotate StackGres pods Before you create a StackGres cluster make sure you add the annotation sidecar.istio.io/inject: 'false' to the pods as is shown below:\napiVersion: stackgres.io/v1 kind: SGCluster metadata: namespace: gitlab-db name: gitlab-db spec: metadata: annotations: pods: sidecar.istio.io/inject: 'false' postgresVersion: '12.3' instances: 3 This will avoid your pods enter in a CrashLoopBackOff state.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/install/prerequisites/monitoring/",
	"title": "Monitoring",
	"tags": [],
	"description": "Details about how to setup and configure prometheus",
	"content": "As early indicated in Component of the Stack StackGres, at the moment, only supports Prometheus integration.\nMonitoring, Observability and Alerting with Prometheus and Grafana Prometheus natively includes the following services:\n Prometheus Server: The core service Alert Manager: Handle events and send notifications to your preferred on-call platform  Installing Community Prometheus Stack If the user is willing to install a full Prometheus Stack (State Metrics, Node Exporter and Grafana), there is a community chart that provides this at kube-prometheus-stack installation instructions.\nFirst, add the Prometheus Community repositories:\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add stable https://charts.helm.sh/stable helm repo update Create the monitoring namespace:\nkubectl create namespace monitoring Install the Prometheus Server Operator:\nhelm install --namespace monitoring prometheus prometheus-community/kube-prometheus-stack --set grafana.enabled=true --version 12.10.6  StackGres provides more and advanced options for monitoring installation, see Operator installation with Helm in the Production installation session.\n Once the operator is installed, take note of the generated secrets as you they are need to be specified at StackGres operator installation. By default are user=admin and password=prom-operator:\nkubectl get secret prometheus-grafana \\  --namespace monitoring \\  --template \u0026#39;{{ printf \u0026#34;user = %s\\npassword = %s\\n\u0026#34; (index .data \u0026#34;admin-user\u0026#34; | base64decode) (index .data \u0026#34;admin-password\u0026#34; | base64decode) }}\u0026#39; Grafana\u0026rsquo;s hostname also can be queried as:\nkubectl get --namespace monitoring deployments prometheus-grafana -o json | jq -r '.metadata.name' Re-routing services to different ports In a production setup, is very likely that you will be installing all the resources in a remote location, so you\u0026rsquo;ll need to route the services through specific interfaces and ports.\n For sake of simplicity, we will port-forward to all interfaces (0.0.0.0), although we strongly recommend to only expose through internal network interfaces when dealing on production.\n Exposing the Grafana UI To access Grafana\u0026rsquo;s dashboard remotely, it can be done through the following step (it will be available at \u0026lt;your server ip\u0026gt;:9999):\nGRAFANA_POD=$(kubectl get pods --namespace monitoring -l \u0026#34;app.kubernetes.io/name=grafana\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl port-forward \u0026#34;$GRAFANA_POD\u0026#34; --address 0.0.0.0 9999:3000 --namespace monitoring Exposing the Prometheus Server UI POD_NAME=$(kubectl get pods --namespace monitoring -l \u0026#34;app=prometheus\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl --namespace monitoring port-forward $POD_NAME --address 0.0.0.0 9090 The Prometheus server serves through port 80 under prometheus-operator-server.monitoring.svc.cluster.local DNS name.\nExposing Alert Manager Over port 80, Prometheus alertmanager can be accessed through prometheus-operator-alertmanager.monitoring.svc.cluster.local DNS name.\nexport POD_NAME=$(kubectl get pods --namespace monitoring -l \u0026quot;app=alertmanager\u0026quot; -o jsonpath=\u0026quot;{.items[0].metadata.name}\u0026quot;) kubectl --namespace monitoring port-forward $POD_NAME --address 0.0.0.0 9093 Pre-existing Grafana Integration and Pre-requisites Integrating Grafana If you already have a Grafana installation in your system you can embed it automatically in the StackGres UI by setting the property grafana.autoEmbed=true:\nhelm install --namespace stackgres stackgres-operator https://stackgres.io/downloads/stackgres-k8s/stackgres/0.9.5/helm/stackgres-operator.tgz \\ --set grafana.autoEmbed=true This method requires the installation process to be able to authenticate using administrative username and password to the Grafana\u0026rsquo;s API (see installation via helm for more options related to automatic embedding of Grafana).\nManual integration Some manual steps are required in order to achieve such integration.\n  Create Grafana dashboard for Postgres exporter and copy/paste share URL:\nUsing the UI: Click on Grafana \u0026gt; Create \u0026gt; Import \u0026gt; Grafana.com Dashboard 9628\nCheck the dashboard for more details.\n  Copy/paste Grafana\u0026rsquo;s dashboard URL for the Postgres exporter:\nUsing the UI: Click on Grafana \u0026gt; Dashboard \u0026gt; Manage \u0026gt; Select Postgres exporter dashboard \u0026gt; Copy URL\n  Create and copy/paste Grafana API token:\nUsing the UI: Grafana \u0026gt; Configuration \u0026gt; API Keys \u0026gt; Add API key (for viewer) \u0026gt; Copy key value\n  Installing Grafana and create basic dashboards If you already installed the prometheus-community/kube-prometheus-stack you can skip this session. It was Get the source repository for the Grafana charts:\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update And install the chart:\nhelm install --namespace monitoring grafana grafana/grafana Get the admin credential:\nkubectl get secret --namespace monitoring grafana -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 --decode ; echo Expose your Grafana service at grafana.monitoring.svc.cluster.local (port 80) through your interfaces and port 3000 to login remotely (using above secret):\nPOD_NAME=$(kubectl get pods --namespace monitoring -l \u0026#34;app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl --namespace monitoring port-forward $POD_NAME --address 0.0.0.0 3000  NOTE: take note of the Grafana\u0026rsquo;s URL grafana.monitoring.svc.cluster.local, which will be used when configuring StackGres Operator.\n The following script, will create a basic PostgreSQL dashboard against Grafana\u0026rsquo;s API (you can change grafana_host to point to the remote location):\ngrafana_host=http://localhost:3000 grafana_admin_cred=$(kubectl get secret --namespace monitoring grafana -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 --decode ; echo) grafana_credentials=admin:${grafana_admin_cred} grafana_prometheus_datasource_name=Prometheus curl_grafana_api() { curl -sk -H \u0026#34;Accept: application/json\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -u \u0026#34;$grafana_credentials\u0026#34; \u0026#34;$@\u0026#34; } get_admin_settings() { # Not executed in the script, but useful to keep this curl_grafana_api -X GET ${grafana_host}/api/admin/settings | jq . } dashboard_id=9628 dashboard_json=\u0026#34;$(cat \u0026lt;\u0026lt; EOF { \u0026#34;dashboard\u0026#34;: $(curl_grafana_api \u0026#34;$grafana_host/api/gnet/dashboards/$dashboard_id\u0026#34; | jq .json), \u0026#34;overwrite\u0026#34;: true, \u0026#34;inputs\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;DS_PROMETHEUS\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;pluginId\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;$grafana_prometheus_datasource_name\u0026#34; }] } EOF )\u0026#34; grafana_dashboard_url=\u0026#34;$(curl_grafana_api -X POST -d \u0026#34;$dashboard_json\u0026#34; \u0026#34;$grafana_host/api/dashboards/import\u0026#34; | jq -r .importedUrl)\u0026#34; echo ${grafana_host}${grafana_dashboard_url} The resulting URL will be the dashboard whether your PostgreSQL metrics will be show up.\nMonitoring Setup validation At this point, you should have ended with the following pods:\n# kubectl get pods -n monitoring NAME READY STATUS RESTARTS AGE alertmanager-prometheus-kube-prometheus-alertmanager-0 2/2 Running 0 20m grafana-7575c4b7b5-2cbvw 1/1 Running 0 14m prometheus-grafana-5b458bf78c-tpqrl 2/2 Running 0 20m prometheus-kube-prometheus-operator-576f4bf45b-w5j9m 2/2 Running 0 20m prometheus-kube-state-metrics-c65b87574-tsx24 1/1 Running 0 20m prometheus-operator-alertmanager-655b8bc7bf-hc6fd 2/2 Running 0 79m prometheus-operator-kube-state-metrics-69fcc8d48c-tmn8j 1/1 Running 0 79m prometheus-operator-node-exporter-28qz9 1/1 Running 0 79m prometheus-operator-pushgateway-888f886ff-bxxtw 1/1 Running 0 79m prometheus-operator-server-7686fc69bd-mlvsx 2/2 Running 0 79m prometheus-prometheus-kube-prometheus-prometheus-0 3/3 Running 1 20m prometheus-prometheus-node-exporter-jbsm2 0/1 Pending 0 20m Enable Prometheus Auto Binding in Cluster To allow the operator discover available Prometheus and create required ServiceMonitors to store StackGres stats in existing instances of prometheus (only for those that are created through the Prometheus Operator) you have to set to true field .spec.prometheusAutobind in your SGCluster:\napiVersion: stackgres.io/v1 kind: SGCluster metadata: name: simple spec: instances: 2 postgresVersion: \u0026#39;latest\u0026#39; pods: persistentVolume: size: \u0026#39;5Gi\u0026#39; prometheusAutobind: true "
},
{
	"uri": "https://stackgres.io/doc/0.9/install/prerequisites/nonproduction/",
	"title": "Non production options",
	"tags": [],
	"description": "Important notes for Non production options in the productiom environment",
	"content": "We recommend to disable all non production options in a production environment. To do so create a YAML values file to include in the helm installation (-f or --values parameters) of the StackGres operator similar to the following:\nnonProductionOptions: {} "
},
{
	"uri": "https://stackgres.io/doc/0.9/runbooks/pgbadger-reports/",
	"title": "PGBadger reports",
	"tags": [],
	"description": "Details about how to generate a pgbadger report from the distributed logs server.",
	"content": "This tutorial expects that you have pgbadger installed on your machine. Check the installation procedure to get it running properly.\nBefore you start Before start, be sure that you have a SGCluster runing that is using a SGDistributedLogs server, like below:\n--- apiVersion: stackgres.io/v1beta1 kind: SGDistributedLogs metadata: namespace: default name: my-distributed-logs spec: persistentVolume: size: 20Gi  Remember to change the size according with your needs.\n PostgreSQL Configuration for PGBadger To generate a pgbadger report, a few configuration parameters are necessary:\n--- apiVersion: stackgres.io/v1beta1 kind: SGPostgresConfig metadata: name: my-postgres-config namespace: default spec: postgresVersion: \u0026#34;12\u0026#34; postgresql.conf: # Logging configuration for pgbadger log_checkpoints: on log_connections: on log_disconnections: on log_lock_waits: on log_temp_files: 0 # Adjust the minimum time to collect data log_min_duration_statement: \u0026#39;5s\u0026#39; log_autovacuum_min_duration: 0 Check pgbadger documentation for more tails about the necessary parameters to setup Postgres.\nCluster configuration The final SGCluster should be something like this:\n--- apiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: my-db-cluster namespace: default spec: # ... configurations: sgPostgresConfig: my-postgres-config distributedLogs: sgDistributedLogs: my-distributed-logs Exporting the log files into CSV Execute the command below to locate the pod of the distributed log server:\nkubectl get pods -o name -l distributed-logs-name=my-distributed-logs # pod/my-distributed-logs-0 Connect on the distributed server and export the log into the CSV format:\nQUERY=$(cat \u0026lt;\u0026lt;EOF COPY ( SELECT log_time, user_name, database_name, process_id, connection_from, session_id, session_line_num, command_tag, session_start_time, virtual_transaction_id, transaction_id, error_severity, sql_state_code, message, detail, hint, internal_query, internal_query_pos, context, query, query_pos, \u0026#34;location\u0026#34;, application_name FROM log_postgres ) to STDOUT CSV DELIMITER \u0026#39;,\u0026#39;; EOF ) kubectl exec -it pod/my-distributed-logs-0 -c patroni -- psql default_my-db-cluster -At -c \u0026#34;${QUERY}\u0026#34; \u0026gt; data.csv  Add a WHERE clause on the SELECT to filter the log on the necessary period, like this:\n--- ...  WHERE log_time \u0026gt; \u0026#39;begin timestamp\u0026#39; and log_time \u0026lt; \u0026#39;end timestamp\u0026#39;  With the csv file, just call pgbadger:\npgbadger --format csv --outfile pgbadger_report.html data.csv All in one script PGbadger has support to a external command to get the log info, using that is possible to create a all-in-one script to generate the pgbadger report.\nPOD=$(kubectl get pods -o name -l distributed-logs-name=my-distributed-logs) CLUSTER_NAME=\u0026#34;my-db-cluster\u0026#34; QUERY=$(cat \u0026lt;\u0026lt;EOF COPY ( SELECT log_time, user_name, database_name, process_id, connection_from, session_id, session_line_num, command_tag, session_start_time, virtual_transaction_id, transaction_id, error_severity, sql_state_code, message, detail, hint, internal_query, internal_query_pos, context, query, query_pos, \u0026#34;location\u0026#34;, application_name FROM log_postgres ) to STDOUT CSV DELIMITER \u0026#39;,\u0026#39;; EOF ) pgbadger \\  --format csv \\  --outfile pgbadger_report.html \\  --command \u0026#34;kubectl exec -it ${POD}-c patroni -- psql default_${CLUSTER_NAME}-At -c \\\u0026#34;${QUERY}\\\u0026#34;\u0026#34; "
},
{
	"uri": "https://stackgres.io/doc/0.9/administration/cluster/pool/admin/",
	"title": "Pooling Administration and Internal Stats",
	"tags": [],
	"description": "",
	"content": "Accessing the Pooling Admin console PgBouncer includes an admin database-style connection for getting valuable information about the pool stats, like counters, aggregations, client and server connection, etc. Those values are critical to be understood for a production alike environment.\nAccess the console through container socket and pgbouncer (this is not a database user) user:\nkubectl exec -it -c postgres-util test-0 -- psql -p 6432 -d pgbouncer pgbouncer psql (12.4 OnGres Inc., server 1.13.0/bouncer) Type \u0026#34;help\u0026#34; for help. pgbouncer=# \\x Expanded display is on. Getting valuable pool information pgbouncer=# show stats; -[ RECORD 1 ]-----+---------- database | pgbouncer total_xact_count | 1 total_query_count | 1 total_received | 0 total_sent | 0 total_xact_time | 0 total_query_time | 0 total_wait_time | 0 avg_xact_count | 0 avg_query_count | 0 avg_recv | 0 avg_sent | 0 avg_xact_time | 0 avg_query_time | 0 avg_wait_time | 0 pgbouncer=# show pools; -[ RECORD 1 ]--------- database | pgbouncer user | pgbouncer cl_active | 1 cl_waiting | 0 sv_active | 0 sv_idle | 0 sv_used | 0 sv_tested | 0 sv_login | 0 maxwait | 0 maxwait_us | 0 pool_mode | statement pgbouncer=# show clients; -[ RECORD 1 ]+------------------------ type | C user | pgbouncer database | pgbouncer state | active addr | unix port | 6432 local_addr | unix local_port | 6432 connect_time | 2020-10-23 13:19:54 UTC request_time | 2020-10-23 14:18:23 UTC wait | 3445 wait_us | 617385 close_needed | 0 ptr | 0x1a5c350 link | remote_pid | 28349 tls | Other useful commands:\n show servers show fds show mem show stats_totals show stat_averages  Reference Available commands:\nSHOW HELP|CONFIG|DATABASES|POOLS|CLIENTS|SERVERS|USERS|VERSION SHOW FDS|SOCKETS|ACTIVE_SOCKETS|LISTS|MEM SHOW DNS_HOSTS|DNS_ZONES SHOW STATS|STATS_TOTALS|STATS_AVERAGES|TOTALS SET key = arg RELOAD PAUSE [\u0026lt;db\u0026gt;] RESUME [\u0026lt;db\u0026gt;] DISABLE \u0026lt;db\u0026gt; ENABLE \u0026lt;db\u0026gt; RECONNECT [\u0026lt;db\u0026gt;] KILL \u0026lt;db\u0026gt; SUSPEND SHUTDOWN "
},
{
	"uri": "https://stackgres.io/doc/0.9/install/prerequisites/",
	"title": "Pre-requisites",
	"tags": [],
	"description": "",
	"content": "As explained in the Demo section, for setting up the Operator and StackGres Cluster, you need to have an environment on top of which it needs to request the necessary resources.\nStackGres is able to run on any Kubernetes installation from 1.11 to 1.17 version, to maintain support for some version, please follow up the open discussion at\u0026quot; #666.\n Backups  Details about how to setup and configure the backups\n   Data Storage  Details about how to setup and configure the storage classes. Storage classes are used by the database clusters and will impact performance and availability of the cluster.\n   Monitoring  Details about how to setup and configure prometheus\n   Non production options  Important notes for Non production options in the productiom environment\n   Service mesh integration  Service mesh integration Details about the different options to integrate StackGres with some service mesh implementations. Istio Details about how to work in a k8s cluster with Istio   Istio  Details about how to work in a k8s cluster with Istio   "
},
{
	"uri": "https://stackgres.io/doc/0.9/developer/prerequisites/",
	"title": "Pre-requisites",
	"tags": [],
	"description": "",
	"content": " Java OpenJDK 1.8 or higher GraalVM 19.2.1 (Optional. Required for native image tests) Maven 3.6.2 ot higher (Optional. The mvnw script wrapper can be used) Docker 19.03.5 or higher (Optional. Required for integration and e2e tests) Kubectl 12.10 or higher (Optional. Required for e2e tests outside of docker) Helm 3.1.0 or higher (Optional. Required for e2e tests outside of docker or installation of charts) Kind 0.6.0 or higher (Optional. Required for e2e tests outside of docker)  "
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "StackGres runs on Kubernetes, and as such requires an operational K8s cluster to run. This section of the tutorial will provide basic guidance on how to set up a K8s cluster, just as a reference. But you may run the tutorial on any K8s-compatible cluster. Please note that all commands in this tutorial are validated with the K8s environments described here, and may require minor adjustments for different K8s environments. If you need further guidance, please ask in our Slack or Discord channels.\nYou may additionally need:\n  Prometheus (or Prometheus-compatible software) and Grafana. They are additional components, not strictly required. But without Prometheus there will be no automatic monitoring; and without Grafana, there will be no integration of the prec-configured StackGres dashboards into the StackGres Web Console. Here you will have basic information on how to install them for the tutorial.\n  An object storage bucket for backups. StackGres uses object storage (S3, S3-compatible, GCS or Azure Blob) for storing the backups. You will need to have one for the tutorial if you want to have backups. Basic guidance will be provided here on how to create a bucket, but you may use any compatible object storage bucket.\n  "
},
{
	"uri": "https://stackgres.io/doc/0.9/demo/setenv/",
	"title": "Setting up the environment",
	"tags": [],
	"description": "",
	"content": "Exists many kubernetes environment out there. Here are some instruction on how to setup some of them.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/reference/crd/sgcluster/",
	"title": "SGCluster",
	"tags": [],
	"description": "Details about SGCluster configurations",
	"content": "StackGres PostgreSQL cluster can be created using a cluster Custom Resource (CR) in Kubernetes.\n Kind: SGCluster\nlistKind: SGClusterList\nplural: sgclusters\nsingular: sgcluster\n Spec\n   Property Required Updatable Type Default Description     postgresVersion ✓ ✓ string  Postgres version used on the cluster. It is either of:\n The string \u0026lsquo;latest\u0026rsquo;, which automatically sets the latest major.minor Postgres version. A major version, like \u0026lsquo;12\u0026rsquo; or \u0026lsquo;11\u0026rsquo;, which sets that major version and the latest minor version. A specific major.minor version, like \u0026lsquo;12.2\u0026rsquo;.      instances ✓ ✓ integer  Number of StackGres instances for the cluster. Each instance contains one Postgres server. Out of all of the Postgres servers, one is elected as the master, the rest remain as read-only replicas.    sgInstanceProfile  ✓ string will be generated Name of the SGInstanceProfile. A SGInstanceProfile defines CPU and memory limits. Must exist before creating a cluster. When no profile is set, a default (currently: 1 core, 2 GiB RAM) one is used.    metadata  ✓ object  Metadata information from cluster created resources.    postgresServices  ✓ object  Kubernetes services created or managed by StackGres.    pods ✓ ✓ object  Cluster pod\u0026rsquo;s configuration.    configurations  ✓ object  Cluster custom configurations.    prometheusAutobind  ✓ boolean false If enabled, a ServiceMonitor is created for each Prometheus instance found in order to collect metrics.    initialData   object  Cluster initialization data options. Cluster may be initialized empty, or from a backup restoration. Specifying scripts to run on the database after cluster creation is also possible.    distributedLogs  ✓ object  StackGres features a functionality for all pods to send Postgres, Patroni and PgBouncer logs to a central (distributed) location, which is in turn another Postgres database. Logs can then be accessed via SQL interface or from the web UI. This section controls whether to enable this feature or not. If not enabled, logs are send to the pod\u0026rsquo;s standard output.    nonProductionOptions  ✓ array       Example:\napiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: stackgres spec: instances: 1 postgresVersion: \u0026#39;latest\u0026#39; pods: persistentVolume: size: \u0026#39;5Gi\u0026#39; sgInstanceProfile: \u0026#39;size-s\u0026#39; Metadata Holds custom metadata information for StackGres generated resources to have.\n   Property Required Updatable Type Default Description     annotations  ✓ object  Custom Kubernetes annotations to be passed to resources created and managed by StackGres.     Annotations Holds custom annotations for StackGres generated resources to have.\n   Property Required Updatable Type Default Description     allResources  ✓ object  Annotations to attach to any resource created or managed by StackGres.    pods  ✓ object  Annotations to attach to pods created or managed by StackGres.    services  ✓ object  Annotations to attach to services created or managed by StackGres.     apiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: stackgres spec: pods: metadata: annotations: allResources: customAnnotations: customAnnotationValue Postgres Services Specifies the service configuration for the cluster:\n   Property Required Updatable Type Default Description     Primary  ✓ object  Configuration for the -primary service. It provides a stable connection (regardless of primary failures or switchovers) to the read-write Postgres server of the cluster.    Replicas  ✓ object  Configuration for the -replicas service. It provides a stable connection (regardless of replica node failures) to any read-only Postgres server of the cluster. Read-only servers are load-balanced via this service.     Primary service type    Property Required Updatable Type Default Description     enabled  ✓ boolean ClusterIP Specify if the -primary service should be created or not.    type  ✓ string ClusterIP Specifies the type of Kubernetes service.    annotations  ✓ object ClusterIP Custom Kubernetes annotations passed to the -primary service.     Replicas service type    Property Required Updatable Type Default Description     enabled  ✓ boolean ClusterIP Specify if the -replicas service should be created or not.    type  ✓ string ClusterIP Specifies the type of Kubernetes service.    annotations  ✓ object ClusterIP Custom Kubernetes annotations passed to the -replicas service.     Pods Cluster\u0026rsquo;s pod configuration\n   Property Required Updatable Type Default Description     persistentVolume ✓ ✓ object  Pod\u0026rsquo;s persistent volume configuration.    disableConnectionPooling  ✓ boolean false If set to true, avoids creating a connection pooling (using PgBouncer) sidecar.    disableMetricsExporter  ✓ boolean false If set to true, avoids creating the Prometheus exporter sidecar. Recommended when there\u0026rsquo;s no intention to use Prometheus for monitoring.    disablePostgresUtil  ✓ boolean false If set to true, avoids creating the postgres-util sidecar. This sidecar contains usual Postgres administration utilities that are not present in the main (patroni) container, like psql. Only disable if you know what you are doing.    metadata  ✓ object  Pod custom metadata information.    scheduling  ✓ object  Pod custom scheduling configuration.     Sidecar containers A sidecar container is a container that adds functionality to PostgreSQL or to the cluster infrastructure. Currently StackGres implement following sidecar containers:\n envoy: this container is always present, and is not possible to disable it. It serve as a edge proxy from client to PostgreSQL instances or between PostgreSQL instances. It enables network metrics collection to provide connection statistics. pgbouncer: a container with pgbouncer as the connection pooling for the PostgreSQL instances. prometheus-postgres-exporter: a container with postgres exporter that exports metrics for the PostgreSQL instances. fluent-bit: a container with fluent-bit that send logs to a distributed logs cluster. postgres-util: a container with psql and all PostgreSQL common tools in order to connect to the database directly as root to perform any administration tasks.  The following example, disable all optional sidecars:\napiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: stackgres spec: pods: disableConnectionPooling: false disableMetricsExporter: false disablePostgresUtil: false Persistent Volume Holds the configurations of the persistent volume that the cluster pods are going to use.\n   Property Required Updatable Type Default Description     size ✓ ✓ string  Size of the PersistentVolume set for each instance of the cluster. This size is specified either in Mebibytes, Gibibytes or Tebibytes (multiples of 2^20, 2^30 or 2^40, respectively).    storageClass  ✓ string default storage class Name of an existing StorageClass in the Kubernetes cluster, used to create the PersistentVolumes for the instances of the cluster.     apiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: stackgres spec: pods: persistentVolume: size: \u0026#39;5Gi\u0026#39; storageClass: default Pods metadata Holds custom metadata information for StackGres pods to have.\n   Property Required Updatable Type Default Description     labels  ✓ string  Additional labels for StackGres Pods.     apiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: stackgres spec: pods: metadata: annotations: customAnnotations: customAnnotationValue labels: customLabel: customLabelValue Scheduling Holds scheduling configuration for StackGres pods to have.\n   Property Required Updatable Type Default Description     nodeSelector  ✓ object  Pod custom node selector.    tolerations  ✓ array  Pod custom node tolerations     Tolerations Holds scheduling configuration for StackGres pods to have.\n   Property Required Updatable Type Default Description     key  ✓ string  Pod custom node selector.    operator  ✓ string Equal Pod custom node tolerations    value  ✓ string  Pod custom node tolerations    effect  ✓ string match all taint effects Pod custom node tolerations     Configurations Custom configurations to be applied to the cluster.\n   Property Required Updatable Type Default Description     sgPostgresConfig  ✓ string will be generated Name of the SGPostgresConfig used for the cluster. It must exist. When not set, a default Postgres config, for the major version selected, is used.    sgPoolingConfig  ✓ string will be generated Name of the SGPoolingConfig used for this cluster. Each pod contains a sidecar with a connection pooler (currently: PgBouncer). The connection pooler is implemented as a sidecar.\nIf not set, a default configuration will be used. Disabling connection pooling altogether is possible if the disableConnectionPooling property of the pods object is set to true.\n    sgBackupConfig  ✓ string  Name of the SGBackupConfig to use for the cluster. It defines the backups policy, storage and retention, among others, applied to the cluster. When not set, a default backup config is used.     Example:\napiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: stackgres spec: configurations: sgPostgresConfig: \u0026#39;postgresconf\u0026#39; sgPoolingConfig: \u0026#39;pgbouncerconf\u0026#39; sgBackupConfig: \u0026#39;backupconf\u0026#39; Initial Data Configuration Specifies the cluster initialization data configurations\n   Property Required Updatable Type Default Description     restore   object      scripts   object  A list of SQL scripts executed in sequence, exactly once, when the database is bootstrap and/or after restore is completed.     Restore configuration By default, stackgres it\u0026rsquo;s creates as an empty database. To create a cluster with data from an existent backup, we have the restore options. It works, by simply indicating the backup CR UUI that we want to restore.\n   Property Required Updatable Type Default Description     fromBackup ✓  string  When set to the UID of an existing SGBackup, the cluster is initialized by restoring the backup data to it. If not set, the cluster is initialized empty. The selected backup must be in the same namespace.    downloadDiskConcurrency   integer 1 The backup fetch process may fetch several streams in parallel. Parallel fetching is enabled when set to a value larger than one.     Example:\napiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: stackgres spec: initialData: restore: fromBackup: d7e660a9-377c-11ea-b04b-0242ac110004 downloadDiskConcurrency: 1 Scripts configuration By default, stackgres creates as an empty database. To execute some scripts, we have the scripts options where you can specify a script or reference a key in a ConfigMap or a Secret that contains the script to execute.\n   Property Required Updatable Type Default Description     name   string  Name of the script. Must be unique across this SGCluster.    database   string postgres Database where the script is executed. Defaults to the postgres database, if not specified.    script   string  Raw SQL script to execute. This field is mutually exclusive with scriptFrom field.    scriptFrom   object  Reference to either a Kubernetes Secret or a ConfigMap that contains the SQL script to execute. This field is mutually exclusive with script field.\nFields secretKeyRef and configMapKeyRef are mutually exclusive, and one of them is required.\n     Example:\napiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: stackgres spec: initialData: scripts: - name: create-stackgres-user scriptFrom: secretKeyRef: # read the user from a Secret to maintain credentials in a safe place name: stackgres-secret-sqls-scripts key: create-stackgres-user.sql - name: create-stackgres-database script: | CREATE DATABASE stackgres WITH OWNER stackgres; - name: create-stackgres-schema database: stackgres scriptFrom: configMapKeyRef: # read long script from a ConfigMap to avoid have to much data in the helm releasea and the sgcluster CR name: stackgres-sqls-scripts key: create-stackgres-schema.sql Script from    Property Required Updatable Type Default Description     configMapKeyRef   object  A ConfigMap reference that contains the SQL script to execute. This field is mutually exclusive with secretKeyRef field.    secretKeyRef   object  A Kubernetes SecretKeySelector that contains the SQL script to execute. This field is mutually exclusive with configMapKeyRef field.     Script from ConfigMap    Property Required Updatable Type Default Description     name   string  The name of the ConfigMap that contains the SQL script to execute.    key   string  The key name within the ConfigMap that contains the SQL script to execute.     Script from Secret    Property Required Updatable Type Default Description     name   string  Name of the referent. More information.    key   string  The key of the secret to select from. Must be a valid secret key.     Distributed logs Specifies the distributed logs cluster to send logs to:\n   Property Required Updatable Type Default Description     sgDistributedLogs   string  Name of the SGDistributedLogs to use for this cluster. It must exist.     Example:\napiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: stackgres spec: distributedLogs: sgDistributedLogs: distributedlogs Non Production options The following options should NOT be enabled in a production environment.\n   Property Required Updatable Type Default Description     disableClusterPodAntiAffinity  ✓ boolean false It is a best practice, on non-containerized environments, when running production workloads, to run each database server on a different server (virtual or physical), i.e., not to co-locate more than one database server per host.\nThe same best practice applies to databases on containers. By default, StackGres will not allow to run more than one StackGres pod on a given Kubernetes node. Set this property to true to allow more than one StackGres pod per node.\n     "
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/prerequisites/software/",
	"title": "Software",
	"tags": [],
	"description": "",
	"content": "To run this tutorial, you will need to have installed in your environment:\n kubectl Helm 3 Some commands may require running them from a shell.  "
},
{
	"uri": "https://stackgres.io/doc/0.9/install/integrations/nutanix-karbon/sgoperator-install/",
	"title": "StackGres Operator Install",
	"tags": [],
	"description": "",
	"content": "StackGres Operator Install The StackGres Operator deployment will run simple commands from the automation process, this is our GitOps for StackGres.\nInstallation Steps Once the Karbon Cluster is ready, start creating the required resources to deploy the StackGres operator as follows.\nStackGres (the operator and associated components) may be installed on any namespace. It is recommended to create a dedicated namespace for StackGres:\nkubectl create namespace stackgres And we should created the namespace where we want to run our clusters\nkubectl create namespace karbon StackGres recommended installation is performed from the published Helm chart. The following command will install StackGres with Helm3, allow StackGres Web Console, and exposing that Web Console via a load balancer\nhelm install --namespace stackgres stackgres-operator \\ --set-string adminui.service.type=LoadBalancer \\ https://stackgres.io/downloads/stackgres-k8s/stackgres/0.9.4/helm/stackgres-operator.tgz Please refer to Helm chart parameters for further customization of the above Helm parameters. Add or Replace them for your custom installation parameters, if needed.\nNote that using adminui.service.type=LoadBalancer will create a network load balancer. You may alternatively use ClusterIP if that\u0026rsquo;s your preference.\nStackGres installation may take a few minutes. The output will be similar to:\nNAME: stackgres-operator LAST DEPLOYED: Mon Mar 1 00:25:10 2021 NAMESPACE: stackgres STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Release Name: stackgres-operator StackGres Version: 0.9.4 _____ _ _ _____ / ____| | | | / ____| | (___ | |_ __ _ ___| | _| | __ _ __ ___ ___ \\___ \\| __/ _` |/ __| |/ / | |_ | \u0026#39;__/ _ \\/ __| ____) | || (_| | (__| \u0026lt;| |__| | | | __/\\__ \\ |_____/ \\__\\__,_|\\___|_|\\_\\\\_____|_| \\___||___/ by OnGres, Inc. Check if the operator was successfully deployed and is available: kubectl describe deployment -n stackgres stackgres-operator kubectl wait -n stackgres deployment/stackgres-operator --for condition=Available Check if the restapi was successfully deployed and is available: kubectl describe deployment -n stackgres stackgres-restapi kubectl wait -n stackgres deployment/stackgres-restapi --for condition=Available To access StackGres Operator UI from localhost, run the below commands: POD_NAME=$(kubectl get pods --namespace stackgres -l \u0026#34;app=stackgres-restapi\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl port-forward \u0026#34;$POD_NAME\u0026#34; 8443:9443 --namespace stackgres Read more about port forwarding here: http://kubernetes.io/docs/user-guide/kubectl/kubectl_port-forward/ Now you can access the StackGres Operator UI on: https://localhost:8443 To get the username, run the command: kubectl get secret -n stackgres stackgres-restapi --template \u0026#39;{{ printf \u0026#34;username = %s\\n\u0026#34; (.data.k8sUsername | base64decode) }}\u0026#39; To get the generated password, run the command: kubectl get secret -n stackgres stackgres-restapi --template \u0026#39;{{ printf \u0026#34;password = %s\\n\u0026#34; (.data.clearPassword | base64decode) }}\u0026#39; Remember to remove the generated password hint from the secret to avoid security flaws: kubectl patch secrets --namespace stackgres stackgres-restapi --type json -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/data/clearPassword\u0026#34;}]\u0026#39; Several useful commands are provided as part of the Helm installation output. Let\u0026rsquo;s use them to connect to the StackGres Web Console. Get user and password and save it to use later:\nkubectl get secret -n stackgres stackgres-restapi --template \u0026#39;{{ printf \u0026#34;username = %s\\n\u0026#34; (.data.k8sUsername | base64decode) }}\u0026#39; kubectl get secret -n stackgres stackgres-restapi --template \u0026#39;{{ printf \u0026#34;password = %s\\n\u0026#34; (.data.clearPassword | base64decode) }}\u0026#39; If you are working in a Karbon Laboratory Cluster and connecting to Karbon through a Jumper host, forwarding the node IP where the StackGres RestApi is running is needed. Running the next command from the Jumper Host will forward the rest api IP to itself.\nPOD_NAME=$(kubectl get pods --namespace stackgres -l \u0026#34;app=stackgres-restapi\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl port-forward “$POD_NAME” 8443:9443 --namespace stackgres If the Jumper Host doesn’t contain a graphical interface, you should create a ssh tunnel to get access in SG UI. Open another terminal and run the following command (don’t close the previous one which holds the kubernetes port forward):\nssh -L 8443:localhost:8443 [Jumper Host IP Address] To access the web console paste the link https://localhost:8443 in the Citrix Instance’s Browser and you should see the SG login page.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/administration/cluster/connection/dns/",
	"title": "Through k8s internal DNS",
	"tags": [],
	"description": "Describes how to connect on the cluster inside the k8s environment.",
	"content": "With every StackGres cluster that you deploy a few of services will be deployed. To connect to the database you only need to be aware of two services: the primary and the replica service.\nThe primary service is used to connect to the primary node and the replica service is used to access any the replica nodes.\nThis services will follow a convention that is based in the cluster name and the function of the service, so that, the name of our services will be:\n ${CLUSTER-NAME}-primary ${CLUSTER-NAME}-replicas  Both services will accept connections from ports 5432 and 5433 where:\n the port 5432 will point to pgbouncer - used by the application the port 5433 will point to postgres - used for replication purposes  Therefore, given a cluster with name \u0026ldquo;stackgres\u0026rdquo; in the namespace \u0026ldquo;demo\u0026rdquo;, the primary node will accessible through the URL: stackgres-primary.demo.svc:5432. Meanwhile, the replica node is accessible through the URL: stackgres-replicas.demo.svc:5432.\nExamples For all the following examples we\u0026rsquo;re going to assume that we have a StackGres cluster named stackgres in the namespace demo.\npsql With a pod with psql running in the same kubernetes cluster than the StackGres cluster, we can connect to the primary node with the following command:\nPGPASSWORD=1775-d517-4136-958 psql -h stackgres-primary.demo.svc -U postgres "
},
{
	"uri": "https://stackgres.io/doc/0.9/intro/about/",
	"title": "What is StackGres?",
	"tags": [],
	"description": "",
	"content": " StackGres - Enterprise-grade, Full Stack PostgreSQL on Kubernetes\n StackGres is a full stack PostgreSQL distribution for Kubernetes, packed into an easy deployment unit. With a carefully selected and tuned set of surrounding PostgreSQL components.\nAn enterprise-grade PostgreSQL stack needs several other ecosystem components and significant tuning. It\u0026rsquo;s not only PostgreSQL. It requires connection pooling, automatic failover and HA, monitoring, backups and DR, centralized logging… we have built them all: a Postgres Stack.\nPostgres is not just the database. It is also all the ecosystem around it. If Postgres would be the Linux kernel, we need a PostgreSQL Distribution, surrounding PostgreSQL, to complement it with the components that are required for a production deployment. This is what we call a PostgreSQL Stack. And the stack needs to be curated. There are often several software for the same functionality. And not all is of the same quality or maturity. There are many pros and cons, and they are often not easy to evaluate. It is better to have an opinionated selection of components, that can be packaged and configured to work together in a predictable and trusted way.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/prerequisites/kubernetes-environment/aws-eks/",
	"title": "AWS EKS",
	"tags": [],
	"description": "",
	"content": "You will need to have installed the AWS CLI installed and configured, with the appropriate credentials to be able to create EKS clusters and create S3 buckets and policies. You will also need to have installed eksctl.\nTo create a cluster, run the following commands, making any necessary adjustment to the variables:\nexport AWS_PROFILE= # optional export AWS_REGION=us-east-2 export K8S_CLUSTER_NAME=stackgres eksctl --region $AWS_REGION create cluster \\ \t--name $K8S_CLUSTER_NAME \\ \t--node-type m5a.2xlarge --node-volume-size 100 --nodes 3 \\ \t--zones ${AWS_REGION}a,${AWS_REGION}b,${AWS_REGION}c \\ \t--version 1.17 This process takes around 20 minutes. The output should be similar to:\n2021-02-28 20:26:56 [ℹ] eksctl version 0.39.0 2021-02-28 20:26:56 [ℹ] using region us-east-2 2021-02-28 20:26:56 [ℹ] subnets for us-east-2a - public:192.168.0.0/19 private:192.168.96.0/19 2021-02-28 20:26:56 [ℹ] subnets for us-east-2b - public:192.168.32.0/19 private:192.168.128.0/19 2021-02-28 20:26:56 [ℹ] subnets for us-east-2c - public:192.168.64.0/19 private:192.168.160.0/19 2021-02-28 20:26:57 [ℹ] nodegroup \u0026#34;ng-092df631\u0026#34; will use \u0026#34;ami-0ba2dda6dd6a9e644\u0026#34; [AmazonLinux2/1.17] 2021-02-28 20:26:58 [ℹ] using Kubernetes version 1.17 2021-02-28 20:26:58 [ℹ] creating EKS cluster \u0026#34;stackgres\u0026#34; in \u0026#34;us-east-2\u0026#34; region with un-managed nodes 2021-02-28 20:26:58 [ℹ] will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup 2021-02-28 20:26:58 [ℹ] if you encounter any issues, check CloudFormation console or try \u0026#39;eksctl utils describe-stacks --region=us-east-2 --cluster=stackgres\u0026#39; 2021-02-28 20:26:58 [ℹ] CloudWatch logging will not be enabled for cluster \u0026#34;stackgres\u0026#34; in \u0026#34;us-east-2\u0026#34; 2021-02-28 20:26:58 [ℹ] you can enable it with \u0026#39;eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-east-2 --cluster=stackgres\u0026#39; 2021-02-28 20:26:58 [ℹ] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \u0026#34;stackgres\u0026#34; in \u0026#34;us-east-2\u0026#34; 2021-02-28 20:26:58 [ℹ] 2 sequential tasks: { create cluster control plane \u0026#34;stackgres\u0026#34;, 2 sequential sub-tasks: { wait for control plane to become ready, create nodegroup \u0026#34;ng-092df631\u0026#34; } } 2021-02-28 20:26:58 [ℹ] building cluster stack \u0026#34;eksctl-stackgres-cluster\u0026#34; 2021-02-28 20:26:59 [ℹ] deploying stack \u0026#34;eksctl-stackgres-cluster\u0026#34; 2021-02-28 20:39:53 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-stackgres-cluster\u0026#34; 2021-02-28 20:39:56 [ℹ] building nodegroup stack \u0026#34;eksctl-stackgres-nodegroup-ng-092df631\u0026#34; 2021-02-28 20:39:56 [ℹ] --nodes-min=3 was set automatically for nodegroup ng-092df631 2021-02-28 20:39:56 [ℹ] --nodes-max=3 was set automatically for nodegroup ng-092df631 2021-02-28 20:39:57 [ℹ] deploying stack \u0026#34;eksctl-stackgres-nodegroup-ng-092df631\u0026#34; 2021-02-28 20:44:24 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-stackgres-nodegroup-ng-092df631\u0026#34; 2021-02-28 20:44:25 [ℹ] waiting for the control plane availability... 2021-02-28 20:44:25 [✔] saved kubeconfig as \u0026#34;/home/aht/.kube/config\u0026#34; 2021-02-28 20:44:25 [ℹ] no tasks 2021-02-28 20:44:25 [✔] all EKS cluster resources for \u0026#34;stackgres\u0026#34; have been created 2021-02-28 20:44:26 [ℹ] adding identity \u0026#34;arn:aws:iam::292778140943:role/eksctl-stackgres-nodegroup-ng-092-NodeInstanceRole-1ITJQEMJMGD8\u0026#34; to auth ConfigMap 2021-02-28 20:44:26 [ℹ] nodegroup \u0026#34;ng-092df631\u0026#34; has 0 node(s) 2021-02-28 20:44:26 [ℹ] waiting for at least 3 node(s) to become ready in \u0026#34;ng-092df631\u0026#34; 2021-02-28 20:44:52 [ℹ] nodegroup \u0026#34;ng-092df631\u0026#34; has 3 node(s) 2021-02-28 20:44:52 [ℹ] node \u0026#34;ip-192-168-13-0.us-east-2.compute.internal\u0026#34; is ready 2021-02-28 20:44:52 [ℹ] node \u0026#34;ip-192-168-53-249.us-east-2.compute.internal\u0026#34; is ready 2021-02-28 20:44:52 [ℹ] node \u0026#34;ip-192-168-94-213.us-east-2.compute.internal\u0026#34; is ready 2021-02-28 20:44:53 [✔] EKS cluster \u0026#34;stackgres\u0026#34; in \u0026#34;us-east-2\u0026#34; region is ready Once your EKS cluster is created, you should have your ~/.kube/config populated, being able to run:\nkubectl cluster-info and get an output similar to:\nKubernetes control plane is running at https://6E48B1E2BBDE5960F174FD2D04C1F554.gr7.us-east-2.eks.amazonaws.com CoreDNS is running at https://6E48B1E2BBDE5960F174FD2D04C1F554.gr7.us-east-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. "
},
{
	"uri": "https://stackgres.io/doc/0.9/developer/stackgres/build/",
	"title": "Building StackGres",
	"tags": [],
	"description": "",
	"content": "To build stackgres run the following command inside folder stackgres-k8s/src:\n./mvnw clean install Build with checks Build with strength checks is needed in order to contribute to the project (since the CI will run those checks). To do so simply add the safer profile:\n./mvnw clean install -P safer "
},
{
	"uri": "https://stackgres.io/doc/0.9/administration/cluster/connection/",
	"title": "Connecting to the Cluster",
	"tags": [],
	"description": "This page contains details about how to connect on the StackGres cluster.",
	"content": "This page contains details about how to connect on the StackGres cluster.\n Through k8s internal DNS  Describes how to connect on the cluster inside the k8s environment.\n Exposing Services  Describes how to connect on the cluster exposing its services on the internet.\n Local connection with the postgres-util sidecar  Describes how to connect on the cluster using kubectl and the postgres-util sidecar container.\n "
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/complete-cluster/postgres-config/",
	"title": "Custom Postgres Configuration",
	"tags": [],
	"description": "",
	"content": "While StackGres comes with a carefully tuned, default Postgres configuration, you may want to set your own configuration and parameters. StackGres uses for this purpose the CRD SGPostgresConfig. By using a CRD, coupled with a webhook validator, instead of a simple ConfigMap, StackGres is able to strongly validate the desired configuration, ensuring that parameters and values are valid for the major Postgres version, and within bounds.\nConfigurations are assigned a name, which you may reference later from one or more Postgres clusters. Create the file sgpostgresconfig-config1.yaml:\napiVersion: stackgres.io/v1 kind: SGPostgresConfig metadata: namespace: demo name: pgconfig1 spec: postgresVersion: \u0026#34;12\u0026#34; postgresql.conf: work_mem: \u0026#39;16MB\u0026#39; shared_buffers: \u0026#39;2GB\u0026#39; random_page_cost: \u0026#39;1.5\u0026#39; password_encryption: \u0026#39;scram-sha-256\u0026#39; log_checkpoints: \u0026#39;on\u0026#39; jit: \u0026#39;off\u0026#39; and deploy to Kubernetes:\nkubectl apply -f sgpostgresconfig-config1.yaml You may use kubectl describe on the created resource to inspect the values that are injected (tuned by default), as well as an indication in the status field of which values are using StackGres defaults:\nkubectl -n demo describe sgpgconfig pgconfig1 Name: pgconfig1 Namespace: demo Labels: \u0026lt;none\u0026gt; Annotations: stackgres.io/operatorVersion: 1.0.0-alpha1 API Version: stackgres.io/v1 Kind: SGPostgresConfig Metadata: Creation Timestamp: 2021-03-01T10:07:10Z Generation: 1 Resource Version: 152394 Self Link: /apis/stackgres.io/v1/namespaces/demo/sgpgconfigs/pgconfig1 UID: 46d3a5c8-6d96-4082-97f3-ae9a66c25237 Spec: Postgres Version: 12 postgresql.conf: autovacuum_max_workers: 3 autovacuum_vacuum_cost_delay: 2 checkpoint_completion_target: 0.9 checkpoint_timeout: 15min default_statistics_target: 200 enable_partitionwise_aggregate: on enable_partitionwise_join: on Jit: off jit_inline_above_cost: -1 log_autovacuum_min_duration: 0 log_checkpoints: on log_connections: on log_disconnections: on log_line_prefix: \u0026#39;%t [%p]: db=%d,user=%u,app=%a,client=%h \u0026#39; log_lock_waits: on log_min_duration_statement: 1000 log_statement: ddl log_temp_files: 0 maintenance_work_mem: 2GB max_locks_per_transaction: 128 max_pred_locks_per_transaction: 128 max_prepared_transactions: 32 max_replication_slots: 20 max_wal_senders: 20 max_wal_size: 2GB min_wal_size: 1GB password_encryption: scram-sha-256 pg_stat_statements.track_utility: off random_page_cost: 1.5 shared_buffers: 2GB shared_preload_libraries: pg_stat_statements, auto_explain superuser_reserved_connections: 8 track_activity_query_size: 4096 track_functions: pl track_io_timing: on wal_keep_segments: 100 work_mem: 16MB Status: Default Parameters: enable_partitionwise_aggregate min_wal_size max_wal_senders log_checkpoints max_prepared_transactions checkpoint_timeout autovacuum_max_workers jit_inline_above_cost track_functions wal_keep_segments checkpoint_completion_target enable_partitionwise_join log_autovacuum_min_duration superuser_reserved_connections log_temp_files log_lock_waits random_page_cost max_locks_per_transaction log_disconnections maintenance_work_mem log_connections shared_preload_libraries pg_stat_statements.track_utility track_activity_query_size max_pred_locks_per_transaction max_wal_size autovacuum_vacuum_cost_delay log_min_duration_statement log_statement max_replication_slots default_statistics_target log_line_prefix track_io_timing Events: \u0026lt;none\u0026gt; "
},
{
	"uri": "https://stackgres.io/doc/0.9/demo/quickstart/",
	"title": "Demo / Quickstart",
	"tags": [],
	"description": "",
	"content": "Chapter 2 Demo / Quickstart Try out StackGres by setting up a demo in your favorite k8s environment.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/administration/cluster/connection/exposed/",
	"title": "Exposing Services",
	"tags": [],
	"description": "Describes how to connect on the cluster exposing its services on the internet.",
	"content": "To allow access outside the k8s cluster is necessary to update the services that exposes access to the StackGres cluster changing it to NodePort or LoadBalancer.\nAll examples on this pages are assuming that there is a cluster named my-db-cluster on the default namespace.\nPortforward access to the SGCluster The easiest way to use kubectl to port-forward the postgres port on the SGCluster:\n## get the service name for the primary database on my-dbcluster kubectl get services -o name -l app=StackGresCluster,cluster-name=my-db-cluster,role=master # service/my-db-cluster-primary kubectl port-forward service/my-db-cluster-primary --address 0.0.0.0 5432:5432 On another session run:\npsql -h localhost -U postgres Updating the service configuration By default, SGCluster services type are ClusterIP which means that the SGCluster will not be opened outside the k8s cluster. To change that behavior, is necessary to update the cluster, changing the service configuration.\nConnecting through NodePort NodePort is a k8s mechanism to expose a service into a dynamic in each cluster nodes. Update the SGCluster configuration like below:\n--- apiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: my-db-cluster namespace: default spec: ## ... postgresServices: primary: type: NodePort replicas: type: NodePort  Check the SGCluster reference for more details about the cluster configuration.\n Once applied, the service configuration is updated to NodePort:\nkubectl get services -l cluster=true,cluster-name=my-db-cluster # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # my-db-cluster-primary NodePort 10.101.139.224 \u0026lt;none\u0026gt; 5432:31884/TCP,5433:31998/TCP 35m # my-db-cluster-replicas NodePort 10.99.44.2 \u0026lt;none\u0026gt; 5432:32106/TCP,5433:31851/TCP 35m Get the node ip address (kind ip address on the example below):\nkubectl get nodes -o wide # NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME # kind-control-plane Ready master 115s v1.17.11 172.18.0.3 \u0026lt;none\u0026gt; Ubuntu Groovy Gorilla (development branch) 5.8.0-36-generic containerd://1.4.0 # kind-worker Ready \u0026lt;none\u0026gt; 79s v1.17.11 172.18.0.4 \u0026lt;none\u0026gt; Ubuntu Groovy Gorilla (development branch) 5.8.0-36-generic containerd://1.4.0 # kind-worker2 Ready \u0026lt;none\u0026gt; 79s v1.17.11 172.18.0.7 \u0026lt;none\u0026gt; Ubuntu Groovy Gorilla (development branch) 5.8.0-36-generic containerd://1.4.0 # kind-worker3 Ready \u0026lt;none\u0026gt; 79s v1.17.11 172.18.0.5 \u0026lt;none\u0026gt; Ubuntu Groovy Gorilla (development branch) 5.8.0-36-generic containerd://1.4.0 # kind-worker4 Ready \u0026lt;none\u0026gt; 79s v1.17.11 172.18.0.2 \u0026lt;none\u0026gt; Ubuntu Groovy Gorilla (development branch) 5.8.0-36-generic containerd://1.4.0 # kind-worker5 Ready \u0026lt;none\u0026gt; 85s v1.17.11 172.18.0.6 \u0026lt;none\u0026gt; Ubuntu Groovy Gorilla (development branch) 5.8.0-36-generic containerd://1.4.0 Connect on the cluster using psql with the INTERNAL IP of any node (172.18.0.2 per example) and the service port (31884 will point to my-db-cluster-primary on port 5432):\npsql -h 172.18.0.2 -U postgres -p 31884 Connecting through a LoadBalancer LoadBalancer is another option to expose cluster access to outside the k8s cluster. This option needs an extra configuration on the k8s cluster to install and configure an Ingress Controller that will route the connections to the target service.\nThe below example is implemented with kind and it uses MetalLB under the hood. For non-premise environments, check your cloud vendor\u0026rsquo;s documentation about the Load Balancer implementation details.\nUpdate the SGCluster configuration like below:\n--- apiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: my-db-cluster namespace: default spec: ## ... postgresServices: primary: type: LoadBalancer replicas: type: LoadBalancer  Check the SGCluster reference for more details about the cluster configuration.\n Once updated, get the service information:\nkubectl get services -l cluster=true,cluster-name=my-db-cluster # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # my-db-cluster-primary LoadBalancer 10.108.32.129 172.18.0.102 5432:30219/TCP,5433:30886/TCP 8m13s # my-db-cluster-replicas LoadBalancer 10.111.30.87 172.18.0.101 5432:31146/TCP,5433:32063/TCP 8m13s  Please note that, since we change both services to LoadBalancer, two loadbalancers were created, one for each service. Be aware that additional charges may apply on your cloud provider.\n To connect on the database, just use the EXTERNAL-IP, like below:\npsql -h 172.18.0.102 -U postgres "
},
{
	"uri": "https://stackgres.io/doc/0.9/intro/features/",
	"title": "Features",
	"tags": [],
	"description": "",
	"content": " Running on Kubernetes. Embracing multi-cloud and on-premise. Enterprise-grade, highly opinionated PostgreSQL stack. DB-as-a-Service without vendor lock-in. Root access. Open source!  Check out the features page to discover all the amazing things StackGres has built in.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/install/helm/",
	"title": "Installation via Helm",
	"tags": [],
	"description": "",
	"content": "StackGres operator and clusters can be installed using Helm version \u0026gt;= 3.1.1. As you may expect, a Production environment will require to install and setup additional components alongside your StackGres Operator and Cluster resources.\nIn this page, we are going through all the necessary steps to setup a Production like environment using Helm repositories and workflow.\nSetting up namespaces Each component of your infrastructure needs to be isolated in different namespaces as a standard practice for reusability and security. For a minimal setup, three namespaces will be created:\nkubectl create namespace stackgres kubectl create namespace monitoring # This should be already created if you followed pre-requisites # steps. kubectl create namespace my-cluster stackgres will be the StackGres' Operator namespace, and my-cluster will be the namespace for the node resources that will contain the data and working backend.\nThe monitoring namespace was created to deploy the Prometheus Operator, which will result in a running Grafana instance.\n For advanced options to the monitoring installation, see the Monitoring session in the Production Installation.\n StackGres Operator installation Now that we have configured a Backup storage place, as indicated in the pre-requisites, and a monitoring system already in place for proper observability, we can proceed to the StackGres Operator itself!\n The grafana.webHost value may change if the installation is not Prometheus' default, as well as grafana.user and grafana.password. Take note of above section\u0026rsquo;s secret outputs and replace them accordingly.\n helm install --namespace stackgres stackgres-operator \\  --set grafana.autoEmbed=true \\  --set-string grafana.webHost=prometheus-operator-grafana.monitoring \\  --set-string grafana.user=admin \\  --set-string grafana.password=prom-operator \\  --set-string adminui.service.type=LoadBalancer \\  https://stackgres.io/downloads/stackgres-k8s/stackgres/0.9.5/helm/stackgres-operator.tgz In the previous example StackGres have included several options to the installation, including the needed options to enable the monitoring. Follow the Cluster Parameters section for a described list.\nCreating and customizing your Postgres Clusters The next step is an optional one, but it will show you how to play with the StackGres versatility.\nYou can instruct StackGres to create your cluster with different hardware specification using the Custom Resource (AKA CR) SGInstanceProfile as follow\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: stackgres.io/v1beta1 kind: SGInstanceProfile metadata: namespace: my-cluster name: size-small spec: cpu: \u0026#34;2\u0026#34; memory: \u0026#34;4Gi\u0026#34; EOF But not only the Instance Profile, you can instruct StackGres to changes PostgreSQL configuration using the CR SGPostgresConfig or the PGBouncer setting with SGPoolingConfig and more, like the backup specification using SGBackupConfig\nThe next code snippets will show you how to play with these CRs.\nStart with PostgreSQL configuration using th SGPostgresConfig as follow\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: stackgres.io/v1beta1 kind: SGPostgresConfig metadata: namespace: my-cluster name: pgconfig1 spec: postgresVersion: \u0026#34;12\u0026#34; postgresql.conf: shared_buffers: \u0026#39;512MB\u0026#39; random_page_cost: \u0026#39;1.5\u0026#39; password_encryption: \u0026#39;scram-sha-256\u0026#39; log_checkpoints: \u0026#39;on\u0026#39; EOF You can easily declare the StackGres supported variables and setup your specific configuration.\nThe pooling CR, is a key piece of a cluster (currently PgBouncer as the default software fot this), as it provides connection scaling capabilities. We\u0026rsquo;ll cover all more details about this in the Customizing Pooling configuration section.\nFor better performance and stability, it is recommended to use pool_mode in transaction. An example configuration would be like this:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: stackgres.io/v1beta1 kind: SGPoolingConfig metadata: namespace: my-cluster name: poolconfig1 spec: pgBouncer: pgbouncer.ini: pool_mode: transaction max_client_conn: \u0026#39;1000\u0026#39; default_pool_size: \u0026#39;80\u0026#39; EOF The longest step for this demonstration is the backup CR:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: stackgres.io/v1beta1 kind: SGBackupConfig metadata: namespace: my-cluster name: backupconfig1 spec: baseBackups: cronSchedule: \u0026#34;*/5 * * * *\u0026#34; retention: 6 storage: type: \u0026#34;gcs\u0026#34; gcs: EOF Alternatively, StackGres could be instructed to use Google Cloud Storage\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: stackgres.io/v1beta1 kind: SGBackupConfig metadata: namespace: my-cluster name: backupconfig1 spec: baseBackups: cronSchedule: \u0026#34;*/5 * * * *\u0026#34; retention: 6 storage: type: \u0026#34;gcs\u0026#34; gcs: bucket: backup-my-cluster-of-stackgres-io gcpCredentials: secretKeySelectors: serviceAccountJSON: name: gcp-backup-bucket-secret key: my-creds.json EOF Or AWS S3 if you want\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: stackgres.io/v1beta1 kind: SGBackupConfig metadata: namespace: my-cluster name: backupconfig1 spec: baseBackups: cronSchedule: \u0026#39;*/5 * * * *\u0026#39; retention: 6 storage: type: \u0026#39;s3\u0026#39; s3: bucket: \u0026#39;backup.my-cluster.stackgres.io\u0026#39; awsCredentials: secretKeySelectors: accessKeyId: {name: \u0026#39;s3-backup-bucket-secret\u0026#39;, key: \u0026#39;accessKeyId\u0026#39;} secretAccessKey: {name: \u0026#39;s3-backup-bucket-secret\u0026#39;, key: \u0026#39;secretAccessKey\u0026#39;} EOF On AWS you will need to define some parameters if already you don\u0026rsquo;t have defined it. As bottom here is some variables and the needed permissions on S3:\nS3_BACKUP_BUCKET=backup.my-cluster.stackgres.io S3_BACKUP_BUCKET_POLICY_NAME=s3_backup_bucket_iam_policy S3_BACKUP_BUCKET_USER=s3_backup_bucket_iam_user S3_BACKUP_CREDENTIALS_K8S_SECRET=s3-backup-bucket-secret CLUSTER_NAMESPACE=my-cluster # May be empty export AWS_PROFILE= # Include the region as you like AWS_REGION= aws=aws [ \u0026#34;\u0026gt;\u0026#34;${AWS_PROFILE}\u0026#34;\u0026lt;\u0026#34; != \u0026#34;\u0026gt;\u0026lt;\u0026#34; ] \u0026amp;\u0026amp; aws=\u0026#34;aws --profile ${AWS_PROFILE}\u0026#34; Is necessary perform the policies generation, access keys and credentials.\n#!/bin/bash  source ./variables tempdir=/tmp/.$RANDOM-$RANDOM mkdir $tempdir cat \u0026lt;\u0026lt; EOF \u0026gt; \u0026#34;$tempdir/policy.json\u0026#34; { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${S3_BACKUP_BUCKET}/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${S3_BACKUP_BUCKET}\u0026#34; ] } ] } EOF { aws iam create-user --region $AWS_REGION --user-name $S3_BACKUP_BUCKET_USER \u0026gt; /dev/null aws iam put-user-policy --region $AWS_REGION --user-name $S3_BACKUP_BUCKET_USER \\ \t--policy-name $S3_BACKUP_BUCKET_POLICY_NAME \\ \t--policy-document \u0026#34;file://$tempdir/policy.json\u0026#34; \u0026gt; /dev/null aws iam create-access-key --region $AWS_REGION --user-name $S3_BACKUP_BUCKET_USER \\ \t\u0026gt; $tempdir/credentials.json aws s3 mb s3://$S3_BACKUP_BUCKET --region $AWS_REGION } \u0026amp;\u0026gt; /dev/null accessKeyId=$(jq -r \u0026#39;.AccessKey.AccessKeyId\u0026#39; \u0026#34;$tempdir/credentials.json\u0026#34;) secretAccessKey=$(jq -r \u0026#39;.AccessKey.SecretAccessKey\u0026#39; \u0026#34;$tempdir/credentials.json\u0026#34;) echo accessKeyId=$accessKeyId echo secretAccessKey=$secretAccessKey echo kubectl --namespace $CLUSTER_NAMESPACE create secret generic $S3_BACKUP_CREDENTIALS_K8S_SECRET \\ \t--from-literal=\u0026#34;accessKeyId=$accessKeyId\u0026#34; \\ \t--from-literal=\u0026#34;secretAccessKey=$secretAccessKey\u0026#34; rm $tempdir/policy.json rm $tempdir/credentials.json rmdir $tempdir Now StackGres is able to use the keys accordingly.\napiVersion: v1 kind: Secret metadata: name: aws-creds-secret type: Opaque data: accessKey: ${accessKey} secretKey: ${secretKey} EOF Finally create the SGDistributedLogs CR to enable a distributed log cluster:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: stackgres.io/v1beta1 kind: SGDistributedLogs metadata: namespace: my-cluster name: distributedlogs spec: persistentVolume: size: 50Gi EOF Notice that each CR was assigned with its own name: which you would keep to define in the cluster creation and aware StackGres about it.\nThe order of the CR creation have some relevance for the Cluster creation, i.e you need perform the access and secrets keys before create the SGDistributedLogs CR.\nBut that is not all, StackGres lets you include several initialData script to perform any operation in the cluster before start.\nIn the given example, we are creating an user to perform some queries using the k8s secret capabilities.\nkubectl -n my-cluster create secret generic pgbench-user-password-secret \\  --from-literal=pgbench-create-user-sql=\u0026#34;create user admin password \u0026#39;admin123\u0026#39;\u0026#34; As you can see, has been created a secret key and its value which will be used in the StackGres cluster creation.\nAll the necessary steps were performed to create your first StackGres Cluster, lets do it.\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: namespace: my-cluster name: cluster spec: postgresVersion: \u0026#39;12.3\u0026#39; instances: 3 sgInstanceProfile: \u0026#39;size-small\u0026#39; pods: persistentVolume: size: \u0026#39;10Gi\u0026#39; configurations: sgPostgresConfig: \u0026#39;pgconfig1\u0026#39; sgPoolingConfig: \u0026#39;poolconfig1\u0026#39; sgBackupConfig: \u0026#39;backupconfig1\u0026#39; distributedLogs: sgDistributedLogs: \u0026#39;distributedlogs\u0026#39; initialData: scripts: - name: create-pgbench-user scriptFrom: secretKeyRef: name: pgbench-user-password-secret key: pgbench-create-user-sql - name: create-pgbench-database script: | create database pgbench owner pgbench; prometheusAutobind: true nonProductionOptions: disableClusterPodAntiAffinity: true EOF Look up to the yaml into the here doc above, every CR previously being included in the right place in the SGCluster CR creation.\nAnd there is in place the script created through the secret, but StackGres includes an extra example for you, the second script show you how to run a SQL instruction directly into the yaml.\nAnother important entry to highlight in the yaml is prometheusAutobind: true. It is not enough to have the Prometheus operator installed to have monitoring, we need to enable this parameter to have monitoring as documentation indicates.\nAwesome, now you can relax and wait for the SGCluster spinning up.\nAccessing the cluster Once the cluster is up and running, we need to expose the main entrypoint port for being accessed remotely:\n WARNING: You don\u0026rsquo;t expose in production to 0.0.0.0 interface, rather than that you need to place the IP of an internal interface to be able to connect remotely within you private network.\n kubectl port-forward -n my-cluster --address 0.0.0.0 statefulset/cluster 7777:7432 In the namespace of the cluster, you should be able to see a set of secrets, we\u0026rsquo;ll get the main superuser password:\nkubectl get secrets -n my-cluster test -o jsonpath='{.data.superuser-password}' | base64 -d You should be able to connect by issuing any client application with the connection string as follows:\npsql -h \u0026lt;the ip of the cluster\u0026gt; -p 7777 -U postgres It is also possible to open a direct port-forward towards the main Postgres pod as follows:\nkubectl port-forward test-0 --address 0.0.0.0 7777:5432 "
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/prerequisites/kubernetes-environment/",
	"title": "Kubernetes environment",
	"tags": [],
	"description": "",
	"content": "To install and run StackGres you need a functional Kubernetes environment. In principle, any K8s compliant environment should work. For convenience, this section provides guidance on how to setup basic K8s environments on certain selected clouds to make it easier and reproducible to follow this tutorial.\nSpecific environment instructions:\n AWS EKS  "
},
{
	"uri": "https://stackgres.io/doc/0.9/demo/operator/install/",
	"title": "Operator installation",
	"tags": [],
	"description": "",
	"content": "Installation with kubectl We ship some kubernetes resources files in order to allow installation of the StackGres operator for demonstration purpose. Assuming you have already installed the the kubectl CLI you can install the operator with the following command:\nkubectl apply -f https://stackgres.io/downloads/stackgres-k8s/stackgres/0.9.5/stackgres-operator-demo.yml  The stackgres-operator-demo.yml will expose the UI as with a LoadBalancer. Note that enabling this feature will probably incur in some fee that depend on the host of the kubernetes cluster (for example this is true for EKS, GKE and AKS).\n To clean up the resources created by the demo just run:\nkubectl delete --ignore-not-found -f https://stackgres.io/downloads/stackgres-k8s/stackgres/0.9.5/stackgres-operator-demo.yml Installation with helm You can also install the StackGres operator using helm version 3.1.x with the following command:\nkubectl create namespace stackgres helm install --namespace stackgres stackgres-operator \\ https://stackgres.io/downloads/stackgres-k8s/stackgres/0.9.5/helm/stackgres-operator.tgz \\ --set-string adminui.service.type=LoadBalancer  The --set-string adminui.service.type=LoadBalancer will expose the UI as with a LoadBalancer. Note that enabling this feature will probably incurr in some fee that depend on the host of the kubernetes cluster (for example this is true for EKS, GKE and AKS).\n To clean up the resources created by the demo just run:\nhelm uninstall --namespace stackgres stackgres-operator Wait for the operator start Use the command below to be sure when the operation is ready to use:\nwhile [ $(kubectl get pods -n stackgres | grep -E \u0026#39;stackgres\\-(operator|restapi)\u0026#39; | grep -E \u0026#39;0/1|1/1|2/2\u0026#39; | grep -E \u0026#39;Running|Completed\u0026#39; | wc -l) -ne 3 ] ; do echo not ready... sleep 3 done Once it\u0026rsquo;s ready you will see that the two pods are Running and the create certificate job is Complete:\n➜ kubectl get pods -n stackgres NAME READY STATUS RESTARTS AGE stackgres-operator-7bfcb56dc7-c2hfs 1/1 Running 0 18m stackgres-operator-create-certificate-2fltp 0/1 Completed 0 18m stackgres-restapi-66db44f45f-l5gz4 2/2 Running 0 18m Connect to the UI To connect to the Web UI of the operator you may forward port 443 of the operator pod:\nPOD_NAME=$(kubectl get pods --namespace stackgres -l \u0026quot;app=stackgres-restapi\u0026quot; -o jsonpath=\u0026quot;{.items[0].metadata.name}\u0026quot;) kubectl port-forward \u0026quot;$POD_NAME\u0026quot; 8443:9443 --namespace stackgres Then open the browser at following address [localhost:8443/admin/]](https://localhost:8443/admin/)\nThe UI will ask for a username and a password. By default those are admin and a randomly generated password. You can run the command below to get the user and password auto-generated:\nkubectl get secret -n stackgres stackgres-restapi --template \u0026#39;username = {{ printf \u0026#34;%s\\n\u0026#34; (.data.k8sUsername | base64decode) }}password = {{ printf \u0026#34;%s\\n\u0026#34; ( .data.clearPassword | base64decode) }}\u0026#39; Changing the UI password You can use the command below to change the password:\nNEW_USER=admin NEW_PASSWORD=password kubectl create secret generic -n stackgres stackgres-restapi --dry-run=client -o json \\  --from-literal=k8sUsername=\u0026#34;$NEW_USER\u0026#34; \\  --from-literal=password=\u0026#34;$(echo -n \u0026#34;${NEW_USER}${NEW_PASSWORD}\u0026#34;| sha256sum | awk \u0026#39;{ print $1 }\u0026#39; )\u0026#34; \u0026gt; password.patch kubectl patch secret -n stackgres stackgres-restapi -p \u0026#34;$(cat password.patch)\u0026#34; \u0026amp;\u0026amp; rm password.patch Remember to remove the generated password hint from the secret to avoid security flaws:\nkubectl patch secrets --namespace stackgres stackgres-restapi --type json -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/data/clearPassword\u0026#34;}]\u0026#39;  See installation via helm section in order to change those.\n "
},
{
	"uri": "https://stackgres.io/doc/0.9/monitoring/metrics/postgres_exporter/",
	"title": "postgres_exporter",
	"tags": [],
	"description": "Contains details about the metrics collected by the postgres_exporter.",
	"content": "The next tables contains details about the metrics collected by the postgres_exporter.\nPostgres cluster metrics    item metric group metric type description     1 pg_postmaster start_time_seconds GAUGE Time at which postmaster started   2 pg_replication        lag GAUGE Replication lag behind master in seconds     is_replica GAUGE Indicates if this host is a slave   3 pg_stat_user_tables        datname LABEL Name of current database     schemaname LABEL Name of the schema that this table is in     relname LABEL Name of this table     seq_scan COUNTER Number of sequential scans initiated on this table     seq_tup_read COUNTER Number of live rows fetched by sequential scans     idx_scan COUNTER Number of index scans initiated on this table     idx_tup_fetch COUNTER Number of live rows fetched by index scans     n_tup_ins COUNTER Number of rows inserted     n_tup_upd COUNTER Number of rows updated     n_tup_del COUNTER Number of rows deleted     n_tup_hot_upd COUNTER Number of rows HOT updated (i.e., with no separate index update required)     n_live_tup GAUGE Estimated number of live rows     n_dead_tup GAUGE Estimated number of dead rows     n_mod_since_analyze GAUGE Estimated number of rows changed since last analyze     last_vacuum GAUGE Last time at which this table was manually vacuumed (not counting VACUUM FULL)     last_autovacuum GAUGE Last time at which this table was vacuumed by the autovacuum daemon     last_analyze GAUGE Last time at which this table was manually analyzed     last_autoanalyze GAUGE Last time at which this table was analyzed by the autovacuum daemon     vacuum_count COUNTER Number of times this table has been manually vacuumed (not counting VACUUM FULL)     autovacuum_count COUNTER Number of times this table has been vacuumed by the autovacuum daemon     analyze_count COUNTER Number of times this table has been manually analyzed     autoanalyze_count COUNTER Number of times this table has been analyzed by the autovacuum daemon   4 pg_statio_user_tables        datname LABEL Name of current database     schemaname LABEL Name of the schema that this table is in     relname LABEL Name of this table     heap_blks_read COUNTER Number of disk blocks read from this table     heap_blks_hit COUNTER Number of buffer hits in this table     idx_blks_read COUNTER Number of disk blocks read from all indexes on this table     idx_blks_hit COUNTER Number of buffer hits in all indexes on this table     toast_blks_read COUNTER Number of disk blocks read from this table\u0026rsquo;s TOAST table (if any)     toast_blks_hit COUNTER Number of buffer hits in this table\u0026rsquo;s TOAST table (if any)     tidx_blks_read COUNTER Number of disk blocks read from this table\u0026rsquo;s TOAST table indexes (if any)     tidx_blks_hit COUNTER Number of buffer hits in this table\u0026rsquo;s TOAST table indexes (if any)   5 pg_database        datname LABEL Name of the database     size_bytes GAUGE Disk space used by the database   6 pg_archiver        pending_wal_count GAUGE No. of pending WAL files to be archived   7 pg_stat_user_indexes        schemaname LABEL Name of the schema that this table is in     relname LABEL Name of the table for this index     indexrelname LABEL Name of this index     idx_scan COUNTER Number of index scans initiated on this index     idx_tup_read COUNTER Number of index entries returned by scans on this index     idx_tup_fetch COUNTER Number of live table rows fetched by simple index scans using this index   8 pg_statio_user_indexes        schemaname LABEL Name of the schema that this table is in     relname LABEL Name of the table for this index     indexrelname LABEL Name of this index     idx_blks_read COUNTER Number of disk blocks read from this index     idx_blks_hit COUNTER Number of buffer hits in this index   9 pg_total_relation_size        datname LABEL Database name     schemaname LABEL Name of the schema that this table is in     relname LABEL Name of this table     bytes GAUGE total disk space usage for the specified table and associated indexes   10 pg_blocked        type LABEL The lock type     datname LABEL Database name     schemaname LABEL The schema on which a query is blocked     reltype LABEL The type of relation     relname LABEL The relation on which a query is blocked     queries GAUGE The current number of blocked queries   11 pg_oldest_blocked        age_seconds GAUGE Largest number of seconds any transaction is currently waiting on a lock     datname LABEL Database name   12 pg_slow        queries GAUGE Current number of slow queries     datname LABEL Database name   13 pg_long_running_transactions        datname LABEL Database name     queries GAUGE Current number of long running transactions     age_in_seconds GAUGE The current maximum transaction age in seconds   14 pg_vacuum        datname LABEL Database name     queries GAUGE The current number of VACUUM queries     age_in_seconds GAUGE The current maximum VACUUM query age in seconds   15 pg_vacuum_analyze        datname LABEL Database name     queries GAUGE The current number of VACUUM ANALYZE queries     age_in_seconds GAUGE The current maximum VACUUM ANALYZE query age in seconds   16 pg_stuck_idle_in_transaction        datname LABEL Database name     queries GAUGE Current number of queries that are stuck being idle in transactions   17 pg_txid        current COUNTER Current 64-bit transaction id of the query used to collect this metric (truncated to low 52 bits)     xmin COUNTER Oldest transaction id of a transaction still in progress, i.e. not known committed or aborted (truncated to low 52 bits)     xmin_age GAUGE Age of oldest transaction still not committed or aborted measured in transaction ids   18 pg_database_datfrozenxid        datname LABEL Database name     age GAUGE Age of the oldest transaction that has not been frozen   19 pg_wal_position        bytes COUNTER Postgres LSN (log sequence number) being generated on primary or replayed on replica (truncated to low 52 bits)   20 pg_replication_slots        slot_name LABEL Slot Name     slot_type LABEL Slot Type     active GAUGE Boolean flag indicating whether this slot has a consumer streaming from it     xmin_age GAUGE Age of oldest transaction that cannot be vacuumed due to this replica     catalog_xmin_age GAUGE Age of oldest transaction that cannot be vacuumed from catalogs due to this replica (used by logical replication)     restart_lsn_bytes GAUGE Amount of data on in xlog that must be this replica may need to complete recovery     confirmed_flush_lsn_bytes GAUGE Amount of data on in xlog that must be this replica has not yet received   21 pg_stat_ssl        pid LABEL Process ID of a backend or WAL sender process     active GAUGE Boolean flag indicating if SSL is used on this connection     bits GAUGE Number of bits in the encryption algorithm is in use     compression GAUGE Boolean flag indicating if SSL compression is in use   22 pg_table_bloat        datname LABEL Database name     schemaname LABEL Schema name     tablename LABEL Table name     real_size GAUGE Table real size     extra_size GAUGE Estimated extra size not used/needed in the table. This extra size is composed by the fillfactor, bloat and alignment padding spaces     extra_ratio GAUGE Estimated ratio of the real size used by extra_size     fillfactor GAUGE Table fillfactor     bloat_size GAUGE Estimated size of the bloat without the extra space kept for the fillfactor     bloat_ratio GAUGE Estimated ratio of the real size used by bloat_size     is_na GAUGE Estimation not aplicable, If true, do not trust the stats   23 pg_index        datname LABEL Database name     schema_name LABEL Schema name     tblname LABEL Table name     idxname LABEL Index Name     real_size GAUGE Index size     extra_size GAUGE Index extra size     extra_ratio GAUGE Index extra ratio     fillfactor GAUGE Fillfactor     bloat_size GAUGE Estimate index bloat size     bloat_ratio GAUGE Estimate index bloat size ratio     is_na GAUGE Estimate Not aplicable, bad statistic   24 pg_replication_status        application_name LABEL Application or node name     client_addr LABEL Client ip address     state LABEL Client replication state     lag_size_bytes GAUGE Replication lag size in bytes    Connection pooling metrics    item metric group metric type description     1 pgbouncer_show_clients        type LABEL C, for client     user LABEL Client connected user     database LABEL Database name     state LABEL State of the client connection, one of active or waiting     addr LABEL IP address of client     port GAUGE Port client is connected to     local_addr LABEL Connection end address on local machine     local_port GAUGE Connection end port on local machine     connect_time LABEL Timestamp of connect time     request_time LABEL Timestamp of latest client request     wait GAUGE Current waiting time in seconds     wait_us GAUGE Microsecond part of the current waiting time     close_needed GAUGE not used for clients     ptr LABEL Address of internal object for this connection. Used as unique ID     link LABEL Address of server connection the client is paired with     remote_pid GAUGE Process ID, in case client connects over Unix socket and OS supports getting it     tls LABEL A string with TLS connection information, or empty if not using TLS   2 pgbouncer_show_pools        database LABEL Database name     user LABEL User name     cl_active GAUGE Client connections that are linked to server connection and can process queries     cl_waiting GAUGE Client connections that have sent queries but have not yet got a server connection     sv_active GAUGE Server connections that are linked to a client     sv_idle GAUGE Server connections that are unused and immediately usable for client queries     sv_used GAUGE Server connections that have been idle for more than server_check_delay so they need server_check_query to run on them     sv_tested GAUGE Server connections that are currently running either server_reset_query or server_check_query     sv_login GAUGE Server connections currently in the process of logging in     maxwait GAUGE How long the first oldest client in the queue has waited, in seconds     maxwait_us GAUGE Microsecond part of the maximum waiting time     pool_mode LABEL The pooling mode in use   3 pgbouncer_show_databases        name LABEL Name of configured database entry     host LABEL Host pgbouncer connects to     port GAUGE Port pgbouncer connects to     database LABEL Actual database name pgbouncer connects to     force_user LABEL When the user is part of the connection string the connection between pgbouncer and PostgreSQL is forced to the given user     pool_size GAUGE Maximum number of server connections     reserve_pool GAUGE Maximum number of additional connections for this database     pool_mode LABEL The database override pool_mode     max_connections GAUGE Maximum number of allowed connections for this database     current_connections GAUGE Current number of connections for this database     paused GAUGE 1 if this database is currently paused, else 0     disabled GAUGE 1 if this database is currently disabled, else 0   4 pgbouncer_show_stats_totals        database LABEL Database name     xact_count GAUGE Number of SQL transactions pooled     query_count GAUGE Number of SQL queries pooled     bytes_received GAUGE Volume in bytes of network traffic received     bytes_sent GAUGE Volume in bytes of network traffic sent     xact_time GAUGE Number of microseconds spent by pgbouncer when connected to PostgreSQL in a transaction     query_time GAUGE Number of microseconds spent by pgbouncer when actively connected to PostgreSQL   5 pgbouncer_show_stats        database LABEL Database name     total_xact_count GAUGE Total number of SQL transactions pooled     total_query_count GAUGE Total number of SQL queries pooled     total_received GAUGE Total volume in bytes of network traffic received     total_sent GAUGE Total volume in bytes of network traffic sent     total_xact_time GAUGE Total number of microseconds spent by pgbouncer when connected to PostgreSQL in a transaction     total_query_time GAUGE Total number of microseconds spent by pgbouncer when actively connected to PostgreSQL     total_wait_time GAUGE Time spent by clients waiting for a server, in microseconds     avg_xact_count GAUGE Average transactions per second in last stat period     avg_query_count GAUGE Average queries per second in last stat period     avg_recv GAUGE Average received from clients bytes per second     avg_sent GAUGE Average sent to clients bytes per second     avg_xact_time GAUGE Average transaction duration, in microseconds     avg_query_time GAUGE Average query duration, in microseconds     avg_wait_time GAUGE Time spent by clients waiting for a server, in microseconds average per second    "
},
{
	"uri": "https://stackgres.io/doc/0.9/api/rbac/",
	"title": "RBAC Authorization Overview",
	"tags": [],
	"description": "",
	"content": "Authentication In Kubernetes, you must be authenticated (logged in) before your request can be authorized (granted permission to access). The same applies to the Web UI of StackGres, you can create users to authenticate against a Kubernetes using a special Secret designed for that purpose to log in in the Web UI.\nThe data that contains the secret must be in base64 format, and the password should be the concatenation of the api username plus the password itself.\napiVersion: v1 kind: Secret metadata: name: webapi-user-demo namespace: stackgres labels: api.stackgres.io/auth: user type: Opaque data: apiUsername: \u0026#34;demo@example.com | b64enc\u0026#34; k8sUsername: \u0026#34;username | b64enc\u0026#34; password: \u0026#34;{{ user + password | sha256sum | b64enc }}\u0026#34; You might wonder why are two username fields in the secret, the apiUsername is optional and is used to \u0026ldquo;customize\u0026rdquo; the username used for the login Web UI, the k8sUsername is the username that is used to impersonate the API calls to K8s.\nUsing RBAC Authorization Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization.\nRBAC authorization uses the rbac.authorization.k8s.io API group to drive authorization decisions, allowing you to dynamically configure policies through the Kubernetes API.\n Kubernetes supports others authorizations modes like Attribute-based access control (ABAC), but the RBAC mode must be enabled in Kubernetes for this to work.\n Kubernetes authorizes API requests using the API server. It evaluates all of the request attributes against all policies and allows or denies the request. All parts of an API request must be allowed by some policy in order to proceed. This means that permissions are denied by default.\nAPI objects The RBAC API declares four kinds of Kubernetes object: Role, ClusterRole, RoleBinding and ClusterRoleBinding. You can describe objects, or amend them, using tools such as kubectl, just like any other Kubernetes object.\n Please check https://kubernetes.io/docs/reference/access-authn-authz/rbac/#api-overview for a comprenhensive description on how it works.\n ClusterRole An RBAC ClusterRole contains rules that represent a set of permissions. Permissions are purely additive (there are no \u0026ldquo;deny\u0026rdquo; rules).\nStackGres handles different namespaces, so for the moment a ClusterRole is required to properly work.\nClusterRole example The following example shows a ClusterRole with basic permisions for read stackgres resources:\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: stackgres-reader rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - namespaces - pods - secrets verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;storage.k8s.io\u0026#34;] resources: - storageclasses verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;apiextensions.k8s.io\u0026#34;] resources: - customresourcedefinitions verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;stackgres.io\u0026#34;] resources: - sgclusters - sgpgconfigs - sgbackupconfigs - sgbackups - sgdistributedlogs - sginstanceprofiles - sgpoolconfigs verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] ClusterRoleBinding A role binding grants the permissions defined in a role to a user or set of users.\nThe following example \u0026ldquo;binds\u0026rdquo; the previous stackgres-reader ClusterRole to the example user:\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: creationTimestamp: \u0026#34;2020-07-15T16:36:22Z\u0026#34; name: sg-restapi-example-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: stackgres-reader subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: example The same can be archived with: kubectl create clusterrolebinding sg-restapi-example-user --clusterrole=stackgres-reader --user=example\n Please note that the example user must also be mapped in the secret with a password to be able to login to the Web UI.\n Determine the Request Verb The following is a table with the resources of StackGres that can be used for defining the ClusterRole:\n   Resource API Group Verbs     sgclusters stackgres.io get, list, create, update, patch, delete   sgpgconfigs stackgres.io get, list, create, update, patch, delete   sginstanceprofiles stackgres.io get, list, create, update, patch, delete   sgbackups stackgres.io get, list, create, update, patch, delete   sgbackupconfigs stackgres.io get, list, create, update, patch, delete   sgdistributedlogs stackgres.io get, list, create, update, patch, delete   sgpoolconfigs stackgres.io get, list, create, update, patch, delete   customresourcedefinitions apiextensions.k8s.io get, list   namespaces  get, list   pods  get, list   secrets  get, list   storageclasses storage.k8s.io get, list    This is not an exhaustive list, but it should help to get started.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/reference/crd/sginstanceprofile/",
	"title": "SGInstanceProfile",
	"tags": [],
	"description": "Details about SGInstanceProfile configurations",
	"content": "The instance profile CR represent the CPU and memory resources assigned to each Pod of the cluster.\n Kind: SGInstanceProfile\nlistKind: SGInstanceProfileList\nplural: sginstanceprofiles\nsingular: sginstanceprofile\n Spec\n   Property Required Updatable Default Type Description     cpu  ✓ 1 string CPU(s) (cores) used for every instance of a SGCluster. Please note that every StackGres pod contains not only the Patroni+Postgres container, but several other sidecar containers. While the majority of the resources are devoted to the main Postgres container, some CPU is needed for the sidecars.\nThe number of cores set is split between all the containers.\nA minimum of 2 cores is recommended.\n    memory  ✓ 2Gi string RAM allocated to every instance of a SGCluster. The suffix Mi or Gi specifies Mebibytes or Gibibytes, respectively. Please note that every StackGres pod contains not only the Patroni+Postgres container, but several other sidecar containers. While the majority of the resources are devoted to the main Postgres container, some RAM is needed for the sidecars.\nThe amount of RAM set is split between all the containers.\nA minimum of 2-4Gi is recommended.\n     Example:\napiVersion: stackgres.io/v1beta1 kind: SGInstanceProfile metadata: name: size-l spec: cpu: \u0026#34;4\u0026#34; memory: 8Gi "
},
{
	"uri": "https://stackgres.io/doc/0.9/install/integrations/nutanix-karbon/cluster-deploy/",
	"title": "StackGres Cluster Deploy",
	"tags": [],
	"description": "",
	"content": "StackGres Cluster Deploy The StackGres Operator and RestApi have been installed with success and the web access is ready, now you can proceed with the StackGres Cluster deployment.\nThe cluster could be created with default parameters, but to get the most of this, several resources will be created to show the versatility of StackGres. You can open and inspect the YAML files to understand the parameters of the resources following the StackGres Documentation.\nLets create the cluster starting with a custom profile for instances.\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: stackgres.io/v1beta1 kind: SGInstanceProfile metadata: namespace: karbon name: size-s spec: cpu: \u0026#34;500m\u0026#34; memory: \u0026#34;512Mi\u0026#34; EOF Create a Postgres custom configuration:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: stackgres.io/v1beta1 kind: SGPostgresConfig metadata: namespace: karbon name: pgconfig spec: postgresVersion: \u0026#34;12\u0026#34; postgresql.conf: shared_buffers: \u0026#39;256MB\u0026#39; random_page_cost: \u0026#39;1.5\u0026#39; password_encryption: \u0026#39;scram-sha-256\u0026#39; checkpoint_timeout: \u0026#39;30\u0026#39; max_connections: \u0026#39;100\u0026#39; jit: \u0026#39;off\u0026#39; EOF Create a specific pooling configuration:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: stackgres.io/v1beta1 kind: SGPoolingConfig metadata: namespace: karbon name: poolconfig spec: pgBouncer: pgbouncer.ini: pool_mode: transaction max_client_conn: \u0026#39;2000\u0026#39; default_pool_size: \u0026#39;50\u0026#39; log_connections: \u0026#39;1\u0026#39; log_disconnections: \u0026#39;1\u0026#39; log_stats: \u0026#39;1\u0026#39; EOF And create a resource for Distributed logs:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: stackgres.io/v1beta1 kind: SGDistributedLogs metadata: name: distributedlogs namespace: karbon spec: persistentVolume: size: 50Gi EOF Backups StackGres support Backups with the following storage options - AWS S3 - S3 Compatible Storage - Google Cloud Storage - Azure Blob Storage\nDepending on the storage you choose check the StackGres backups Documentation to verify the params according to your choice. For the purpose an S3 Compatible Storage (Minio) will be configured:\nCreate minio configuration:\nkubectl apply -f https://gitlab.com/ongresinc/stackgres-tutorial/-/blob/master/sg_demo_karbon/07-minio.yaml Create the backups configuration:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: stackgres.io/v1beta1 kind: SGBackupConfig metadata: name: backupconfig namespace: karbon spec: storage: type: s3Compatible s3Compatible: bucket: stackgres region: k8s enablePathStyleAddressing: true endpoint: http://minio:9000 awsCredentials: secretKeySelectors: accessKeyId: key: accesskey name: minio secretAccessKey: key: secretkey name: minio EOF Create a k8s secret with the SQL sentence to create the some user\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - kubectl -n karbon create secret generic admin-user-password --from-literal=admin-create-user-sql=\u0026#34;create user admin password \u0026#39;admin\u0026#39;\u0026#34; All the above configuration resources will be used to create an SGCLuster:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: namespace: karbon name: karbon-db spec: postgresVersion: \u0026#39;12.3\u0026#39; instances: 3 sgInstanceProfile: \u0026#39;size-s\u0026#39; pods: persistentVolume: size: \u0026#39;20Gi\u0026#39; configurations: sgPostgresConfig: \u0026#39;pgconfig\u0026#39; sgPoolingConfig: \u0026#39;poolconfig\u0026#39; sgBackupConfig: \u0026#39;backupconfig\u0026#39; distributedLogs: sgDistributedLogs: \u0026#39;distributedlogs\u0026#39; initialData: scripts: - name: create-admin-user scriptFrom: secretKeyRef: name: admin-user-password key: admin-create-user-sql - name: create-database script: | create database admin owner admin; prometheusAutobind: true nonProductionOptions: disableClusterPodAntiAffinity: true EOF As you can see, we included the initialData section, which give us the option to run our custom scripts, or SQL commands. Now the PostgreSQL cluster could be inspected and monitored through the web console or the kubectl CLI as you wish.\n# kubectl get pods -n karbon NAME READY STATUS RESTARTS AGE distributedlogs-0 2/2 Running 0 10m karbon-db-0 6/6 Running 0 2m40s karbon-db-1 6/6 Running 0 2m7s karbon-db-2 6/6 Running 0 96s The StackGres Cluster installation could be verified using the next commands. It will show the PostgreSQL instances in the cluster and the postgres version installed.\nkubectl exec -it -n demo-karbon karbon-db-0 -c patroni -- patronictl list kubectl exec -it -n demo-karbon karbon-db-0 -c postgres-util -- psql -c \u0026#34;select version()\u0026#34; Summary. StackGres Instllation and Cluster deploy are ready to work on a Nutanix Karbon environment as it was shown with the examples above. All components from StackGres can be executed, configured and all the features work as expected.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/stackgres-installation/",
	"title": "StackGres Installation",
	"tags": [],
	"description": "",
	"content": "StackGres (the operator and associated components) may be installed on any namespace. It is recommended to create a dedicated namespace for StackGres:\nkubectl create namespace stackgres StackGres recommended installation is performed from the published Helm chart. The following command will install StackGres with Helm3, allow Grafana integration with StackGres Web Console, and exposing that Web Console via a load balancer:\nexport SG_VERSION=1.0.0-alpha1 helm install --namespace stackgres stackgres-operator \\ \t--set grafana.autoEmbed=true \\  --set-string grafana.webHost=prometheus-grafana.monitoring \\  --set-string grafana.user=admin \\  --set-string grafana.password=prom-operator \\  --set-string adminui.service.type=LoadBalancer \\  https://stackgres.io/downloads/stackgres-k8s/stackgres/$SG_VERSION/helm/stackgres-operator.tgz Please refer to Helm chart parameters for further customization of the above Helm parameters. The grafana parameters above contain the default values for the installation detailed on the Prometheus and Grafana prerequisite section section. Replace them for your custom installation parameters, if needed.\nNote that using adminui.service.type=LoadBalancer will create a network load balancer, which may incur in additional costs. You may alternatively use ClusterIP if that\u0026rsquo;s your preference.\nStackGres installation may take a few minutes. The output will be similar to:\nNAME: stackgres-operator LAST DEPLOYED: Mon Mar 1 00:25:10 2021 NAMESPACE: stackgres STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Release Name: stackgres-operator StackGres Version: 1.0.0-alpha1 _____ _ _ _____ / ____| | | | / ____| | (___ | |_ __ _ ___| | _| | __ _ __ ___ ___ \\___ \\| __/ _` |/ __| |/ / | |_ | \u0026#39;__/ _ \\/ __| ____) | || (_| | (__| \u0026lt;| |__| | | | __/\\__ \\ |_____/ \\__\\__,_|\\___|_|\\_\\\\_____|_| \\___||___/ by OnGres, Inc. Check if the operator was successfully deployed and is available: kubectl describe deployment -n stackgres stackgres-operator kubectl wait -n stackgres deployment/stackgres-operator --for condition=Available Check if the restapi was successfully deployed and is available: kubectl describe deployment -n stackgres stackgres-restapi kubectl wait -n stackgres deployment/stackgres-restapi --for condition=Available To access StackGres Operator UI from localhost, run the below commands: POD_NAME=$(kubectl get pods --namespace stackgres -l \u0026#34;app=stackgres-restapi\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl port-forward \u0026#34;$POD_NAME\u0026#34; 8443:9443 --namespace stackgres Read more about port forwarding here: http://kubernetes.io/docs/user-guide/kubectl/kubectl_port-forward/ Now you can access the StackGres Operator UI on: https://localhost:8443 To get the username, run the command: kubectl get secret -n stackgres stackgres-restapi --template \u0026#39;{{ printf \u0026#34;username = %s\\n\u0026#34; (.data.k8sUsername | base64decode) }}\u0026#39; To get the generated password, run the command: kubectl get secret -n stackgres stackgres-restapi --template \u0026#39;{{ printf \u0026#34;password = %s\\n\u0026#34; (.data.clearPassword | base64decode) }}\u0026#39; Remember to remove the generated password hint from the secret to avoid security flaws: kubectl patch secrets --namespace stackgres stackgres-restapi --type json -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/data/clearPassword\u0026#34;}]\u0026#39; Several useful commands are provided as part of the Helm installation output. Let\u0026rsquo;s use them to connect to the StackGres Web Console. If the LoadBalancer parameter was used, let\u0026rsquo;s query the URL of the created load balancer, by querying the K8s Service created:\nkubectl -n stackgres get svc --field-selector metadata.name=stackgres-restapi Note: StackGres deploys the Web Console with the service of the Rest API, hence the query above.\n NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE stackgres-restapi LoadBalancer 10.100.194.154 aa82c8ec1082142cba68d9f19980478d-2039466138.us-east-2.elb.amazonaws.com 443:30010/TCP 20m  Note the external IP (probably a DNS name, like above) and use it prepended by https:// to access the Web Console. By default StackGres would have used a self-signed SSL certificate, so it is expected that you will be presented with a warning from the web browser. Accept and continue to the login dialogue.\nYou can get the username and password for the Web Console from the commands output from StackGres Helm chart installation. Unless you changed it via helm parameters, the username will be admin. The password is dynamically generated and can be queried via the following command (pipe or redirect if on a non-public environment):\nkubectl get secret -n stackgres stackgres-restapi --template \u0026#39;{{ print (.data.clearPassword | base64decode) }}\u0026#39; You should see a web UI similar to:\nIf you prefer, it supports dark mode. Just click the icon on the top right bar.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/runbooks/volume-downsize/",
	"title": "Volume downsize",
	"tags": [],
	"description": "Steps about how to perform a volume downsize",
	"content": "This runbook will show you how to perform a volume downsize. The normal operation is to extend a volume but in some cases you probably over-dimension your volumes and you might need to reduce cost and resource usage so you\u0026rsquo;ll need to downsize your resources, in this case, your volumes.\nScenario Having a StackGres cluster with:\n Instances: 3 Namespace: ongres-db Cluster name: ongres-db Volume size: 20Gi  $ kubectl exec -it -n ongres-db ongres-db-2 -c patroni -- patronictl list + Cluster: ongres-db (6918002883456245883) -------+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +-------------+----------------+--------+---------+----+-----------+ | ongres-db-0 | 10.0.7.11:7433 | Leader | running | 3 | | | ongres-db-1 | 10.0.0.10:7433 | | running | 3 | 0 | | ongres-db-2 | 10.0.6.9:7433 | | running | 3 | 0 | +-------------+----------------+--------+---------+----+-----------+ Verifying the PVC\u0026rsquo;s:\n$ kubectl get pvc -n ongres-db NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE distributedlogs-data-distributedlogs-0 Bound pvc-9bab7a68-a209-4d9a-93f7-871a217a28b1 50Gi RWO standard 162m ongres-db-data-ongres-db-0 Bound pvc-a2aa5198-c553-4e0d-a1e1-914669abb69f 20Gi RWO gp2-data 11m ongres-db-data-ongres-db-1 Bound pvc-c724b2bf-cf17-4f57-a882-3a5da6947f44 20Gi RWO gp2-data 10m ongres-db-data-ongres-db-2 Bound pvc-5124b9d2-ec35-46d7-9eda-7543d9ed7148 20Gi RWO gp2-data 4m47s Assuming the disk size is over-dimensioned, and you need to perform a downsize to 15Gi.\nPerform a switchover to the pod with higher index number ( ongres-db-2) Execute:\nkubectl exec -it -n ongres-db ongres-db-0 -c patroni -- patronictl switchover\nMaster [ongres-db-0]: Candidate ['ongres-db-1', 'ongres-db-2'] []: ongres-db-2 When should the switchover take place (e.g. 2021-01-15T16:40 ) [now]: Current cluster topology + Cluster: ongres-db (6918002883456245883) -------+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +-------------+----------------+--------+---------+----+-----------+ | ongres-db-0 | 10.0.7.11:7433 | Leader | running | 3 | | | ongres-db-1 | 10.0.0.10:7433 | | running | 3 | 0 | | ongres-db-2 | 10.0.6.9:7433 | | running | 3 | 0 | +-------------+----------------+--------+---------+----+-----------+ Are you sure you want to switchover cluster ongres-db, demoting current master ongres-db-0? [y/N]:y 2021-01-15 15:41:11.93457 Successfully switched over to \u0026quot;ongres-db-2\u0026quot; + Cluster: ongres-db (6918002883456245883) -------+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +-------------+----------------+--------+---------+----+-----------+ | ongres-db-0 | 10.0.7.11:7433 | | stopped | | unknown | | ongres-db-1 | 10.0.0.10:7433 | | running | 3 | 0 | | ongres-db-2 | 10.0.6.9:7433 | Leader | running | 3 | | +-------------+----------------+--------+---------+----+-----------+ Now check the cluster state:\n$ kubectl exec -it -n ongres-db ongres-db-2 -c patroni -- patronictl list + Cluster: ongres-db (6918002883456245883) -------+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +-------------+----------------+--------+---------+----+-----------+ | ongres-db-0 | 10.0.7.11:7433 | | running | 4 | 0 | | ongres-db-1 | 10.0.0.10:7433 | | running | 4 | 0 | | ongres-db-2 | 10.0.6.9:7433 | Leader | running | 4 | | +-------------+----------------+--------+---------+----+-----------+ Edit the SGCluster definition with the new size Update the volume size to with the new value:\nkubectl patch sgclusters -n ongres-db ongres-db --type='json' -p '[{ \u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/pods/persistentVolume/size\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;10Gi\u0026quot; }]' You\u0026rsquo;ll get the next message:\nsgcluster.stackgres.io/ongres-db patched Now if you check the events you will see an error like:\nkubectl get events -n ongres-db .... Failure executing: PATCH at: https://10.96.0.1/apis/apps/v1/namespaces/ongres-db/statefulsets/ongres-db. Message: StatefulSet.apps \u0026quot;ongres-db\u0026quot; is invalid: spec: Forbidden: updates to statefulset spec for fields other than 'replicas', 'template', and 'updateStrategy' are forbidden. Received status: Status(apiVersion=v1, code=422, details=StatusDetails(causes=[StatusCause(field=spec, message=Forbidden: updates to statefulset spec for fields other than 'replicas', 'template', and 'updateStrategy' are forbidden, reason=FieldValueForbidden, additionalProperties={})], group=apps, kind=StatefulSet, name=ongres-db, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=StatefulSet.apps \u0026quot;ongres-db\u0026quot; is invalid: spec: Forbidden: updates to statefulset spec for fields other than 'replicas', 'template', and 'updateStrategy' are forbidden, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Invalid, status=Failure, additionalProperties={}). .... This is expected because is forbidden to change the size value into a StatefulSet, so proceed to delete the statefulset and let the StackGres Operator recreate it.\n$ kubectl delete sts -n ongres-db ongres-db --cascade=orphan  Very important note: Do not forget the paramater --cascade=orphan because this will keep the existent pods.\n Verify the StatefulSet has the new volume size: $ kubectl describe sts -n ongres-db ongres-db | grep -i capacity Capacity: 15Gi Edit the replica size to 1 (One) $ kubectl patch sgclusters -n ongres-db ongres-db --type='json' -p '[{ \u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/instances\u0026quot;, \u0026quot;value\u0026quot;: 1 }]' Once you decrease the replicas you\u0026rsquo;ll see something like:\n$ kubectl get pods -n ongres-db NAME READY STATUS RESTARTS AGE distributedlogs-0 2/2 Running 0 3h4m ongres-db-2 6/6 Running 0 27m Delete the non use PVCs and PVs Proceed to delete the PVCs ongres-db-data-ongres-db-0 and ongres-db-data-ongres-db-1:\n$ kubectl delete pvc -n ongres-db ongres-db-data-ongres-db-0 persistentvolumeclaim \u0026quot;ongres-db-data-ongres-db-0\u0026quot; deleted $ kubectl delete pvc -n ongres-db ongres-db-data-ongres-db-1 persistentvolumeclaim \u0026quot;ongres-db-data-ongres-db-1\u0026quot; deleted This will release the PV and then you can proceed to delete them:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-5124b9d2-ec35-46d7-9eda-7543d9ed7148 20Gi RWO Retain Bound ongres-db/ongres-db-data-ongres-db-2 gp2-data 32m pvc-9bab7a68-a209-4d9a-93f7-871a217a28b1 50Gi RWO Delete Bound ongres-db/distributedlogs-data-distributedlogs-0 standard 3h10m pvc-a2aa5198-c553-4e0d-a1e1-914669abb69f 20Gi RWO Retain Released ongres-db/ongres-db-data-ongres-db-0 gp2-data 39m pvc-c724b2bf-cf17-4f57-a882-3a5da6947f44 20Gi RWO Retain Released ongres-db/ongres-db-data-ongres-db-1 gp2-data 38m Delete the disks with Released state:\n$ kubectl delete pv pvc-a2aa5198-c553-4e0d-a1e1-914669abb69f persistentvolume \u0026quot;pvc-a2aa5198-c553-4e0d-a1e1-914669abb69f\u0026quot; deleted and\n$ kubectl delete pv pvc-c724b2bf-cf17-4f57-a882-3a5da6947f44 persistentvolume \u0026quot;pvc-c724b2bf-cf17-4f57-a882-3a5da6947f44\u0026quot; deleted Increase the replica size to 2 (Two): $ kubectl patch sgclusters -n ongres-db ongres-db --type='json' -p '[{ \u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/instances\u0026quot;, \u0026quot;value\u0026quot;: 2 }]' Now you cluster will have 2 pods:\n$ kubectl get pods -n ongres-db NAME READY STATUS RESTARTS AGE distributedlogs-0 2/2 Running 0 3h15m ongres-db-0 6/6 Running 0 49s ongres-db-2 6/6 Running 0 37m Check again the cluster state:\n$ kubectl exec -it -n ongres-db ongres-db-2 -c patroni -- patronictl list + Cluster: ongres-db (6918002883456245883) -------+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +-------------+----------------+--------+---------+----+-----------+ | ongres-db-0 | 10.0.7.12:7433 | | running | 4 | 0 | | ongres-db-2 | 10.0.6.9:7433 | Leader | running | 4 | | +-------------+----------------+--------+---------+----+-----------+ And the new pod will have the new disk size:\n$ kubectl get pvc -n ongres-db NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE distributedlogs-data-distributedlogs-0 Bound pvc-9bab7a68-a209-4d9a-93f7-871a217a28b1 50Gi RWO standard 3h17m ongres-db-data-ongres-db-0 Bound pvc-37d96872-b132-4a89-a579-d87f8cf1fa92 15Gi RWO gp2-data 2m47s ongres-db-data-ongres-db-2 Bound pvc-5124b9d2-ec35-46d7-9eda-7543d9ed7148 20Gi RWO gp2-data 39m Perform a switchover to node ongres-db-0 $ kubectl exec -it -n ongres-db ongres-db-2 -c patroni -- patronictl switchover Master [ongres-db-2]: Candidate ['ongres-db-0'] []: ongres-db-0 When should the switchover take place (e.g. 2021-01-15T17:12 ) [now]: Current cluster topology + Cluster: ongres-db (6918002883456245883) -------+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +-------------+----------------+--------+---------+----+-----------+ | ongres-db-0 | 10.0.7.12:7433 | | running | 4 | 0 | | ongres-db-2 | 10.0.6.9:7433 | Leader | running | 4 | | +-------------+----------------+--------+---------+----+-----------+ Are you sure you want to switchover cluster ongres-db, demoting current master ongres-db-2? [y/N]: y 2021-01-15 16:12:57.14561 Successfully switched over to \u0026quot;ongres-db-0\u0026quot; + Cluster: ongres-db (6918002883456245883) -------+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +-------------+----------------+--------+---------+----+-----------+ | ongres-db-0 | 10.0.7.12:7433 | Leader | running | 4 | | | ongres-db-2 | 10.0.6.9:7433 | | stopped | | unknown | +-------------+----------------+--------+---------+----+-----------+ This will delete the pod ongres-db-2 and create the pod ongres-db-1\nNAME READY STATUS RESTARTS AGE distributedlogs-0 2/2 Running 0 3h19m ongres-db-0 6/6 Running 0 4m51s ongres-db-1 6/6 Running 0 41s You can proceed to delete the PVC and PV of ongres-db-2\n$ kubectl delete pvc -n ongres-db ongres-db-data-ongres-db-2 persistentvolumeclaim \u0026quot;ongres-db-data-ongres-db-2\u0026quot; deleted and\n$ kubectl delete pv pvc-5124b9d2-ec35-46d7-9eda-7543d9ed7148 persistentvolume \u0026quot;pvc-5124b9d2-ec35-46d7-9eda-7543d9ed7148\u0026quot; deleted Now your cluster will have the new reduce disk size:\n$ kubectl get pvc -n ongres-db NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE distributedlogs-data-distributedlogs-0 Bound pvc-9bab7a68-a209-4d9a-93f7-871a217a28b1 50Gi RWO standard 3h24m ongres-db-data-ongres-db-0 Bound pvc-37d96872-b132-4a89-a579-d87f8cf1fa92 15Gi RWO gp2-data 9m21s ongres-db-data-ongres-db-1 Bound pvc-46c1433b-26e8-422c-aecf-145b1bb5aac1 15Gi RWO gp2-data 5m11s "
},
{
	"uri": "https://stackgres.io/doc/0.9/intro/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "The Operator An Operator is a method of packaging, deploying and managing a Kubernetes application. Some applications, such as databases, required more hand-holding, and a cloud-native Postgres requires an operator to provide additional knowledge of how to maintain state and integrate all the components. The StackGres operator allow to deploy a StackGres cluster using a few custom resources created by the user.\nOperator availability concerns Operator availability only affect operational plane, data plane is not affected at all since the database will work as expected when the operator is offline. There is a best effort in maintaining the operator available. It is expected that if at some point the operator is unavailable and this could lead to unavailability of following operational aspects:\n Cluster creation / update Cluster configuration creation / update / deletion Backups generation Reconciliation of modified resources controlled by the operator (when modified by the user or some other means)  Operator unavailability does not affect following functional aspects:\n Database high availability Connection pooling Incremental backups Stats collection  The Cluster A StackGres cluster is basically a StatefulSet where each pod is a database instance. The StatefulSet guarantees that each pod is always bind to its own persistent volume therefore the database instance data will be mapped to the state of a patroni instance inside kubernetes.\nStackGres Cluster Architecture diagram StackGres Pod Architecture diagram We use a pattern called sidecar where a main application run in a container and other container are providing a side functionality like connection pooling, export of stats, edge proxying, logging dispatcher or database utilities.\n UDS: Unix Domain Socket\n "
},
{
	"uri": "https://stackgres.io/doc/0.9/developer/stackgres/build/operator/own/",
	"title": "Building your own StackGres operator containers",
	"tags": [],
	"description": "",
	"content": "Build stackgres images are fairly simple. You can build in two forms:\n build using zulu JVM as a base image (Default) build using ubi8 as a base image  Building stackgres zulu based image Compiles fast but has a higher memory footprint. So it useful for local environment testing\nGo to the stackgres/stackgres-k8s/src folder and the execute:\n./mvnw clean verify -P build-image-jvm You can find the image in th stackgres/operator repository tagged as development-jvm\nBuilding stackgres native image Takes long to compile but has a lower memory footprint. Is recommended for production workloads.\nThe native image use GraalVM but at the expense of some limitations. So once your development is ready, be sure to test it with the native image.\nTo generate a native image simpleme run:\n./mvnw clean verify -P native,build-image-native "
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/simple-cluster/",
	"title": "Create a simple cluster",
	"tags": [],
	"description": "",
	"content": "StackGres is operated via CRDs (here is the full CRD Referece). As such, you don\u0026rsquo;t need to install any separate tool or kubectl plugin, and it allows you to easily create GitOps workflows. StackGres abstracts away all the internals and complexity of creating a complete Postgres cluster with high availability, connection pooling, tuned parameters by default, monitoring, and many others; and exposes a very simple, high level interface as part of its CRDs.\nThe main CRD that drives Postgres cluster creation is called SGCluster. Here we will create a simple one, which will include several values and parameters by default. The next section will create a more advanced cluster, while keeping the simplicity and hiding away the Postgres expertise required.\nCreate the file simple-cluster.yaml:\napiVersion: stackgres.io/v1 kind: SGCluster metadata: name: simple spec: instances: 2 postgresVersion: \u0026#39;latest\u0026#39; pods: persistentVolume: size: \u0026#39;5Gi\u0026#39; And deploy to Kubernetes:\nkubectl apply -f simple-cluster.yaml This operation will take some around a minute. At the end, you will be able to see two instances (pods) created on the default namespace:\nkubectl get pods NAME READY STATUS RESTARTS AGE simple-0 5/5 Running 0 3m21s simple-1 5/5 Running 0 2m5s You can also check the pod creation from the Web Console:\nTo connect to the cluster you have several options:\n  Run the Postgres client psql locally to the pod. One of the StackGres pod\u0026rsquo;s containers is called postgresql-util, and contains administrative tools. Run the command:\nkubectl exec -it simple-0 -c postgres-util -- psql psql (12.4 OnGres Inc.) Type \u0026#34;help\u0026#34; for help. postgres=#   Via the exposed Kubernetes service. Services are named ${cluster_name}-[primary|replicas]. The -primary service always connect to the primary (read-write) node, and the -replicas service to the load-balanced set of replicas (read-only). We may create any pod that contains a Postgres client, and then connect using the service as a DNS for the Postgres host. We will also need to get the generated superuser password:\nkubectl get secret simple --template \u0026#39;{{ (index .data \u0026#34;superuser-password\u0026#34; | base64decode) }}\u0026#39; kubectl run psql --image postgres --env=\u0026#34;POSTGRES_PASSWORD=whatever\u0026#34; kubectl exec -it psql -- psql -h simple-primary postgres postgres Cleanup:\nkubectl delete pod psql   When connecting via the postgres-util, how do we know which pod is the primary? StackGres uses Patroni for high availability. The main pod\u0026rsquo;s container (called patroni) contains the Patroni CLI, which can be used to query the status of the cluster and the primary (\u0026ldquo;leader\u0026rdquo; in Patroni\u0026rsquo;s terminology) node:\nkubectl exec -t simple-0 -c patroni -- patronictl list + Cluster: simple (6934472607315959877) --+---------+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +----------+---------------------+--------+---------+----+-----------+ | simple-0 | 192.168.7.119:7433 | Leader | running | 1 | | | simple-1 | 192.168.73.238:7433 | | running | 1 | 0 | +----------+---------------------+--------+---------+----+-----------+ Finally, let\u0026rsquo;s delete the cluster, we will be creating more advanced ones in the next section:\nkubectl delete -f simple-cluster.yaml "
},
{
	"uri": "https://stackgres.io/doc/0.9/demo/cluster/create/",
	"title": "Create your first cluster",
	"tags": [],
	"description": "",
	"content": "Installation with kubectl To create your first StackGres cluster you have to create a simple custom resource that reflect the cluster configuration. Assuming you have already installed the kubectl CLI you can proceed by installing a StackGres cluster using the following command:\ncat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; | kubectl create -f - apiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: simple spec: instances: 2 postgresVersion: \u0026#39;latest\u0026#39; pods: persistentVolume: size: \u0026#39;5Gi\u0026#39; EOF Enable backups This will create a cluster using latest available PostgreSQL version with 2 nodes each with a disk of 5Gi using the default storage class and a set of default configurations for PostgreSQL, connection pooling and resource profile.\nBy default backup are not enabled. To enable them you have to provide a storage configuration (AWS S3, Google Cloud Storage or Azure Blob Storage). We ship a kubernetes resources file in order to allow easy installation of Minio service that is compatible with AWS S3.\nClean up the previously created cluster:\nkubectl delete sgcluster simple Create the minio service and the backup configuration with default parameters:\nkubectl create -f https://stackgres.io/downloads/stackgres-k8s/stackgres/0.9.5/minio-demo.yml cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; | kubectl create -f - apiVersion: stackgres.io/v1beta1 kind: SGBackupConfig metadata: name: simple spec: storage: type: s3Compatible s3Compatible: bucket: stackgres region: k8s enablePathStyleAddressing: true endpoint: http://minio:9000 awsCredentials: secretKeySelectors: accessKeyId: key: accesskey name: minio secretAccessKey: key: secretkey name: minio EOF Then create the StackGres cluster indicating the previously created backup configuration:\ncat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; | kubectl create -f - apiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: simple spec: instances: 2 postgresVersion: \u0026#39;latest\u0026#39; pods: persistentVolume: size: \u0026#39;5Gi\u0026#39; configurations: sgBackupConfig: simple EOF To clean up the resources created by this demo just run:\nkubectl delete sgcluster simple kubectl delete sgbackupconfig simple kubectl delete -f https://stackgres.io/downloads/stackgres-k8s/stackgres/0.9.5/minio-demo.yml Installation with helm You can also install a StackGres cluster using helm vesion 3.x with the following command:\nhelm install simple \\ https://stackgres.io/downloads/stackgres-k8s/stackgres/0.9.5/helm/stackgres-cluster-demo.tgz To clean up the resources created by the demo run:\nhelm uninstall --keep-history simple helm get hooks simple | kubectl delete --ignore-not-found -f - helm uninstall simple Check cluster A cluster called simple will be deployed in the default namespace that is configured in your environment (normally this is the namespace default).\nwatch kubectl get pod -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE default simple-0 5/5 Running 0 97s 10.244.2.5 kind-worker2 \u0026lt;none\u0026gt; default simple-1 0/5 PodInitializing 0 41s 10.244.1.7 kind-worker \u0026lt;none\u0026gt; default simple-minio-7dfd746f88-7ndmq 1/1 Running 0 99s 10.244.1.5 kind-worker \u0026lt;none\u0026gt; Open a psql console To open a psql console and manage the PostgreSQL cluster you may connect to the postgres-util container of primary instance (with label role: master):\nkubectl exec -ti \u0026#34;$(kubectl get pod --selector app=StackGresCluster,cluster=true,role=master -o name)\u0026#34; -c postgres-util -- psql  IMPORTANT: Connecting directly trough the postgres-util sidecar will grant you access with the postgres user. It will work similar to sudo -i postgres -c psql.\n A full example:\n➜ kubectl exec -ti \u0026#34;$(kubectl get pod --selector app=StackGresCluster,cluster=true,role=master -o name)\u0026#34; -c postgres-util -- psql psql (12.3 OnGres Inc.) Type \u0026#34;help\u0026#34; for help. postgres=# select current_user; current_user -------------- postgres (1 row) postgres=# CREATE USER app WITH PASSWORD \u0026#39;test\u0026#39;; CREATE ROLE postgres=# CREATE DATABASE app WITH OWNER app; CREATE DATABASE postgres=# \\q Please check about the postgres-util side car and how to connect to the postgres cluster for more details.\nManage the status of the PostgreSQL cluster You can also open a shell in any instance to use patronictl and control the status of the cluster:\nkubectl exec -ti \u0026#34;$(kubectl get pod --selector app=StackGresCluster,cluster=true -o name | head -n 1)\u0026#34; -c patroni -- patronictl list Full example:\n➜ kubectl exec -ti \u0026#34;$(kubectl get pod --selector app=StackGresCluster,cluster=true -o name | head -n 1)\u0026#34; -c patroni -- patronictl list + Cluster: simple (6868989109118287945) ---------+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +----------+------------------+--------+---------+----+-----------+ | simple-0 | 10.244.0.9:7433 | Leader | running | 1 | | | simple-1 | 10.244.0.11:7433 | | running | 1 | 0 | +----------+------------------+--------+---------+----+-----------+ Connect from an application Each SGCluster will create a service for both the primary and the replicas. They will be create as ${CLUSTER-NAME}-primary and ${CLUSTER-NAME}-replicas.\nYou will be able to connect to the cluster primary instance using service DNS simple-primary from any pod in the same namespace.\nFor example:\n➜ kubectl run -it psql-connect --rm --image=postgres:12 -- psql -U postgres -h simple-primary If you don\u0026#39;t see a command prompt, try pressing enter. psql (12.4 (Debian 12.4-1.pgdg100+1), server 12.3 OnGres Inc.) Type \u0026#34;help\u0026#34; for help. postgres=# \\q Session ended, resume using \u0026#39;kubectl attach psql-connect -c psql-connect -i -t\u0026#39; command when the pod is running pod \u0026#34;psql-connect\u0026#34; deleted Check how to connect to the cluster for more details.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/complete-cluster/pooling-config/",
	"title": "Custom Connection Pooling Configuration",
	"tags": [],
	"description": "",
	"content": "StackGres will deploy Postgres clusters, by default, with a sidecar with a connection pooler (set the SGCluster.pods.disableConnectionPooling property if you don\u0026rsquo;t want such a connection pooler). The goal of this connection pooler fronting the database is to allow to control the incoming connections (fan-in) and keep Postgres operating with a lower number of concurrent connections, while allowing a higher number of external connections.\nA default configuration is provided by StackGres. But you may provide your own, creating an instance of the CRD SGPoolingConfig. StackGres currently uses PgBouncer. To create a custom PgBouncer configuration, create the file sgpoolingconfig-pgbouncer1.yaml:\napiVersion: stackgres.io/v1 kind: SGPoolingConfig metadata: namespace: demo name: poolconfig1 spec: pgBouncer: pgbouncer.ini: pool_mode: session max_client_conn: \u0026#39;200\u0026#39; default_pool_size: \u0026#39;200\u0026#39; and deploy to Kubernetes:\nkubectl apply -f sgpoolingconfig-pgbouncer1.yaml You may inspect the default values provided by StackGres by describing the created CRD:\nkubectl -n demo describe sgpoolconfig poolconfig1 Name: poolconfig1 Namespace: demo Labels: \u0026lt;none\u0026gt; Annotations: stackgres.io/operatorVersion: 1.0.0-alpha1 API Version: stackgres.io/v1 Kind: SGPoolingConfig Metadata: Creation Timestamp: 2021-03-01T10:18:20Z Generation: 1 Resource Version: 154323 Self Link: /apis/stackgres.io/v1/namespaces/demo/sgpoolconfigs/poolconfig1 UID: 2c4f5b08-041c-463d-a5ab-13dc8deabc93 Spec: Pg Bouncer: pgbouncer.ini: admin_users: postgres application_name_add_host: 1 auth_query: SELECT usename, passwd FROM pg_shadow WHERE usename=$1 auth_type: md5 auth_user: authenticator default_pool_size: 200 ignore_startup_parameters: extra_float_digits listen_addr: 127.0.0.1 max_client_conn: 200 max_db_connections: 0 max_user_connections: 0 pool_mode: session stats_users: postgres Status: Pg Bouncer: Default Parameters: stats_users ignore_startup_parameters auth_type max_db_connections pool_mode auth_query application_name_add_host max_user_connections auth_user listen_addr admin_users Events: \u0026lt;none\u0026gt; "
},
{
	"uri": "https://stackgres.io/doc/0.9/administration/cluster/connection/pgutil/",
	"title": "Local connection with the postgres-util sidecar",
	"tags": [],
	"description": "Describes how to connect on the cluster using kubectl and the postgres-util sidecar container.",
	"content": "Local Connection to the database has to be through the postgres-utils sidecar. This sidecar has all PostgreSQL binaries that are not present in the main container called patroni like the psql command.\nThis main container only have the required binaries and utilities to be able to configure the postgres cluster and the HA configuration.\nAccess to postgres-util sidecar First we\u0026rsquo;ll check the if the container is present in the pods, for these example we have a cluster named stackgres, composed of three pods and installed in the default namespace:\nkubectl get pods -n default -l app=StackGresCluster,cluster=true output:\nNAME READY STATUS RESTARTS AGE stackgres-0 5/5 Running 0 12m stackgres-1 5/5 Running 0 12m stackgres-2 5/5 Running 0 11m As you can see in the list we have 5/5 containers (sidecars) ready. To check the list of these containers we can run the next command:\nkubectl get pods stackgres-0 -n default -o jsonpath=\u0026#39;{.spec.containers[*].name}*\u0026#39; output:\npatroni envoy pgbouncer postgres-util prometheus-postgres-exporter At this point we already checked that sidecar postgres-util is up and running. Now to access the postgres instance through this sidecar you have two options:\n  Access directly from the kubectl commmand\nkubectl exec -it stackgres-0 -c postgres-util -- psql\nThen you will be into the postgresql console. You can access through the port 5432 this will connect via unix socket directly to postgres instances and will not required a password or you can use the port 6432 and the connection will be through the conection pooling tool (pgbouncer) and you will be ask for the password to connect.\npsql (11.6 OnGres Inc.) Type \u0026quot;help\u0026quot; for help. postgres=#   Access the sidecar console\nTo access the sidecar console run the next command:\n➜ kubectl exec -it simple-0 -c postgres-util -- bash bash-4.4$  Note: You will be able to run any linux command and have access to all the PostgreSQL binaries.\n Connect to postgres console:\nbash-4.4$ psql psql (12.3 OnGres Inc.) Type \u0026#34;help\u0026#34; for help. postgres=#    "
},
{
	"uri": "https://stackgres.io/doc/0.9/monitoring/metrics/node_exporter/",
	"title": "node_exporter",
	"tags": [],
	"description": "Contains details about the metrics collected by the node_exporter.",
	"content": "The next table contains details about the metrics collected by the node_exporter.\nFile system metrics    item metric group metric type description     1 node_filesystem        device LABEL Device of the filesystem     mountpoint LABEL Mount point of the filesystem     fstype LABEL The type of filesystem     size_bytes GAUGE Filesystem size in bytes     avail_bytes GAUGE Filesystem space available to non-root users in bytes     files GAUGE Filesystem total file nodes     files_free GAUGE Filesystem total free file nodes     device_error GAUGE Whether an error occurred while getting statistics for the given device    "
},
{
	"uri": "https://stackgres.io/doc/0.9/install/post/install/",
	"title": "Post-installation",
	"tags": [],
	"description": "",
	"content": "Exposing the UI StackGres publish a Web UI that can be accessed by pointing to port 443 with DNS stackgres-operator.stackgres.svc. It is not recommended to expose this Web UI to public internet without protecting it with some secure access bridge.\nYou can expose the UI using the command below:\nPOD_NAME=$(kubectl get pods --namespace stackgres -l \u0026quot;app=stackgres-restapi\u0026quot; -o jsonpath=\u0026quot;{.items[0].metadata.name}\u0026quot;) kubectl port-forward ${POD_NAME} --address 0.0.0.0 8443:9443 --namespace stackgres Connect to the UI Connect to https://\u0026lt;your-host\u0026gt;:8443/admin/ and get your UI credentials:\nkubectl get secret -n stackgres stackgres-restapi --template \u0026#39;{{ printf \u0026#34;username = %s\\n\u0026#34; (.data.k8sUsername | base64decode) }}\u0026#39; kubectl get secret -n stackgres stackgres-restapi --template \u0026#39;{{ printf \u0026#34;password = %s\\n\u0026#34; (.data.clearPassword | base64decode) }}\u0026#39; "
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/prerequisites/prometheus-grafana/",
	"title": "Prometheus and Grafana",
	"tags": [],
	"description": "",
	"content": "Prometheus (or Prometheus-compatible software) and Grafana are optional components, but it is recommended that you will follow these instructions to install them for the tutorial. Otherwise, monitoring and dashboard integration in the StackGres Web Console will not be enabled.\nAny Prometheus and Grafana installation may work with StackGres. But if you install modifying some defaults, you may need to make adjustments to the StackGres Helm installation. Use the following commands to run the tutorial without needing further adjustments.\nFirst, create a namespace for monitoring purposes:\nkubectl create namespace monitoring Then install both Prometheus and Grafana with the combined Helm chart:\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm install --namespace monitoring \\ \tprometheus prometheus-community/kube-prometheus-stack \\ \t--version 13.4.0 \\ \t--set grafana.enabled=true After 1-2 minutes you should get an output similar to:\nNAME: prometheus LAST DEPLOYED: Sun Feb 28 23:28:21 2021 NAMESPACE: monitoring STATUS: deployed REVISION: 1 NOTES: kube-prometheus-stack has been installed. Check its status by running: kubectl --namespace monitoring get pods -l \u0026#34;release=prometheus\u0026#34; Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026amp; configure Alertmanager and Prometheus instances using the Operator. You may also check the pods running on the monitoring namespace:\nkubectl -n monitoring get pods NAME READY STATUS RESTARTS AGE alertmanager-prometheus-kube-prometheus-alertmanager-0 2/2 Running 0 4m14s prometheus-grafana-6b87549fff-djkh6 2/2 Running 0 4m18s prometheus-kube-prometheus-operator-8d85d6d94-jjdn6 1/1 Running 0 4m18s prometheus-kube-state-metrics-6df5d44568-6z9qn 1/1 Running 0 4m18s prometheus-prometheus-kube-prometheus-prometheus-0 2/2 Running 1 4m14s prometheus-prometheus-node-exporter-5bc5q 1/1 Running 0 4m18s prometheus-prometheus-node-exporter-ccbkl 1/1 Running 0 4m18s prometheus-prometheus-node-exporter-x6mdz 1/1 Running 0 4m18s "
},
{
	"uri": "https://stackgres.io/doc/0.9/runbooks/restore-backup/",
	"title": "Restore a backup",
	"tags": [],
	"description": "Details about how to restore a StackGres cluster backup.",
	"content": "This runbook will show you how to restore a cluster backup. All the steps explained here are also available from the StackGres web console.\nCheck the database size The demo cluster ongres-db has one database:\n$ kubectl exec -it --namespace ongres-db ongres-db -c postgres-util -- psql -c '\\l' List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges | Size | Tablespace | Description -----------+----------+----------+---------+---------+-----------------------+---------+------------+-------------------------------------------- demo_db | postgres | UTF8 | C.UTF-8 | C.UTF-8 | | 20 MB | pg_default | postgres | postgres | UTF8 | C.UTF-8 | C.UTF-8 | | 7977 kB | pg_default | default administrative connection database template0 | postgres | UTF8 | C.UTF-8 | C.UTF-8 | =c/postgres +| 7793 kB | pg_default | unmodifiable empty database | | | | | postgres=CTc/postgres | | | template1 | postgres | UTF8 | C.UTF-8 | C.UTF-8 | =c/postgres +| 7793 kB | pg_default | default template for new databases | | | | | postgres=CTc/postgres | | | (4 rows) Get the backups list $ kubectl get sgbackups --namespace ongres-db NAME AGE backup-demo-1 3h33m backup-demo-2 3h11m backup-demo-3 55s Configuring the instance profile The restore consist in create a new cluster from any of the backups taked. You\u0026rsquo;re able to specify any of the cluster params and if you do not specify a SGInstanceProfile this will use the default profile with 1 CPU and 2Gi of RAM.\nCreate an instance profile specific for the restore (Assign the resources according with your environment). Create a file with the next content and then apply it:\napiVersion: stackgres.io/v1 kind: SGInstanceProfile metadata: namespace: ongres-db name: size-s spec: cpu: \u0026quot;500m\u0026quot; memory: \u0026quot;256Mi\u0026quot;  Note: The restore process needs to be done in the same namespace as the cluster to be restored.\n Get de UID from the backups to be restore kubectl get sgbackups --namespace ongres-db backup-demo-3 -o jsonpath=\u0026quot;{.metadata.uid}\u0026quot; This command will print the UID:\n0a3bb287-6b3f-4309-87bf-8d7c4c9e1beb Restore the backup To restore the backup you need to create a new SGCluster specifying the section initialData setting the param fromBackup with UID value from the previous step.\nCreate a yaml file with the next content and apply it (Change the values according to your environment):\napiVersion: stackgres.io/v1 kind: SGCluster metadata: name: demo-restore namespace: ongres-db spec: instances: 1 postgresVersion: '12' sgInstanceProfile: 'size-s' pods: persistentVolume: size: '10Gi' initialData: restore: fromBackup: 0a3bb287-6b3f-4309-87bf-8d7c4c9e1beb Now you should have a new cluster called demo-restore with all the data restored:\n$ kubectl exec -it -n ongres-db demo-restore-0 -c postgres-util -- psql -c '\\l+' List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges | Size | Tablespace | Description -----------+----------+----------+---------+---------+-----------------------+---------+------------+-------------------------------------------- demo_db | postgres | UTF8 | C.UTF-8 | C.UTF-8 | | 20 MB | pg_default | postgres | postgres | UTF8 | C.UTF-8 | C.UTF-8 | | 7977 kB | pg_default | default administrative connection database template0 | postgres | UTF8 | C.UTF-8 | C.UTF-8 | =c/postgres +| 7793 kB | pg_default | unmodifiable empty database | | | | | postgres=CTc/postgres | | | template1 | postgres | UTF8 | C.UTF-8 | C.UTF-8 | =c/postgres +| 7793 kB | pg_default | default template for new databases | | | | | postgres=CTc/postgres | | | (4 rows) "
},
{
	"uri": "https://stackgres.io/doc/0.9/reference/crd/sgpgconfig/",
	"title": "SGPostgresConfig",
	"tags": [],
	"description": "Details about SGPostgresConfig configurations",
	"content": "A PostgreSQL configuration CR represent the configuration of a specific PostgreSQL major version.\nHave a look at postgresqlco.nf to help you tune and optimize your PostgreSQL configuration.\n Kind: SGPostgresConfig\nlistKind: SGPostgresConfigList\nplural: sgpgconfigs\nsingular: sgpgconfig\n Spec\n   Property Required Updatable Type Default Description     postgresVersion   string 12 The major Postgres version the configuration is for. Postgres major versions contain one number starting with version 10 (10, 11, 12, etc), and two numbers separated by a dot for previous versions (9.6, 9.5, etc).\nNote that Postgres maintains full compatibility across minor versions, and hence a configuration for a given major version will work for any minor version of that same major version.\nCheck StackGres component versions to see the Postgres versions supported by this version of StackGres.\n    postgresql.conf  ✓ object see below The postgresql.conf parameters the configuration contains, represented as an object where the keys are valid names for the postgresql.conf configuration file parameters of the given postgresVersion. You may check postgresqlco.nf as a reference on how to tune and find the valid parameters for a given major version.     Default value of postgresql.conf property:\ncheckpoint_completion_target: \u0026#34;0.9\u0026#34; checkpoint_timeout: 15min default_statistics_target: \u0026#34;250\u0026#34; extra_float_digits: \u0026#34;1\u0026#34; huge_pages: \u0026#34;off\u0026#34; lc_messages: C random_page_cost: \u0026#34;2.0\u0026#34; shared_preload_libraries: pg_stat_statements track_activity_query_size: \u0026#34;2048\u0026#34; track_functions: pl track_io_timing: \u0026#34;on\u0026#34; wal_compression: \u0026#34;on\u0026#34; Example:\napiVersion: stackgres.io/v1beta1 kind: SGPostgresConfig metadata: name: postgresconf spec: postgresVersion: \u0026#34;11\u0026#34; postgresql.conf: password_encryption: \u0026#39;scram-sha-256\u0026#39; random_page_cost: \u0026#39;1.5\u0026#39; shared_buffers: \u0026#39;256MB\u0026#39; wal_compression: \u0026#39;on\u0026#39; To guarantee a functional postgres configuration some of the parameters specified in postgres configuration documentation have been blacklisted and will be ignored. The parameters that will be ignored are:\n   Blacklisted parameter     listen_addresses   port   cluster_name   hot_standby   fsync   full_page_writes   log_destination   logging_collector   max_replication_slots   max_wal_senders   wal_keep_segments   wal_level   wal_log_hints   archive_mode   archive_command    "
},
{
	"uri": "https://stackgres.io/doc/0.9/install/integrations/nutanix-karbon/cluster-delete/",
	"title": "StackGres Cluster Delete",
	"tags": [],
	"description": "",
	"content": "StackGres Cluster Delete Delete a SG cluster within all Postgres servers inside is not a common task, but if need to do here is the step\ncat \u0026lt;\u0026lt;EOF | kubectl delete -f - apiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: namespace: karbon name: karbon-db spec: postgresVersion: \u0026#39;12.3\u0026#39; instances: 3 sgInstanceProfile: \u0026#39;size-s\u0026#39; pods: persistentVolume: size: \u0026#39;20Gi\u0026#39; configurations: sgPostgresConfig: \u0026#39;pgconfig\u0026#39; sgPoolingConfig: \u0026#39;poolconfig\u0026#39; sgBackupConfig: \u0026#39;backupconfig\u0026#39; distributedLogs: sgDistributedLogs: \u0026#39;distributedlogs\u0026#39; initialData: scripts: - name: create-admin-user scriptFrom: secretKeyRef: name: admin-user-password key: admin-create-user-sql - name: create-database script: | create database admin owner admin; prometheusAutobind: true nonProductionOptions: disableClusterPodAntiAffinity: true EOF  You should use the same YAML used to create the cluster, that is a good reason to save all this into yaml files.\n "
},
{
	"uri": "https://stackgres.io/doc/0.9/08-stackgres-web-api/03-api-reference/",
	"title": "StackGres REST API Reference",
	"tags": [],
	"description": "",
	"content": "The StackGres REST API offers a way to interact with StackGres in a similar way as using the kubernetes API through the StackGres CRDs but with more functionality, like creating secrets and configmap in the same call and leveraging the kubernetes RBAC permission system.\nThe API is provided by a kubernetes deployment that is installed together with the operator. It is mainly used by the Web UI that run in the same deployment.\nSwagger is in essence an Interface Description Language for describing RESTful APIs expressed using JSON. Swagger is used together with a set of open-source software tools to design, build, document, and use RESTful web services. Swagger includes automated documentation, code generation (into many programming languages), and test-case generation.\nTo access those endpoints that require authentication you will need a valid JWT. You may obtain the JWT by authenticating using the login endpoint (see AUTH section above). To set up a user and permission see the RBAC authorization section.\n  window.onload = function() { const ui = SwaggerUIBundle({ url: \"https:\\/\\/stackgres.io\\/doc\\/0.9/sg-swagger.yaml\", dom_id: '#ohpen_swagger_ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset ], docExpansion: \"list\" }) window.ui = ui } \t"
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/",
	"title": "Tutorial",
	"tags": [],
	"description": "",
	"content": "Chapter 3 Tutorial Self-paced tutorial. Discover and practice the most significant StackGres features with guided examples.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/complete-cluster/backup-configuration/",
	"title": "Backup configuration",
	"tags": [],
	"description": "",
	"content": "StackGres takes and maintains backups of your clusters automatically, if configured to do so. You will need an object storage backup, see Object Storage Prerequisite if you don\u0026rsquo;t have one configured yet.\nBackups are created with Postgres continous archiving (physical backups), which allow essentially zero or close to zero data loss. Base backups are taken regularly, and every generated WAL file is pushed to the object storage to account for data changes in between base backups.\nTo access the bucket, StackGres will expect you to have configured a Kubernetes Secret where the bucket credentials are stored. The keys of the secret may have any name that you want, they will be later referenced by a SecretKeySelector where you can specify the names of the keys that contain the credentials. Credentials may vary depending on the object storage technology used, continue to the technology of your choice link below to configure the credentials and the backup configuration:\n AWS S3  "
},
{
	"uri": "https://stackgres.io/doc/0.9/install/restart/",
	"title": "Cluster Restart",
	"tags": [],
	"description": "",
	"content": "This procedure can be used in general after a configuration change that requires restart (including postgres, pgbouncer or any configuration of the StackGres cluster). As a reference, a restart is required when the cluster condition PendingRestart inside .status.conditions property is True.\nkubectl get sgclusters.stackgres.io -A --template \u0026#39; {{- range $item := .items }} {{- range $item.status.conditions }} {{- if eq .type \u0026#34;PendingRestart\u0026#34; }} {{- printf \u0026#34;%s.%s %s=%s\\n\u0026#34; $item.metadata.namespace $item.metadata.name .type .status }} {{- end }} {{- end }} {{- end }}\u0026#39; The restart procedure will generate a service disruption. The service disruption will start for the read write connections when the primary pod is deleted and will end when Patroni elect the new primary. For read only connections the service disruption will start when only one replica exists and the replica pod is deleted and will end when Patroni set the role of the pod to replica.\nThere are two restart strategy:\n In-Place Restart: this procedure does not require more resources than those that are available. In case only an instance of the StackGres cluster is present this mean the service disruption will last longer so we encourage use the reduced impact restart and especially for a production environment. Reduced Impact Restart: this procedure is the same as the in-place restart but require additional resources in order to spawn a new updated replica that will be removed when the procedure completes.  Those procedures includes some shell script snippet examples. In those snippet we assume the following environment variables are set with values of the StackGres cluster you want to restart:\nNOTE: If any of postgres\u0026rsquo;s parameters max_connections, max_prepared_transactions, max_wal_senders, max_wal_senders or max_locks_per_transaction are changed to a lower value than they were set the primary have to be restarted before any replica can be restarted too, the service disruption for read write connection will last longer in this case depending how long it take to the primary to restart.\nNAMESPACE=default SGCLUSTER=example  NOTE: If any error arise at any point during restart of a cluster please refer to our Cluster Restart Troubleshooting section to find solutions to common issues or, if no similar issue exists, feel free to open an issue on the StackGres project.\n 1. [Reduced-impact Restart] - Add read-only instace [Optional, only for the reduced-impact restart]\nEdit the SGCluster and increment by one the number of instances.\nINSTANCES=\u0026#34;$(kubectl get sgcluster -n \u0026#34;$NAMESPACE\u0026#34; \u0026#34;$SGCLUSTER\u0026#34; --template \u0026#34;{{ .spec.instances }}\u0026#34;)\u0026#34; echo \u0026#34;Inreasing cluster instances from $INSTANCESto $((INSTANCES+1))\u0026#34; kubectl patch sgcluster -n \u0026#34;$NAMESPACE\u0026#34; \u0026#34;$SGCLUSTER\u0026#34; --type merge -p \u0026#34;spec: { instances: $((INSTANCES+1))}\u0026#34; Wait until the new instance is created and operational, receiving traffic from the LB. This new replica has already been initialized with the new components. Note the name of the new pod.\nREAD_ONLY_POD=\u0026#34;$SGCLUSTER-$INSTANCES\u0026#34; echo \u0026#34;Waiting for pod $READ_ONLY_POD\u0026#34; kubectl wait --for=condition=Ready -n \u0026#34;$NAMESPACE\u0026#34; \u0026#34;pod/$READ_ONLY_POD\u0026#34; while kubectl get pod -n \u0026#34;$NAMESPACE\u0026#34; \\  -l \u0026#34;app=StackGresCluster,cluster-name=$SGCLUSTER,cluster=true,role=replica\u0026#34; -o name \\  | grep -F \u0026#34;pod/$READ_ONLY_POD\u0026#34; | wc -l | grep -q 0; do sleep 1; done 2. [In-place Restart] - Restart primary first [Optional, if max_connections, max_prepared_transactions, max_wal_senders, max_wal_senders or max_locks_per_transaction are changed to a lower value than they were set]\nPRIMARY_POD=\u0026#34;$(kubectl get pod -n \u0026#34;$NAMESPACE\u0026#34; \\  -l \u0026#34;app=StackGresCluster,cluster-name=$SGCLUSTER,cluster=true,role=master\u0026#34; -o name | head -n 1)\u0026#34; PRIMARY_POD=\u0026#34;${PRIMARY_POD#pod/}\u0026#34; echo \u0026#34;Restart primary instance $PRIMARY_POD\u0026#34; kubectl exec -t -n \u0026#34;$NAMESPACE\u0026#34; \u0026#34;$PRIMARY_POD\u0026#34; -- patronictl restart \u0026#34;$SGCLUSTER\u0026#34; \u0026#34;$PRIMARY_POD\u0026#34; --force echo \u0026#34;Waiting for primary pod $PRIMARY_POD\u0026#34; kubectl wait --for=condition=Ready -n \u0026#34;$NAMESPACE\u0026#34; \u0026#34;pod/$PRIMARY_POD\u0026#34; 3. [In-place Restart] - Check read-only pods to restart Check which read-only pods requires to be restarted.\nREAD_ONLY_PODS=\u0026#34;$([ -z \u0026#34;$READ_ONLY_PODS\u0026#34; ] \\  \u0026amp;\u0026amp; kubectl get pod -n \u0026#34;$NAMESPACE\u0026#34; --sort-by \u0026#39;{.metadata.name}\u0026#39; \\  -l \u0026#34;app=StackGresCluster,cluster-name=$SGCLUSTER,cluster=true,role=replica\u0026#34; \\  --template \u0026#39;{{ range .items }}{{ printf \u0026#34;%s\\n\u0026#34; .metadata.name }}{{ end }}\u0026#39; \\  || (echo \u0026#34;$READ_ONLY_PODS\u0026#34; | tail -n +2))\u0026#34; echo \u0026#34;Read only pods to restart:\u0026#34; echo \u0026#34;$READ_ONLY_PODS\u0026#34; READ_ONLY_POD=\u0026#34;$(echo \u0026#34;$READ_ONLY_PODS\u0026#34; | head -n 1)\u0026#34; [ -z \u0026#34;$READ_ONLY_POD\u0026#34; ] \u0026amp;\u0026amp; echo \u0026#34;No more read only pods needs restart\u0026#34; \\  || echo \u0026#34;$READ_ONLY_PODwill be restarted\u0026#34; 4. [In-place Restart] - Delete a read-only pod Delete one of the read-only pods.\necho \u0026#34;Deleting read-only pod $READ_ONLY_POD\u0026#34; kubectl delete -n \u0026#34;$NAMESPACE\u0026#34; pod \u0026#34;$READ_ONLY_POD\u0026#34; A new one will be created, and will also have the new components. Wait until fully operational and note the name of the new pod.\necho \u0026#34;Waiting for pod $READ_ONLY_POD\u0026#34; kubectl wait --for=condition=Ready -n \u0026#34;$NAMESPACE\u0026#34; \u0026#34;pod/$READ_ONLY_POD\u0026#34; 5. [In-place Restart] - Repeat two previous steps Repeat the previous two steps until no more read-only pods requires restart. In this moment, you have a cluster with N+1 instances (pods), all upgraded with the new components except for the primary instance.\n6. [In-place Restart] - Perform switchover If you have at least a read-only pod perform a switchover of the primary pod.\nREAD_ONLY_POD=\u0026#34;$(kubectl get pod -n \u0026#34;$NAMESPACE\u0026#34; \\  -l \u0026#34;app=StackGresCluster,cluster-name=$SGCLUSTER,cluster=true,role=replica\u0026#34; -o name | head -n 1)\u0026#34; PRIMARY_POD=\u0026#34;$(kubectl get pod -n \u0026#34;$NAMESPACE\u0026#34; \\  -l \u0026#34;app=StackGresCluster,cluster-name=$SGCLUSTER,cluster=true,role=master\u0026#34; -o name | head -n 1)\u0026#34; READ_ONLY_POD=\u0026#34;${READ_ONLY_POD#pod/}\u0026#34; PRIMARY_POD=\u0026#34;${PRIMARY_POD#pod/}\u0026#34; if [ -n \u0026#34;$READ_ONLY_POD\u0026#34; ] then echo \u0026#34;Performing switchover from primary pod $PRIMARY_PODto read only pod $READ_ONLY_POD\u0026#34; [ -n \u0026#34;$PRIMARY_POD\u0026#34; ] \\  \u0026amp;\u0026amp; kubectl exec -ti -n \u0026#34;$NAMESPACE\u0026#34; \u0026#34;$PRIMARY_POD\u0026#34; -c patroni -- \\  patronictl switchover --master \u0026#34;$PRIMARY_POD\u0026#34; --candidate \u0026#34;$READ_ONLY_POD\u0026#34; --force else echo \u0026#34;Can not perform switchover, no read only pod found\u0026#34; fi 7. [In-place Restart] - Delete primary pod Delete the primary pod.\nif [ -n \u0026#34;$READ_ONLY_POD\u0026#34; ] then echo \u0026#34;Deleting read-only pod $PRIMARY_POD\u0026#34; else echo \u0026#34;Deleting primary pod $PRIMARY_POD\u0026#34; fi kubectl delete -n \u0026#34;$NAMESPACE\u0026#34; pod \u0026#34;$PRIMARY_POD\u0026#34; A new read-only (or primary if there were only a single instance) instance will be created. Wait until it is fully operational.\necho \u0026#34;Waiting for pod $PRIMARY_POD\u0026#34; kubectl wait --for=condition=Ready -n \u0026#34;$NAMESPACE\u0026#34; pod \u0026#34;$PRIMARY_POD\u0026#34; 8. [Reduced-impact Restart] - Scale back the cluster size, editing the [Optional, only for the small impact procedure]\nScale back the cluster size, editing the SGCluster and decrementing by one the number of instances.\nINSTANCES=\u0026#34;$(kubectl get sgcluster -n \u0026#34;$NAMESPACE\u0026#34; \u0026#34;$SGCLUSTER\u0026#34; --template \u0026#34;{{ .spec.instances }}\u0026#34;)\u0026#34; echo \u0026#34;Decreasing cluster instances from $INSTANCESto $((INSTANCES-1))\u0026#34; kubectl patch sgcluster -n \u0026#34;$NAMESPACE\u0026#34; \u0026#34;$SGCLUSTER\u0026#34; --type merge -p \u0026#34;spec: { instances: $((INSTANCES-1))}\u0026#34; "
},
{
	"uri": "https://stackgres.io/doc/0.9/install/restart/troubleshooting/",
	"title": "Cluster Restart Troubleshooting",
	"tags": [],
	"description": "",
	"content": "This section is currently empty.\n NOTE: If any error arise at any point during restart of a cluster please if no similar issue exists, feel free to open an issue on the StackGres project.\n "
},
{
	"uri": "https://stackgres.io/doc/0.9/intro/stack-components/",
	"title": "Components of the Stack",
	"tags": [],
	"description": "",
	"content": "Running Postgres in production requires \u0026ldquo;a RedHat\u0026rdquo; of PostgreSQL. A curated set of open source components built, verified and packaged together. In this sense Postgres is like the Linux kernel, it needs many components around it to provide what a Linux distribution provide.\nExists an ecosystem of tools built around Postgres that can be used to build a Postgres distribution. This is what we call the stack of components.\nChoosing the right component of this stack is an hard task. Exists many components that overlap functionalities or have pros and cons to take into account before choosing one over another. It is required an high understanding of all the components in order to chose the ones that fit together and provide a production ready Postgres distribution.\nOur Postgres distribution is composed on a central core component (Postgres) and some other components that fulfill requirements in each different area required in the Postgres production distribution.\nCore The main container used for a Postgres cluster node uses an UBI 8 minimal image as its base to which is added a vanilla PostgreSQL v11, v12. It uses a persistent storage configured via StorageClass. Is always deployed with a sidecar util container to allow access for a system/database administrator.\nConfiguration Run PostgreSQL using default configuration is a really bad idea in a production environment. PostgreSQL uses very conservative defaults so it must be tuned in order to achieve good performance of the database. Exists some places where you can find information about Postgres configuration parameters and best practices:\n Postgres Official Documentation https://postgresqlco.nf (see PostgreSQL Configuration for Humans) The Internals of PostgreSQL  StackGres is tuned by default to achieve better performance than using the default configuration. User can still be configured by user in order to give the flexibility that some users needs.\nConnection pooling Connecting directly to PostgreSQL does not scale very well.\npg_bench, scale 2000, m4.large (2 vCPU, 8GB RAM, 1k IOPS) Connection pooling is required in order to not saturate PostgreSQL processes by creating queue of sessions, transactions or statements (depending on the application requirements).\nExists 3 alternatives solutions:\n PgPool PgBouncer Odyssey  Which one to chose?\nThe StackGres chosen solution is PgBouncer. It is enough simple and stable to be used for connection pooling. The disadvantage is the lack of multithreading that can lead to CPU saturation when connections increase over certain limit that depends on the performance of a single CPU\u0026rsquo;s core where it is running. Odyssey will be a good candidate to replace PgBouncer when it will become more mature.\nHigh availability If a Postgres instance goes down or is not working properly we want our cluster to recover by choosing a working instance to convert to the new primary and configure all the other instances and the application to point to this new primary. We want all this to happen without manual intervention.\nA high availability solution allow to achieve this feature. Exists many solutions to this problem and is really hard to chose one among them:\n PgPool Repmgr Patroni pg_autofailover PAF Stolon  Patroni is the HA solution chosen for StackGres. It is a well proved solution that relies on distributed consensus algorithms in order to provide a consistent mechanism for primary election. In particular it is able to use the same distributed consensus algorithm used by Kubernetes so that it does not requires installation of other services.\nBackup and disaster recovery Backup tools solutions are also a very higly populated ecosystem:\n pg_dump Barman PgBackrest Wal-e / Wal-g pg_probackup  Also, where do we store our backups?\n Disk Cloud storage  And finally, will our backup work when needed or will it fail?\nWal-g, the successor of Wal-e, is the most complete and lightweight solution to provide both incremental (trough archive command) and full backup support. Also, it provides out of the box features that allow store backup in a persistent volume (using a storage class that supports ReadWriteMany access mode) or a cloud storage between AWS S3, Google Cloud Storage or Azure Blob Storage. It also allow configure aspects like bandwidth or disk usage rate.\nLog We want to store our logs distributed across all our containers in a central location and be able to analyze them when needed. It does not exists a good solution for that so you have to build one. Exists fluentd and Loki, this last does not work very well with Postgres. An alternative is to store all the logs in Postgres using Timescale.\nProxy How do I locate the primary, if it might be changing? How do I obtain traffic metrics? It is possible to manage traffic: duplicate, A/B to test cluster or event inspect it?\nEnvoy is an open source edge and service proxy, designed for cloud-native applications. It is extensible in order to provide advanced functionality based on the actual traffic (for example the Postgres could be parsed in order to offer stats) or on connection characteristic (like the TLS certificate in order to chose to which node the connection have to be dispatched.\nIt is also capable of exporting metrics using well established prometheus format.\nOnGres Inc. sponsors the Envoy Proxy project, with contributions such as exposing stat metrics and SSL support (currently WIP).\nMonitoring Which monitoring solution can we use to monitor a Postgres cluster?\n Zabbix Okmeter Pganalyze Pgwatch2 PoWA New Relic DataDog Prometheus  StackGres approach here is to enable as much monitoring solution as possible. Currently, only Prometheus can connect to StackGres stats using the PostgreSQL Server Exporter and integrates as a sidecar offering an auto binding mechanism if prometheus is installed using the prometheus operator.\nTake in account that Prometheus is a dependency and that StackGres expects that you install and configure it separately.\nOf course, StackGres provides an option to deploy Prometheus alongside the StackGres Operator as part of the Helm chart and you can follow the steps there to set the Helm chart needed parameters so that monitoring integration works as expected. Please, read and review the steps and notes for a successful installation.\nPlease note that Prometheus will be removed from the Helm chart at some point, so the actual instructions will change and become obsolete.\nGrafana integration By default helm chart of prometheus operator comes with grafana and StackGres offer an integration to allow monitoring a StackGres cluster pod directly from the StackGres UI. There are various options to achieve it.\nStackGres includes two ways to perform such integration.\n Automatic integration Manual integration  Some manual steps are required in order to achieve such integration.\nUser interface Exists some user interface to interact with Postgres like DBeaver that allow to look at the database content and configuration. We need a user interface that is capable of manage an entire cluster. How do I list the clusters? How many nodes have a cluster? What is the status of replication? How many resources are used by a node? How to get monitoring info of a particular node?\nStackGres provide a Web and CLI user interface able to monitor and interact with the created StackGres clusters. It allow to do basic and advanced tasks like list/get/create/update/delete a cluster or execute a switchover or a backup recovery.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/complete-cluster/",
	"title": "Create a complete cluster",
	"tags": [],
	"description": "",
	"content": "We will start by creating a dedicated namespace for the cluster we will be creating:\nkubectl create namespace demo Then proceed with the following steps:\n             "
},
{
	"uri": "https://stackgres.io/doc/0.9/administration/custom/postgres/config/",
	"title": "Customize Postgres Configuration",
	"tags": [],
	"description": "",
	"content": "StackGres operator creates default configurations in the same namespace where the operator has been installed. Those configuration CRs are read-only and can only be modified by the operator itself. User can create modified version of those default configurations by creating one in the same namespace where the cluster will be created.\nWhen configuration resource is created the operator will look for a default configuration in the same namespace as the operator. With the default configuration found it will merge fields in the spec section that are not present in the created configuration filling them with values from the default configuration.\nIf a StackGres cluster is created without specifying PostgreSQL configuration (SGPostgresConfig) or resource profile configuration (sgInstanceProfile) the operator will look for default configuration of those kinds in the same namespace as the cluster or will create one using the default configuration in the same namespace where the operator is installed.\nHere is the list of default configuration name with his kind:\n   Name Kind     defaultpgconfig SGPostgresConfig   defaultprofile SGInstanceProfile   defaultpgbouncer SGPoolingConfig   defaultbackupconfig SGBackupConfig    "
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/prerequisites/object-storage/",
	"title": "Object Storage",
	"tags": [],
	"description": "",
	"content": "StackGres stores backups on object storage buckets. Currently supported are S3, GCS, Azure Blob and S3-compatible APIs. You will need a bucket and user credentials to access the bucket, create paths and read and write to it.\nYou should refer to the documentation of the respective providers on how to configure and provide appropriate credentials for your preferred type of object storage. For convenience, sample commands are contained within this section on how to create buckets and credentials for some of the above object storage providers.\nProviders:\n AWS S3  Alternatively, you may consider using MinIO, which works as an S3-compatible API for StackGres, and allows to run object storage locally.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/install/",
	"title": "Production Installation",
	"tags": [],
	"description": "",
	"content": "Chapter 4 Production Installation StackGres installation documentation for production environment.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/developer/stackgres/outside/",
	"title": "Running StackGres outside of K8s",
	"tags": [],
	"description": "",
	"content": "To run StackGres outside of kubernetes you will first need to install some required kubernetes resources:\nhelm install stackgres-operator stackgres-k8s/install/helm/stackgres-operator --namespace stackgres --set deploy.create=false --set-string cert.crt=\u0026quot;$(base64 stackgres-k8s/src/test/resources/certs/server.crt)\u0026quot; --set-string cert.key=\u0026quot;$(base64 stackgres-k8s/src/test/resources/certs/server-key.pem)\u0026quot; You have also to create a service in order to allow admission web hooks to work from outside of kubernetes:\ncat \u0026lt;\u0026lt; 'EOF' | kubectl create -f - --- kind: Service apiVersion: v1 metadata: namespace: stackgres name: stackgres-operator spec: ports: - port: 443 targetPort: 8443 --- kind: Endpoints apiVersion: v1 metadata: namespace: stackgres name: stackgres-operator subsets: - addresses: - ip: 172.17.0.1 ports: - port: 8443 EOF This configuration only works if you use kind.\nThen you may start the operator outside of kubernetes using the following command (remember to build the operator first, see building stackgres section):\njava -cp stackgres-k8s/src/operator/target/stackgres-operator-runner.jar \\ -Dquarkus.http.ssl.certificate.file=stackgres-k8s/src/test/resources/certs/server.crt \\ -Dquarkus.http.ssl.certificate.key-file=src/test/resources/certs/server-key.pem -Dquarkus.http.port=8080 -Dquarkus.http.ssl-port=8443 "
},
{
	"uri": "https://stackgres.io/doc/0.9/reference/crd/sgpoolingconfig/",
	"title": "SGPoolingConfig",
	"tags": [],
	"description": "Details about SGPoolingConfig configurations",
	"content": "The connection pooling CR represent the configuration of PgBouncer.\n Kind: SGPoolingConfig\nlistKind: SGPoolingConfigList\nplural: sgpoolconfigs\nsingular: sgpoolconfig\n Spec\n   Property Required Updatable Type Default Description     pgBouncer  ✓ object see below Connection pooling configuration based on PgBouncer.     PgBouncer    Property Required Updatable Type Default Description     pgbouncer.ini  ✓ object see below The pgbouncer.ini parameters the configuration contains, represented as an object where the keys are valid names for the pgbouncer.ini configuration file parameters.\nCheck pgbouncer configuration for more information about supported parameters.\n     Default value of pgbouncer.ini property:\npool_mode: session max_client_conn: \u0026#34;1000\u0026#34; Example:\napiVersion: stackgres.io/v1beta1 kind: SGPoolingConfig metadata: name: pgbouncerconf spec: pgbouncer.ini: pool_mode: \u0026#39;transaction\u0026#39; max_client_conn: \u0026#39;2000\u0026#39; To guarantee a functional pgbouncer configuration most of the parameters specified in pgbouncer configuration documentation for section [pgbouncer] have been blacklisted and will be ignored. The only parameters that can be changed are:\n   Whitelisted parameter     pool_mode   max_client_conn    For reference this is the list of parameters that will be ignored:\n   Blacklisted parameter     logfile   pidfile   listen_addr   listen_port   unix_socket_dir   unix_socket_mode   unix_socket_group   user   auth_file   auth_hba_file   auth_type   auth_query   auth_user   default_pool_size   min_pool_size   reserve_pool_size   reserve_pool_timeout   max_db_connections   max_user_connections   server_round_robin   ignore_startup_parameters   disable_pqexec   application_name_add_host   conffile   service_name   job_name   stats_period   syslog   syslog_ident   syslog_facility   log_connections   log_disconnections   log_pooler_errors   log_stats   verbose   admin_users   stats_users   server_check_delay   server_check_query   server_fast_close   server_lifetime   server_idle_timeout   server_connect_timeout   server_login_retry   client_login_timeout   autodb_idle_timeout   dns_max_ttl   dns_nxdomain_ttl   dns_zone_check_period   client_tls_sslmode   client_tls_key_file   client_tls_cert_file   client_tls_ca_file   client_tls_protocols   client_tls_ciphers   client_tls_ecdhcurve   client_tls_dheparams   server_tls_sslmode   server_tls_ca_file   server_tls_key_file   server_tls_cert_file   server_tls_protocols   server_tls_ciphers   query_timeout   query_wait_timeout   client_idle_timeout   idle_transaction_timeout   pkt_buf   max_packet_size   listen_backlog   sbuf_loopcnt   suspend_timeout   tcp_defer_accept   tcp_socket_buffer   tcp_keepalive   tcp_keepcnt   tcp_keepidle   tcp_keepintvl    "
},
{
	"uri": "https://stackgres.io/doc/0.9/administration/",
	"title": "Administration Guide",
	"tags": [],
	"description": "",
	"content": "Chapter 5 Postgres Administration Administration of postgres clusters created with StackGres.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/administration/cluster/pool/",
	"title": "Customize Connection Pooling Configuration",
	"tags": [],
	"description": "",
	"content": "If you happen to be reading this, it\u0026rsquo;s because you are aware of your application characteristics and needs for scaling connections on a production environment.\nA simple way to target this correctly, is to verify the usage of Prepared Statements, on top of which session mode will be the only compatible.\nSome applications, do not handle connection closing properly, which may require to add certain timeouts for releasing server connections.\nReloading configuration In the Customizing Pooling configuration section, it is explained the different sauces for scaling connections properly.\nEach configuration, once applied, need to be reloaded. This can be done by getting the corresponding primary node pod name and issue the same signal it is done on most of the environments:\nPRIMARY=$(kubectl get pod -l role=master -n my-cluster -o name) kubectl exec -n my-cluster -it ${PRIMARY} -c postgres-util -- pkill --signal HUP pgbouncer "
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/complete-cluster/distributed-logs/",
	"title": "Distributed Logs",
	"tags": [],
	"description": "",
	"content": "By default, Postgres logs are written to the ephemeral storage of the Patroni container, and can be accessed in the usual manner. However, this is not ideal because of the ephemeral nature of the storage, and the fact that logs from all pods are on different locations.\nStackGres has created a technology stack to send Postgres and Patroni logs to a separate location, called a Distributed Logs Server. This server is represented by the CRD SGDistributedLogs. It is a separate Postgres instance, optimized for log storage, using the time-series Timescale extension to support high volume injection and automatic partitioning of logs, as well as log rotation.\nThis is all handled transparently for you, just go ahead and create the file sgdistributedlogs-server1.yaml to use this functionality:\napiVersion: stackgres.io/v1 kind: SGDistributedLogs metadata: namespace: demo name: distributedlogs spec: persistentVolume: size: 50Gi and deploy to Kubernetes:\nkubectl apply -f sgdistributedlogs-server1.yaml This last command will trigger the creation of some resources (other than metadata). In particular, it will create a pod for storing the mentioned distributed logs:\nkubectl -n demo get pods NAME READY STATUS RESTARTS AGE distributedlogs-0 3/3 Running 1 73s Distributed logs server are multi-tenant: you may reference a distributed log server from more than one cluster. If used this functionality, logs will be sent to the distributed log server, and not stored in the ephemeral pod storage (other than temporarily in small buffers).\nTo see the distributed logs, you may view them from the Web Console, or connect via psql and query them with SQL!\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/intro/license/",
	"title": "Licensing",
	"tags": [],
	"description": "",
	"content": "StackGres source code is licensed under the OSI-approved open source license GNU Affero General Public License version 3 (AGPLv3). All the source code is available on the gitlab repository.\nWe respect others who switch to or are directly built as source-available software, but we don’t follow this approach. We love the concept of GitLab’s stewardship, and in the same spirit, we promise you that StackGres will always be open source software.\nContact us at StackGres at OnGres dot com if you want a trial or commercial license that does not contain the GPL clauses.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/developer/stackgres/e2e/",
	"title": "Running the e2e tests",
	"tags": [],
	"description": "",
	"content": "Given the operator nature we rely heavily on our integration and e2e tests.\nE2E tests are mainly composed of POSIX complaint scripts (only exception is the use of local variables in functions), and we intend to keep them that way.\nThe easiest way to run the e2e scripts is by executing the stackgres-k8s/e2e/run-all-tests.sh file. This script will configure a kuberentes cluster (by default kind), then it will generate jvm version of the operator and deploy it on the configured kubernetes cluster.\nThe E2E tests are grouped by specs, and they are contained in the folder stackgres-k8s/e2e/spec.\nThere is also several util functions which are localted in the folder stackgres-k8s/e2e/utils\nE2E tests with kind By default e2e tests are made with kind, so you don\u0026rsquo;t need to specify anything to use it other than having kind installed.\nE2E tests with other environment Environment can be specified using the ENV environment variable. Currently we support following environment:\n kind minikube gke  "
},
{
	"uri": "https://stackgres.io/doc/0.9/reference/crd/sgbackupconfig/",
	"title": "SGBackupConfig",
	"tags": [],
	"description": "Details about SGBackupConfig configurations",
	"content": "Configuration Backup configuration allow to specify when and how backups are performed. By default this is done at 5am UTC in a window of 1 hour, you may change this value in order to perform backups for another time zone and period of time. The backup configuration CR represent the backups configuration of the cluster.\n Kind: SGBackupConfig\nlistKind: SGBackupConfigList\nplural: sgbackupconfigs\nsingular: sgbackupconfig\n Spec\n   Property Required Updatable Type Default Description     baseBackups  ✓ object  Back backups configuration.    storage  ✓ object  Backup storage configuration.     Example:\napiVersion: stackgres.io/v1beta1 kind: SGBackupConfig metadata: name: backupconf spec: baseBackups: retention: 5 cronSchedule: 0 5 * * * compression: lz4 performance: maxDiskBandwitdh: 26214400 #25 MB per seceod maxNetworkBandwitdh: 52428800 #50 MB per second uploadDiskConcurrency: 2 storage: type: s3Compatible s3Compatible: bucket: stackgres region: k8s enablePathStyleAddressing: true endpoint: http://my-cluster-minio:9000 awsCredentials: secretKeySelectors: accessKeyId: key: accesskey name: my-cluster-minio secretAccessKey: key: secretkey name: my-cluster-minio Base Backups    Property Required Updatable Type Default Description     retention  ✓ integer 5 When an automatic retention policy is defined to delete old base backups, this parameter specifies the number of base backups to keep, in a sliding window.\nConsequently, the time range covered by backups is periodicity*retention, where periodicity is the separation between backups as specified by the cronSchedule property.\nDefault is 5.\n    cronSchedule  ✓ string 05:00 UTC Continuous Archiving backups are composed of periodic base backups and all the WAL segments produced in between those base backups. This parameter specifies at what time and with what frequency to start performing a new base backup.\nUse cron syntax (m h dom mon dow) for this parameter, i.e., 5 values separated by spaces:\n m: minute, 0 to 59. h: hour, 0 to 23. dom: day of month, 1 to 31 (recommended not to set it higher than 28). mon: month, 1 to 12. dow: day of week, 0 to 7 (0 and 7 both represent Sunday).  Also ranges of values (start-end), the symbol * (meaning first-last) or even */N, where N is a number, meaning \u0026ldquo;\u0026ldquo;every N, may be used. All times are UTC. It is recommended to avoid 00:00 as base backup time, to avoid overlapping with any other external operations happening at this time.\nIf not set, full backups are performed each day at 05:00 UTC.\n    compression  ✓ string lz4 Specifies the backup compression algorithm. Possible options are: lz4, lzma, brotli. The default method is lz4. LZ4 is the fastest method, but compression ratio is the worst. LZMA is way slower, but it compresses backups about 6 times better than LZ4. Brotli is a good trade-off between speed and compression ratio, being about 3 times better than LZ4.    performance  ✓ object       Base Backup Performance    Property Required Updatable Type Default Description     maxDiskBandwitdh  ✓ integer unlimited Maximum disk read I/O when performing a backup. In bytes (per second).    maxNetworkBandwitdh  ✓ integer unlimited Maximum storage upload bandwidth used when storing a backup. In bytes (per second).    uploadDiskConcurrency  ✓ integer 1 Backup storage may use several concurrent streams to store the data. This parameter configures the number of parallel streams to use. By default, it\u0026rsquo;s set to 1 (use one stream).     Storage Configuration    Property Required Updatable Type Default Description     type ✓ ✓ string  Determine the type of object storage used for storing the base backups and WAL segments. Possible values:\n s3: Amazon Web Services S3 (Simple Storage Service). s3Compatible: non-AWS services that implement a compatibility API with AWS S3. gcs: Google Cloud Storage. azureBlob: Microsoft Azure Blob Storage.      s3 if type = s3 ✓ object  Amazon Web Services S3 configuration.    s3Compatible if type = s3Compatible ✓ object  AWS S3-Compatible API configuration    gcs if type = gcs ✓ object  Google Cloud Storage configuration.    azureBlob if type = azureblob ✓ object  Azure Blob Storage configuration.     S3 S3 - Amazon Web Services S3 configuration    Property Required Updatable Type Default Description     bucket ✓ ✓ string  AWS S3 bucket name.    path  ✓ string  Optional path within the S3 bucket. Note that StackGres generates in any case a folder per StackGres cluster, using the SGCluster.metadata.name.    awsCredentials ✓ ✓ object  The credentials to access AWS S3 for writing and reading.    region  ✓ string  The AWS S3 region. The Region may be detected using s3:GetBucketLocation, but if you wish to avoid giving permissions to this API call or forbid it from the applicable IAM policy, you must then specify this property.    storageClass  ✓ string  The Amazon S3 Storage Class to use for the backup object storage. By default, the STANDARD storage class is used. Other supported values include STANDARD_IA for Infrequent Access and REDUCED_REDUNDANCY.     S3 - Amazon Web Services S3 Compatible configuration    Property Required Updatable Type Default Description     bucket ✓ ✓ string  Bucket name.    path  ✓ string  Optional path within the S3 bucket. Note that StackGres generates in any case a folder per StackGres cluster, using the SGCluster.metadata.name.    awsCredentials ✓ ✓ object  The credentials to access AWS S3 for writing and reading.    region  ✓ string  The AWS S3 region. The Region may be detected using s3:GetBucketLocation, but if you wish to avoid giving permissions to this API call or forbid it from the applicable IAM policy, you must then specify this property.    storageClass  ✓ string  The Amazon S3 Storage Class to use for the backup object storage. By default, the STANDARD storage class is used. Other supported values include STANDARD_IA for Infrequent Access and REDUCED_REDUNDANCY.    endpoint  ✓ string  Overrides the default url to connect to an S3-compatible service. For example: http://s3-like-service:9000.    enablePathStyleAddressing  ✓ boolean  Enable path-style addressing (i.e. http://s3.amazonaws.com/BUCKET/KEY) when connecting to an S3-compatible service that lacks support for sub-domain style bucket URLs (i.e. http://BUCKET.s3.amazonaws.com/KEY).\nDefaults to false.\n     Amazon Web Services Credentials    Property Required Updatable Type Default Description     secretKeySelectors ✓ ✓ object  Kubernetes SecretKeySelector(s) to reference the Secret(s) that contain the information about the awsCredentials. Note that you may use the same or different Secrets for the accessKeyId and the secretAccessKey. In the former case, the keys that identify each must be, obviously, different.     Amazon Web Services Secret Key Selector    Property Required Updatable Type Default Description     accessKeyId ✓ ✓ object  AWS access key ID. For example, AKIAIOSFODNN7EXAMPLE.    secretAccessKey ✓ ✓ object  AWS secret access key. For example, wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY.     GSC - Google Cloud Storage configuration    Property Required Updatable Type Default Description     bucket ✓ ✓ string  GCS bucket name.    path  ✓ string  Optional path within the GCS bucket. Note that StackGres generates in any case a folder per StackGres cluster, using the SGCluster.metadata.name.    gcpCredentials ✓ ✓ object  The credentials to access GCS for writing and reading.     GCP Credentials    Property Required Updatable Type Default Description     secretKeySelectors ✓ ✓ object  A Kubernetes SecretKeySelector to reference the Secrets that contain the information about the Service Account to access GCS.     GCP Secret Key Selector    Property Required Updatable Type Default Description     serviceAccountJSON ✓ ✓ object  A service account key from GCP. In JSON format, as downloaded from the GCP Console.     AZURE - Azure Blob Storage configuration    Property Required Updatable Type Default Description     bucket ✓ ✓ string  Azure Blob Storage bucket name.    path  ✓ string  Optional path within the Azure Blob bucket. Note that StackGres generates in any case a folder per StackGres cluster, using the SGCluster.metadata.name.    azureCredentials ✓ ✓ object  The credentials to access Azure Blob Storage for writing and reading.     Azure Credentials    Property Required Updatable Type Default Description     secretKeySelectors ✓ ✓ object  Kubernetes SecretKeySelector(s) to reference the Secret(s) that contain the information about the azureCredentials. . Note that you may use the same or different Secrets for the storageAccount and the accessKey. In the former case, the keys that identify each must be, obviously, different.     Azure Secret Key Selector    Property Required Updatable Type Default Description     storageAccount ✓ ✓ object  The Storage Account that contains the Blob bucket to be used.    accessKey ✓ ✓ object  The storage account access key.     "
},
{
	"uri": "https://stackgres.io/doc/0.9/install/helm/upgrade/",
	"title": "Upgrade via Helm",
	"tags": [],
	"description": "",
	"content": "Upgrade Operator Upgrade the operator with the following command:\nhelm upgrade --namespace stackgres stackgres-operator \\  --values my-operator-values.yml \\  https://stackgres.io/downloads/stackgres-k8s/stackgres/0.9.5/helm/stackgres-operator.tgz  Important note: Do not use the --reuse-values option from Helm, this prevent the new operator resources and the new values needed to be set. Pass your installation params using the values file or setting the values directly in the command using --set-string option.\n The main recommendation is to pass the same installation values in the upgrade command or using a values.yaml.\nUpgrade of an operator can serve two purpose:\n Configuration change Operator upgrade  Operator upgrade After the upgrade completes any new cluster that will be created, will be created with the new updated components. For existing clusters, there are two mechanisms in order to update components: in-place restart and reduced-impact restart. Both procedures are essentially the same but reduced-impact restart allow to restart a cluster with minimal throughput reduction for read-only connections (we will not apply draining here) or for read-write connections when a single node clusters is used.\nFor more details please see the cluster restart section\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/install/cluster/parameters/",
	"title": "Cluster Parameters",
	"tags": [],
	"description": "",
	"content": "AdminUI Parameters You can specify following parameters values:\n   Parameter Description Default     adminui.service.type The type used for the service of the UI:\n Set to LoadBalancer to create a load balancer (if supported by the kubernetes cluster) to allow connect from Internet to the UI. Note that enabling this feature will probably incurr in some fee that depend on the host of the kubernetes cluster (for example this is true for EKS, GKE and AKS). Set to NodePort to expose admin UI from kubernetes nodes.    ClusterIP   adminui.service.loadBalancerIP LoadBalancer will get created with the IP specified in this field. This feature depends on whether the underlying cloud-provider supports specifying the loadBalancerIP when a load balancer is created. This field will be ignored if the cloud-provider does not support the feature.     adminui.service.loadBalancerSourceRanges If specified and supported by the platform, this will restrict traffic through the cloud-provider load-balancer will be restricted to the specified client IPs. This field will be ignored if the cloud-provider does not support the feature. More info: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/     adminui.service.nodePort The port used to expose the service on kubernetes nodes     authentication.user Username that will be required to access the UI.  admin   authentication.password Password that will be required to access the UI.  Autogenerated random value    Grafana Parameters    Parameter Description Default     grafana.autoEmbed Embed an existing grafana by setting grafana.autoEmbed to true  true   grafana.schema The schema to access grafana. By default http. (used to embed manually and automatically grafana)  http   grafana.webHost The service host name to access grafana (used to embed manually and automatically grafana). The parameter value should point to the grafana service following the DNS reference svc_name.namespace     grafana.user The username to access grafana. By default admin. (used to embed automatically grafana)     grafana.password The password to access grafana. By default prom-operator (the default in for kube-prometheus-stack helm chart). (used to embed automatically grafana)     grafana.secretNamespace The namespace of secret with credentials to access grafana. (used to embed automatically grafana, alternative to use grafana.user and grafana.password)     grafana.secretName The name of secret with credentials to access grafana. (used to embed automatically grafana, alternative to use grafana.user and grafana.password)     grafana.secretUserKey The key of secret with username used to access grafana. (used to embed automatically grafana, alternative to use grafana.user and grafana.password)     grafana.secretPasswordKey The key of secret with password used to access grafana. (used to embed automatically grafana, alternative to use grafana.user and grafana.password)     grafana.datasourceName The datasource name used by dashboard that will be created in grafana. By default Prometheus. (used to embed automatically grafana)  Prometheus   grafana.dashboardConfigMap {{ \u0026lt; description stackgres-operator.grafana.dashboardConfigMap \u0026gt; }}    grafana.dashboardId The dashboard id that will be create in grafana (see https://grafana.com/grafana/dashboards). By default 9628. (used to embed automatically grafana)     grafana.url The URL of the PostgreSQL dashboard created in grafana (used to embed manually grafana)     grafana.token The grafana API token to access the PostgreSQL dashboard created in grafana (used to embed manually grafana)      Prometheus Parameters    Parameter Description Default     prometheus.allowAutobind If set to false disable automatic bind to prometheus created using the prometheus operator. If disabled the cluster will not be binded to prometheus automatically and will require manual interventin by the kubernetes cluster administrator.  true    Certificates Parameters    Parameter Description Default     cert.autoapprove If set to false disable automatic approve of certificate used by the operator. If disabled the operator installation will not complete until the certificate is approved by the kubernetes cluster administrator.  true   cert.key The private RSA key used to generated the cert.crt certificate that uses the kubernetes cluster CA.  true   cert.crt The certificate that is generated using the cert.key private RSA key and uses the kubernetes cluster CA.  true   cert.jwtRsaKey The private RSA key used to generate JWTs used in REST API authentication.  true   cert.jwtRsaPub The public RSA key used to verify JWTs used in REST API authentication.  true    Configuration Cluster Parameters You can specify following parameters values:\n   Parameter Description Default     cluster.create If false does not create the cluster (useful to just create configurations).  true   cluster.postgresVersion Postgres version used on the cluster. It is either of:\n The string \u0026lsquo;latest\u0026rsquo;, which automatically sets the latest major.minor Postgres version. A major version, like \u0026lsquo;12\u0026rsquo; or \u0026lsquo;11\u0026rsquo;, which sets that major version and the latest minor version. A specific major.minor version, like \u0026lsquo;12.2\u0026rsquo;.    12.2   cluster.instances Number of StackGres instances for the cluster. Each instance contains one Postgres server. Out of all of the Postgres servers, one is elected as the master, the rest remain as read-only replicas.  1   cluster.sgInstanceProfile Name of the SGInstanceProfile. A SGInstanceProfile defines CPU and memory limits. Must exist before creating a cluster. When no profile is set, a default (currently: 1 core, 2 GiB RAM) one is used.  size-s   cluster.configurations.sgPostgresConfig Name of the SGPostgresConfig used for the cluster. It must exist. When not set, a default Postgres config, for the major version selected, is used.  postgresconfig   cluster.configurations.sgPoolingConfig Name of the SGPoolingConfig used for this cluster. Each pod contains a sidecar with a connection pooler (currently: PgBouncer). The connection pooler is implemented as a sidecar.\nIf not set, a default configuration will be used. Disabling connection pooling altogether is possible if the disableConnectionPooling property of the pods object is set to true.\n  poolingconfig   cluster.configurations.sgBackupConfig Name of the SGBackupConfig to use for the cluster. It defines the backups policy, storage and retention, among others, applied to the cluster. When not set, a default backup config is used.  backupconfig   cluster.prometheusAutobind If enabled, a ServiceMonitor is created for each Prometheus instance found in order to collect metrics.  true   instanceProfiles An array of instance profiles (see instance profiles), if null or empty does not create any instance profile.  See instance profiles   configurations.create If false does not create configuration CRs.  true   configurations.postgresconfig The PostgreSQL configuration CR name (see postgres configuration).  See postgres configuration   configurations.poolingconfig The connection pooling configuration CR name (see connection pooling configuration).  See connection pooling configuration   configurations.backupconfig The backup configuration CR name (see backup configuration).  See backup configuration    Postgres Services    Parameter Description Default     cluster.postgresServices.primary.enabled Specify if the -primary service should be created or not.  true   cluster.postgresServices.primary.type Specifies the type of Kubernetes service.  ClusterIP   cluster.postgresServices.primary.annotations Custom Kubernetes annotations passed to the -primary service.     cluster.postgresServices.replicas.enabled Specify if the -replicas service should be created or not.  true   cluster.postgresServices.replicas.type Specifies the type of Kubernetes service.  ClusterIP   cluster.postgresServices.replicas.annotations Custom Kubernetes annotations passed to the -replicas service.      Pods    Parameter Description Default     cluster.pods.persistentVolume.size Size of the PersistentVolume set for each instance of the cluster. This size is specified either in Mebibytes, Gibibytes or Tebibytes (multiples of 2^20, 2^30 or 2^40, respectively).  5Gi   cluster.pods.persistentVolume.storageclass Name of an existing StorageClass in the Kubernetes cluster, used to create the PersistentVolumes for the instances of the cluster.     cluster.pods.disableConnectionPooling If set to true, avoids creating a connection pooling (using PgBouncer) sidecar.  false   cluster.pods.disableMetricsExporter If set to true, avoids creating the Prometheus exporter sidecar. Recommended when there\u0026rsquo;s no intention to use Prometheus for monitoring.  false   cluster.pods.disablePostgresUtil If set to true, avoids creating the postgres-util sidecar. This sidecar contains usual Postgres administration utilities that are not present in the main (patroni) container, like psql. Only disable if you know what you are doing.  false   cluster.pods.metadata.labels Additional labels for StackGres Pods.     cluster.pods.scheduling.nodeSelector Pod custom node selector.      Resources metadata    Parameter Description Default     cluster.metadata.annotations.allResources Annotations to attach to any resource created or managed by StackGres.     cluster.metadata.annotations.pods Annotations to attach to pods created or managed by StackGres.     cluster.metadata.annotations.services Annotations to attach to services created or managed by StackGres.  false    Instance profiles    Parameter Description Default     instanceProfiles.\u0026lt;index\u0026gt;.name Name of the Instance Profile. An instance profile represents a \u0026ldquo;\u0026ldquo;kind\u0026rdquo;\u0026rdquo; of server (CPU and RAM) where you may run StackGres, classified by a given name. The profile may be referenced by zero or more SGClusters, and if so it would be referenced by its name. Following Kubernetes naming conventions, it must be an rfc1035/rfc1123 subdomain, that is, up to 253 characters consisting of one or more lowercase labels separated by .. Where each label is an alphanumeric (a-z, and 0-9) string, with a maximum length of 63 characters, with the - character allowed anywhere except the first or last character.\nThe name must be unique across all instance profiles in the same namespace.\u0026quot;\n  See below   instanceProfiles.\u0026lt;index\u0026gt;.cpu CPU(s) (cores) used for every instance of a SGCluster. Please note that every StackGres pod contains not only the Patroni+Postgres container, but several other sidecar containers. While the majority of the resources are devoted to the main Postgres container, some CPU is needed for the sidecars.\nThe number of cores set is split between all the containers.\nA minimum of 2 cores is recommended.\n  See below   instanceProfiles.\u0026lt;index\u0026gt;.memory RAM allocated to every instance of a SGCluster. The suffix Mi or Gi specifies Mebibytes or Gibibytes, respectively. Please note that every StackGres pod contains not only the Patroni+Postgres container, but several other sidecar containers. While the majority of the resources are devoted to the main Postgres container, some RAM is needed for the sidecars.\nThe amount of RAM set is split between all the containers.\nA minimum of 2-4Gi is recommended.\n  See below    By default following profiles are created:\ninstanceProfiles: - name: size-s cpu: \u0026#34;500m\u0026#34; memory: \u0026#34;512Mi\u0026#34; - name: size-s cpu: \u0026#34;1\u0026#34; memory: \u0026#34;2Gi\u0026#34; - name: size-m cpu: \u0026#34;2\u0026#34; memory: \u0026#34;4Gi\u0026#34; - name: size-l cpu: \u0026#34;4\u0026#34; memory: \u0026#34;8Gi\u0026#34; - name: size-xl cpu: \u0026#34;6\u0026#34; memory: \u0026#34;16Gi\u0026#34; - name: size-xxl cpu: \u0026#34;8\u0026#34; memory: \u0026#34;32Gi\u0026#34; Postgres configuration    Parameter Description Default     configurations.postgresconfig.postgresql\\.conf The postgresql.conf parameters the configuration contains, represented as an object where the keys are valid names for the postgresql.conf configuration file parameters of the given postgresVersion. You may check postgresqlco.nf as a reference on how to tune and find the valid parameters for a given major version.  See below    By default following parameters are specified:\nconfigurations: postgresconfig: postgresql.conf: shared_buffers: \u0026#39;256MB\u0026#39; random_page_cost: \u0026#39;1.5\u0026#39; password_encryption: \u0026#39;scram-sha-256\u0026#39; wal_compression: \u0026#39;on\u0026#39; checkpoint_timeout: \u0026#39;30\u0026#39; Connection pooling configuration    Parameter Description Default     configurations.poolingconfig.pgBouncer.pgbouncer\\.ini The pgbouncer.ini parameters the configuration contains, represented as an object where the keys are valid names for the pgbouncer.ini configuration file parameters.\nCheck pgbouncer configuration for more information about supported parameters.\n  See below    By default following parameters are specified:\nconfigurations: poolingconfig: pgBouncer: pgbouncer.ini: pool_mode: transaction max_client_conn: \u0026#39;200\u0026#39; default_pool_size: \u0026#39;200\u0026#39; Backup configuration By default the chart create a storage class backed by an MinIO server. To avoid the creation of the MinIO server set nonProductionOptions.createMinio to false and fill any of the configurations.backupconfig.storage.s3, configurations.backupconfig.storage.gcs or configurations.backupconfig.storage.azureBlob sections.\n   Parameter Description Default     configurations.backupconfig.create If true create and set the backup configuration for the cluster.  true   configurations.backupconfig.baseBackups.retention When an automatic retention policy is defined to delete old base backups, this parameter specifies the number of base backups to keep, in a sliding window.\nConsequently, the time range covered by backups is periodicity*retention, where periodicity is the separation between backups as specified by the cronSchedule property.\nDefault is 5.\n  5   configurations.backupconfig.baseBackups.cronSchedule Continuous Archiving backups are composed of periodic base backups and all the WAL segments produced in between those base backups. This parameter specifies at what time and with what frequency to start performing a new base backup.\nUse cron syntax (m h dom mon dow) for this parameter, i.e., 5 values separated by spaces:\n m: minute, 0 to 59. h: hour, 0 to 23. dom: day of month, 1 to 31 (recommended not to set it higher than 28). mon: month, 1 to 12. dow: day of week, 0 to 7 (0 and 7 both represent Sunday).  Also ranges of values (start-end), the symbol * (meaning first-last) or even */N, where N is a number, meaning \u0026ldquo;\u0026ldquo;every N, may be used. All times are UTC. It is recommended to avoid 00:00 as base backup time, to avoid overlapping with any other external operations happening at this time.\nIf not set, full backups are performed each day at 05:00 UTC.\n  Each 2 minutes   configurations.backupconfig.baseBackups.compression Specifies the backup compression algorithm. Possible options are: lz4, lzma, brotli. The default method is lz4. LZ4 is the fastest method, but compression ratio is the worst. LZMA is way slower, but it compresses backups about 6 times better than LZ4. Brotli is a good trade-off between speed and compression ratio, being about 3 times better than LZ4.  lz4   configurations.backupconfig.baseBackups.performance.uploadDiskConcurrency Backup storage may use several concurrent streams to store the data. This parameter configures the number of parallel streams to use. By default, it\u0026rsquo;s set to 1 (use one stream).  1   configurations.backupconfig.baseBackups.performance.maxNetworkBandwitdh Maximum disk read I/O when performing a backup. In bytes (per second).  unlimited   configurations.backupconfig.baseBackups.performance.maxDiskBandwitdh Maximum storage upload bandwidth used when storing a backup. In bytes (per second).  unlimited    Amazon Web Services S3    Parameter Description Default     configurations.backupconfig.storage.s3.bucket AWS S3 bucket name.     configurations.backupconfig.storage.s3.path Optional path within the S3 bucket. Note that StackGres generates in any case a folder per StackGres cluster, using the SGCluster.metadata.name.     configurations.backupconfig.storage.s3.awsCredentials.secretKeySelectors.accessKeyId AWS access key ID. For example, AKIAIOSFODNN7EXAMPLE.     configurations.backupconfig.storage.s3.awsCredentials.secretKeySelectors.accessKeyId.name Name of the referent. More information.     configurations.backupconfig.storage.s3.awsCredentials.secretKeySelectors.accessKeyId.key The key of the secret to select from. Must be a valid secret key.     configurations.backupconfig.storage.s3.awsCredentials.secretKeySelectors.secretAccessKey AWS secret access key. For example, wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY.     configurations.backupconfig.storage.s3.awsCredentials.secretKeySelectors.secretAccessKey.name Name of the referent. More information.     configurations.backupconfig.storage.s3.awsCredentials.secretKeySelectors.secretAccessKey.key The key of the secret to select from. Must be a valid secret key.     configurations.backupconfig.storage.s3.region The AWS S3 region. The Region may be detected using s3:GetBucketLocation, but if you wish to avoid giving permissions to this API call or forbid it from the applicable IAM policy, you must then specify this property.     configurations.backupconfig.storage.s3.storageClass The Amazon S3 Storage Class to use for the backup object storage. By default, the STANDARD storage class is used. Other supported values include STANDARD_IA for Infrequent Access and REDUCED_REDUNDANCY.      Amazon Web Services S3 Compatible    Parameter Description Default     configurations.backupconfig.storage.s3Compatible.bucket Bucket name.     configurations.backupconfig.storage.s3Compatible.path Optional path within the S3 bucket. Note that StackGres generates in any case a folder per StackGres cluster, using the SGCluster.metadata.name.     configurations.backupconfig.storage.s3Compatible.bucket The AWS S3 bucket (eg. bucket).    configurations.backupconfig.storage.s3Compatible.path The AWS S3 bucket path (eg. /path/to/folder).    configurations.backupconfig.storage.s3Compatible.awsCredentials.secretKeySelectors.accessKeyId AWS access key ID. For example, AKIAIOSFODNN7EXAMPLE.     configurations.backupconfig.storage.s3Compatible.awsCredentials.secretKeySelectors.accessKeyId.name Name of the referent. More information.     configurations.backupconfig.storage.s3Compatible.awsCredentials.secretKeySelectors.accessKeyId.key The key of the secret to select from. Must be a valid secret key.     configurations.backupconfig.storage.s3Compatible.awsCredentials.secretKeySelectors.secretAccessKey AWS secret access key. For example, wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY.     configurations.backupconfig.storage.s3Compatible.awsCredentials.secretKeySelectors.secretAccessKey.name Name of the referent. More information.     configurations.backupconfig.storage.s3Compatible.awsCredentials.secretKeySelectors.secretAccessKey.key The key of the secret to select from. Must be a valid secret key.     configurations.backupconfig.storage.s3Compatible.region The AWS S3 region. The Region may be detected using s3:GetBucketLocation, but if you wish to avoid giving permissions to this API call or forbid it from the applicable IAM policy, you must then specify this property.     configurations.backupconfig.storage.s3Compatible.storageClass The Amazon S3 Storage Class to use for the backup object storage. By default, the STANDARD storage class is used. Other supported values include STANDARD_IA for Infrequent Access and REDUCED_REDUNDANCY.     configurations.backupconfig.storage.s3Compatible.endpoint Overrides the default url to connect to an S3-compatible service. For example: http://s3-like-service:9000.     configurations.backupconfig.storage.s3Compatible.enablePathStyleAddressing Enable path-style addressing (i.e. http://s3.amazonaws.com/BUCKET/KEY) when connecting to an S3-compatible service that lacks support for sub-domain style bucket URLs (i.e. http://BUCKET.s3.amazonaws.com/KEY).\nDefaults to false.\n      Google Cloud Storage    Parameter Description Default     configurations.backupconfig.storage.gcs.bucket GCS bucket name.     configurations.backupconfig.storage.gcs.path Optional path within the GCS bucket. Note that StackGres generates in any case a folder per StackGres cluster, using the SGCluster.metadata.name.     configurations.backupconfig.storage.gcs.gcpCredentials.secretKeySelectors.serviceAccountJSON A service account key from GCP. In JSON format, as downloaded from the GCP Console.     configurations.backupconfig.storage.gcs.gcpCredentials.secretKeySelectors.serviceAccountJSON.name Name of the referent. More information.     configurations.backupconfig.storage.gcs.gcpCredentials.secretKeySelectors.serviceAccountJSON.key The key of the secret to select from. Must be a valid secret key.      Azure Blob Storage    Parameter Description Default     configurations.backupconfig.storage.azureBlob.bucket Azure Blob Storage bucket name.     configurations.backupconfig.storage.azureBlob.path Optional path within the Azure Blob bucket. Note that StackGres generates in any case a folder per StackGres cluster, using the SGCluster.metadata.name.     configurations.backupconfig.storage.azureBlob.azureCredentials.secretKeySelectors.storageAccount The Storage Account that contains the Blob bucket to be used.     configurations.backupconfig.storage.azureBlob.azureCredentials.secretKeySelectors.storageAccount.name Name of the referent. More information.     configurations.backupconfig.storage.azureBlob.azureCredentials.secretKeySelectors.storageAccount.key The key of the secret to select from. Must be a valid secret key.     configurations.backupconfig.storage.azureBlob.azureCredentials.secretKeySelectors.accessKey The storage account access key.     configurations.backupconfig.storage.azureBlob.azureCredentials.secretKeySelectors.accessKey.name Name of the referent. More information.     configurations.backupconfig.storage.azureBlob.azureCredentials.secretKeySelectors.accessKey.key The key of the secret to select from. Must be a valid secret key.      Restore configuration By default, stackgres creates as an empty database. To create a cluster with data from an existent backup, we have the restore options. It works, by simply indicating the backup CR Uid that we want to restore.\n   Parameter Description Default     cluster.initialData.restore.fromBackup When set to the UID of an existing SGBackup, the cluster is initialized by restoring the backup data to it. If not set, the cluster is initialized empty. The selected backup must be in the same namespace.     cluster.initialData.restore.downloadDiskConcurrency The backup fetch process may fetch several streams in parallel. Parallel fetching is enabled when set to a value larger than one.      Scripts configuration By default, stackgres creates as an empty database. To execute some scripts, we have the scripts options where you can specify a script or reference a key in a ConfigMap or a Secret that contains the script to execute.\n   Parameter Description Default     cluster.initialData.scripts.\u0026lt;index\u0026gt;.name Name of the script. Must be unique across this SGCluster.     cluster.initialData.scripts.\u0026lt;index\u0026gt;.database Database where the script is executed. Defaults to the postgres database, if not specified.  postgres   cluster.initialData.scripts.\u0026lt;index\u0026gt;.script Raw SQL script to execute. This field is mutually exclusive with scriptFrom field.     cluster.initialData.scripts.\u0026lt;index\u0026gt;.scriptFrom Reference to either a Kubernetes Secret or a ConfigMap that contains the SQL script to execute. This field is mutually exclusive with script field.\nFields secretKeyRef and configMapKeyRef are mutually exclusive, and one of them is required.\n     cluster.initialData.scripts.\u0026lt;index\u0026gt;.scriptFrom.configMapKeyRef A ConfigMap reference that contains the SQL script to execute. This field is mutually exclusive with secretKeyRef field.     cluster.initialData.scripts.\u0026lt;index\u0026gt;.scriptFrom.configMapKeyRef.name The name of the ConfigMap that contains the SQL script to execute.     cluster.initialData.scripts.\u0026lt;index\u0026gt;.scriptFrom.configMapKeyRef.key The key name within the ConfigMap that contains the SQL script to execute.     cluster.initialData.scripts.\u0026lt;index\u0026gt;.scriptFrom.secretKeyRef A Kubernetes SecretKeySelector that contains the SQL script to execute. This field is mutually exclusive with configMapKeyRef field.     cluster.initialData.scripts.\u0026lt;index\u0026gt;.scriptFrom.secretKeyRef.name Name of the referent. More information.     cluster.initialData.scripts.\u0026lt;index\u0026gt;.scriptFrom.secretKeyRef.key The key of the secret to select from. Must be a valid secret key.      Distributed logs By default, stackgres send logs to container stdout. To send logs to a distributed logs create a distributed logs cluster and configure the cluster to use it by setting distributedLogs.enabled to true.\n   Parameter Description Default     cluster.distributedLogs.sgDistributedLogs Name of the SGDistributedLogs to use for this cluster. It must exist.  distributedlogs   distributedLogs.enabled It enables distributed logs cluster creation and configuration in order to be used by the cluster.  false   distributedLogs.create It controls the creation of the distirbuted logs cluster. If set to false the distributed logs cluster will not be created and it must already exists in order to be used if distributedLogs.enabled is set to true.  true   distributedLogs.persistentVolume.size Size of the PersistentVolume set for the pod of the cluster for distributed logs. This size is specified either in Mebibytes, Gibibytes or Tebibytes (multiples of 2^20, 2^30 or 2^40, respectively).  5Gi   distributedLogs.persistentVolume.storageClass Name of an existing StorageClass in the Kubernetes cluster, used to create the PersistentVolumes for the instances of the cluster.      Non production options The following options should NOT be enabled in a production environment.\n   Parameter Description Default     nonProductionOptions.disableClusterPodAntiAffinity It is a best practice, on non-containerized environments, when running production workloads, to run each database server on a different server (virtual or physical), i.e., not to co-locate more than one database server per host.\nThe same best practice applies to databases on containers. By default, StackGres will not allow to run more than one StackGres pod on a given Kubernetes node. Set this property to true to allow more than one StackGres pod per node.\n  true   nonProductionOptions.createMinio If true create a MinIO server that will be used to store backups.  true    "
},
{
	"uri": "https://stackgres.io/doc/0.9/developer/stackgres/contrib/",
	"title": "Contributing guide",
	"tags": [],
	"description": "",
	"content": "StackGres is open source, and as such we welcome any external contribution, in the form of feedback, testing, resources, documentation and, of course, code. Merge requests are always welcome.\nPlease observe the following rules when contributing to StackGres:\n  Create an issue with any question or improvements about the source code, to keep the discussion organized.\n  Contact us at stackgres at ongres dot com before sending a pull request, to have a contributor agreement signed with us. This is a requirement for your merge request to be merged upstream.\n  Changes and merge requests should be performed from the development branch, instead of master. Please adhere as much as possible to the apparent style of the code you are editing.\n  "
},
{
	"uri": "https://stackgres.io/doc/0.9/reference/crd/",
	"title": "CRD Reference",
	"tags": [],
	"description": "",
	"content": "Chapter 6 CRD Reference Alongside with the Administration Guide StackGres provides a detailed list of each CRD entry.\n SGCluster  Details about SGCluster configurations\n   SGInstanceProfile  Details about SGInstanceProfile configurations\n   SGPostgresConfig  Details about SGPostgresConfig configurations\n   SGPoolingConfig  Details about SGPoolingConfig configurations\n   SGBackupConfig  Details about SGBackupConfig configurations\n   SGBackup  Details about SGBackup configurations\n   SGDistributedLogs  Details about SGDistributedLogs configurations\n   "
},
{
	"uri": "https://stackgres.io/doc/0.9/tutorial/complete-cluster/create-cluster/",
	"title": "Create the Cluster",
	"tags": [],
	"description": "",
	"content": "In the section Create a simple cluster it was already presented how to create a simple cluster. Here a more advanced cluster will be created, referencing all the configurations and infrastructure already prepared.\nFor more information, review the SGCluster CRD specification. Create the file sgcluster-cluster1.yaml with the following content:\napiVersion: stackgres.io/v1 kind: SGCluster metadata: namespace: demo name: cluster spec: postgresVersion: \u0026#39;12.3\u0026#39; instances: 3 sgInstanceProfile: \u0026#39;size-small\u0026#39; pods: persistentVolume: size: \u0026#39;10Gi\u0026#39; configurations: sgPostgresConfig: \u0026#39;pgconfig1\u0026#39; sgPoolingConfig: \u0026#39;poolconfig1\u0026#39; sgBackupConfig: \u0026#39;backupconfig1\u0026#39; distributedLogs: sgDistributedLogs: \u0026#39;distributedlogs\u0026#39; prometheusAutobind: true and deploy to Kubernetes:\nkubectl apply -f sgcluster-cluster1.yaml You may watch pod and container creation:\nkubectl -n demo get pods --watch or from the Web Console:\nWhile the cluster is being created, you may notice a blip on the distributed logs server, where a container is restarted. This is a normal process, and does only pause temporarily the collection of logs (no logs are lost, since they are buffered on the source pods). This is caused by a re-configuration which requires a container restart.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/developer/stackgres/debug/",
	"title": "Debug",
	"tags": [],
	"description": "",
	"content": "Enable Kubernetes Client Logs Kubernetes client logs uses OkHttp library to perform operations with the Kubernetes API. To enable logs and see what it is sent and what is received set the JAVA_OPTS environment variable with value -Dquarkus.log.category.\\\u0026quot;okhttp3.logging.HttpLoggingInterceptor\\\u0026quot;.level=TRACE for the operator Deployment:\nkubectl set env -n stackgres deployment/stackgres-operator JAVA_OPTS=-Dquarkus.log.category.\\\u0026#34;okhttp3.logging.HttpLoggingInterceptor\\\u0026#34;.level=TRACE "
},
{
	"uri": "https://stackgres.io/doc/0.9/install/prerequisites/services-mesh-integration/",
	"title": "Service mesh integration",
	"tags": [],
	"description": "",
	"content": "Service mesh integration Details about the different options to integrate StackGres with some service mesh implementations.\n Istio  Details about how to work in a k8s cluster with Istio    "
},
{
	"uri": "https://stackgres.io/doc/0.9/intro/versions/",
	"title": "Versions",
	"tags": [],
	"description": "",
	"content": "StackGres 0.9.5    Component Versions     PostgreSQL 12.6, 12.4, 12.3, 11.11, 11.9 and 11.8   Patroni 1.6.5   WAL-G 0.2.19   PgBouncer 1.13.0   Postgres Exporter 0.8.0   Envoy 1.15.3   Fluent-Bit 1.4.6   Fluentd 1.9.3    Additional extensions included on StackGres All extensions below are available for both PostgreSQL 11 and 12:\n   Extension Version     postgis 3.0.1   timescaledb 1.7.4   pgbouncer_fdw 0.2   pg_sphere 1.0   pg_repack 1.4.6   pg_healpix 1.0   q3c 2.0.0    StackGres 0.9.4    Component Versions     PostgreSQL 12.4, 12.3, 11.8 and 11.9   Patroni 1.6.5   WAL-G 0.2.15   PgBouncer 1.13.0   Postgres Exporter 0.8.0   Envoy 1.15.0   Fluent-Bit 1.4.4   Fluentd 1.9.3    Additional extensions included on StackGres All extensions below are available for both PostgreSQL 11 and 12:\n   Extension Version     postgis 3.0.1   timescaledb 1.7.1   pgbouncer_fdw 0.2   pg_sphere 1.0   pg_repack 1.4.5   pg_healpix 1.0   q3c 2.0.0    StackGres 0.9.3    Component Versions     PostgreSQL 12.4, 12.3, 11.8 and 11.9   Patroni 1.6.5   WAL-G 0.2.15   PgBouncer 1.13.0   Postgres Exporter 0.8.0   Envoy 1.15.0   Fluent-Bit 1.4.4   Fluentd 1.9.3    Additional extensions included on StackGres All extensions below are available for both PostgreSQL 11 and 12:\n   Extension Version     postgis 3.0.1   timescaledb 1.7.1   pgbouncer_fdw 0.2   pg_sphere 1.0   pg_repack 1.4.5   pg_healpix 1.0   q3c 2.0.0    StackGres 0.9.2    Component Versions     PostgreSQL 12.4, 12.3, 11.8 and 11.9   Patroni 1.6.5   WAL-G 0.2.15   PgBouncer 1.13.0   Postgres Exporter 0.8.0   Envoy 1.15.0   Fluent-Bit 1.4.4   Fluentd 1.9.3    Additional extensions included on StackGres All extensions below are available for both PostgreSQL 11 and 12:\n   Extension Version     postgis 3.0.1   timescaledb 1.7.1   pgbouncer_fdw 0.2   pg_sphere 1.0   pg_repack 1.4.5   pg_healpix 1.0   q3c 2.0.0    StackGres 0.9.1    Component Versions     PostgreSQL 12.4, 12.3, 11.8 and 11.9   Patroni 1.6.5   WAL-G 0.2.15   PgBouncer 1.13.0   Postgres Exporter 0.8.0   Envoy 1.15.0   Fluent-Bit 1.4.4   Fluentd 1.9.3    Additional extensions included on StackGres All extensions below are available for both PostgreSQL 11 and 12:\n   Extension Version     postgis 3.0.1   timescaledb 1.7.1   pgbouncer_fdw 0.2   pg_sphere 1.0   pg_repack 1.4.5   pg_healpix 1.0   q3c 2.0.0    StackGres 0.9    Component Versions     PostgreSQL 12.3 and 11.8   Patroni 1.6.5   WAL-G 0.2.15   PgBouncer 1.13.0   Postgres Exporter 0.8.0   Envoy 1.15.0   Fluent-Bit 1.4.4   Fluentd 1.9.3    Additional extensions included on StackGres All extensions below are available for both PostgreSQL 11 and 12:\n   Extension Version     postgis 3.0.1   timescaledb 1.7.1   pgbouncer_fdw 0.2   pg_sphere 1.0   pg_repack 1.4.5   pg_healpix 1.0   q3c 2.0.0    StackGres 0.9-RC3    Component Versions     PostgreSQL 12.3 and 11.8   Patroni 1.6.5   WAL-G 0.2.15   PgBouncer 1.13.0   Postgres Exporter 0.8.0   Envoy 1.15.0   Fluent-Bit 1.4.4   Fluentd 1.9.3    StackGres 0.9-RC2    Component Versions     PostgreSQL 12.3 and 11.8   Patroni 1.6.5   WAL-G 0.2.15   PgBouncer 1.13.0   Postgres Exporter 0.8.0   Envoy 1.15.0   Fluent-Bit 1.4.4   Fluentd 1.9.3    StackGres 0.9-RC1    Component Versions     PostgreSQL 12.3 and 11.8   Patroni 1.6.5   WAL-G 0.2.15   PgBouncer 1.13.0   Postgres Exporter 0.8.0   Envoy 1.15.0   Fluent-Bit 1.4.4   Fluentd 1.9.3    StackGres 0.9-beta3    Component Versions     PostgreSQL 12.2 and 11.7   Patroni 1.6.5   WAL-G 0.2.15   PgBouncer 1.12.0   Postgres Exporter 0.8.0   Envoy 1.13.1   Fluent-Bit 1.4.1   Fluentd 1.9.3    StackGres 0.9-beta2    Component Versions     PostgreSQL 12.2 and 11.7   Patroni 1.6.5   WAL-G 0.2.15   PgBouncer 1.12.0   Postgres Exporter 0.8.0   Envoy 1.13.1   Fluent-Bit 1.4.1   Fluentd 1.9.3    StackGres 0.9-beta1    Component Versions     PostgreSQL 12.2 and 11.7   Patroni 1.6.5   WAL-G 0.2.15   PgBouncer 1.12.0   Postgres Exporter 0.8.0   Envoy 1.13.1   Fluent-Bit 1.4.1   Fluentd 1.9.3    StackGres 0.9-alpha1    Component Versions     PostgreSQL 12.2 and 11.7   Patroni 1.6.4   WAL-G 0.2.14   PgBouncer 1.12.0   Postgres Exporter 0.8.0   Envoy 1.13.1    StackGres 0.8    Component Versions     PostgreSQL 12.1 and 11.6   Patroni 1.6.3   WAL-G 0.2.14   PgBouncer 1.12.0   Postgres Exporter 0.8.0   Envoy 1.12.1    "
},
{
	"uri": "https://stackgres.io/doc/0.9/developer/",
	"title": "Developer documentation",
	"tags": [],
	"description": "",
	"content": "Chapter 7 Developer documentation Try the latest functionalities or enhance StackGres yourself.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/install/integrations/",
	"title": "Integrations",
	"tags": [],
	"description": "",
	"content": "Chapter 7 Integrations Discover what StackGres is all about integrations with different k8s platforms.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/demo/setenv/kind/",
	"title": "Kind",
	"tags": [],
	"description": "",
	"content": "Kind is a tool for running local Kubernetes clusters using Docker container \u0026ldquo;nodes\u0026rdquo;. kind is primarily designed for testing Kubernetes 1.11+, initially targeting the conformance tests.\n  Download kind \u0026amp; install executable:\nsudo wget -q -L -O /usr/local/bin/kind https://github.com/kubernetes-sigs/kind/releases/download/v0.8.1/kind-$(uname)-amd64 sudo chmod a+x /usr/local/bin/kind   Create a kind cluster\nFor newcomers and testing purposes, we recommend to use the latest version available in the 1.17 tag.\nkind create cluster --image kindest/node:v1.17.11 For checking the latest available version for the kindest/node, you can do so as follows:\ncurl https://registry.hub.docker.com/v2/repositories/kindest/node/tags/ \\  | jq \u0026#39;.results[] | select(.name|test(\u0026#34;1.17\u0026#34;)) | .name\u0026#39; Keep in mind that there is no guarantee that at some point, after some kind release, that image will not be overwritten with a new one that is not compatible with previous version. This means that to use a specific version of Kubernetes we have to include the SHA256 of the image in the name. ie, the lowest supported kindest version for StackGres is the 1.16.x family, with the following tag:\nkind create cluster --image kindest/node:v1.16.9@sha256:7175872357bc85847ec4b1aba46ed1d12fa054c83ac7a8a11f5c268957fd5765   Install NFS utility for backups:\nThis is required in order to store PostgreSQL backups in a shared NFS disk.\ndocker exec -ti kind-control-plane sh -c \u0026#39;DEBIAN_FRONTEND=noninteractive apt-get update -y -qq \u0026lt; /dev/null \u0026gt; /dev/null\u0026#39; docker exec -ti kind-control-plane sh -c \u0026#39;DEBIAN_FRONTEND=noninteractive apt-get install -y -qq nfs-common \u0026lt; /dev/null \u0026gt; /dev/null\u0026#39;   "
},
{
	"uri": "https://stackgres.io/doc/0.9/reference/crd/sgbackup/",
	"title": "SGBackup",
	"tags": [],
	"description": "Details about SGBackup configurations",
	"content": "Creating a backup The backup CR represent a backup of the cluster. Backups are created automatically by the CronJob generated using the settings in backup configuration or manually by creating a backup CR.\n Kind: SGBackup\nlistKind: SGBackupList\nplural: sgbackups\nsingular: sgbackup\n Spec\n   Property Required Updatable Type Default Description     sgCluster ✓  string  The name of the SGCluster from which this backup is/will be taken.    managedLifecycle  ✓ booolean false Indicate if this backup is permanent and should not be removed by the automated retention policy.     Example:\napiVersion: stackgres.io/v1beta1 kind: SGBackup metadata: name: backup spec: sgCluster: stackgres managedLifecycle: true status: internalName: base_00000002000000000000000E  sgBackupConfig: compression: lz4 storage: s3Compatible: awsCredentials: secretKeySelectors: accessKeyId: key: accesskey name: minio secretAccessKey: key: secretkey name: minio endpoint: http://minio:9000 enablePathStyleAddressing: true bucket: stackgres region: k8s type: s3Compatible process: status: Completed jobPod: backup-backup-q79zq managedLifecycle: true timing: start: \u0026#34;2020-01-22T10:17:24.983902Z\u0026#34; stored: \u0026#34;2020-01-22T10:17:27.183Z\u0026#34; end: \u0026#34;2020-01-22T10:17:27.165204Z\u0026#34; backupInformation: hostname: stackgres-1 systemIdentifier: \u0026#34;6784708504968245298\u0026#34; postgresVersion: \u0026#34;110006\u0026#34; pgData: /var/lib/postgresql/data size: compressed: 6691164 uncompressed: 24037844 lsn: start: \u0026#34;234881064\u0026#34; end: \u0026#34;234881272\u0026#34; startWalFile: 00000002000000000000000E Status\n   Property Type Description     internalName string The name of the backup.    process object     backupInformation object     sgBackupConfig object The name of the backup configuration used to perform this backup.     Backup Process    Property Type Description     status string Status of the backup.    jobPod string Name of the pod assigned to the backup. StackGres utilizes internally a locking mechanism based on the pod name of the job that creates the backup.    failure string If the status is failed this field will contain a message indicating the failure reason.    managedLifecycle boolean Status (may be transient) until converging to spec.managedLifecycle.    timing object      Backup Timing    Property Type Description     start string Start time of backup.    end string End time of backup.    stored string Time at which the backup is safely stored in the object storage.     Backup Information    Property Type Description     hostname (deprecated) string Hostname of the instance where the backup is taken from.    sourcePod string Pod where the backup is taken from.    systemIdentifier string Postgres system identifier of the cluster this backup is taken from.    postgresVersion string Postgres version of the server where the backup is taken from.    pgData string Data directory where the backup is taken from.    size object     lsn object     startWalFile string WAL segment file name when the backup was started.    controlData object An object containing data from the output of pg_controldata on the backup.     Backup Size    Property Type Description     compressed integer Size (in bytes) of the compressed backup.    uncompressed integer Size (in bytes) of the uncompressed backup.     Backup LSN    Property Type Description     start string LSN of when the backup started.    end string LSN of when the backup finished.     Backup Configuration    Property Required Updatable Type Default Description     compression  ✓ string lz4 Select the backup compression algorithm. Possible options are: lz4, lzma, brotli. The default method is lz4. LZ4 is the fastest method, but compression ratio is the worst. LZMA is way slower, but it compresses backups about 6 times better than LZ4. Brotli is a good trade-off between speed and compression ratio, being about 3 times better than LZ4.    storage  ✓ object  Backup storage configuration.     Storage Configuration    Property Required Updatable Type Default Description     type ✓ ✓ string  Specifies the type of object storage used for storing the base backups and WAL segments. Possible values:\n s3: Amazon Web Services S3 (Simple Storage Service). s3Compatible: non-AWS services that implement a compatibility API with AWS S3. gcs: Google Cloud Storage. azureBlob: Microsoft Azure Blob Storage.      s3 if type = s3 ✓ object  Amazon Web Services S3 configuration.    s3Compatible if type = s3Compatible ✓ object  AWS S3-Compatible API configuration    gcs if type = gcs ✓ object  Google Cloud Storage configuration.    azureBlob if type = azureblob ✓ object  Azure Blob Storage configuration.     S3 S3 - Amazon Web Services S3 configuration    Property Required Updatable Type Default Description     bucket ✓ ✓ string  AWS S3 bucket name.    path  ✓ string  Optional path within the S3 bucket. Note that StackGres generates in any case a folder per StackGres cluster, using the SGCluster.metadata.name.    awsCredentials ✓ ✓ object  Credentials to access AWS S3 for writing and reading.    region  ✓ string  AWS S3 region. The Region may be detected using s3:GetBucketLocation, but to avoid giving permissions to this API call or forbid it from the applicable IAM policy, this property must be explicitely specified.    storageClass  ✓ string  Amazon S3 Storage Class used for the backup object storage. By default, the STANDARD storage class is used. Other supported values include STANDARD_IA for Infrequent Access and REDUCED_REDUNDANCY.     S3 - Amazon Web Services S3 Compatible configuration    Property Required Updatable Type Default Description     bucket ✓ ✓ string  Bucket name.    path  ✓ string  Optional path within the S3 bucket. Note that StackGres generates in any case a folder per StackGres cluster, using the SGCluster.metadata.name.    awsCredentials ✓ ✓ object  Credentials to access AWS S3 for writing and reading.    region  ✓ string  AWS S3 region. The Region may be detected using s3:GetBucketLocation, but to avoid giving permissions to this API call or forbid it from the applicable IAM policy, this property must be explicitely specified.    storageClass  ✓ string  Amazon S3 Storage Class used for the backup object storage. By default, the STANDARD storage class is used. Other supported values include STANDARD_IA for Infrequent Access and REDUCED_REDUNDANCY.    endpoint  ✓ string  Overrides the default url to connect to an S3-compatible service. For example: http://s3-like-service:9000.    enablePathStyleAddressing  ✓ boolean  Enable path-style addressing (i.e. http://s3.amazonaws.com/BUCKET/KEY) when connecting to an S3-compatible service that lacks support for sub-domain style bucket URLs (i.e. http://BUCKET.s3.amazonaws.com/KEY). Defaults to false.     Amazon Web Services Credentials    Property Required Updatable Type Default Description     secretKeySelectors ✓ ✓ object  A Kubernetes SecretKeySelector to reference the Secrets that contain the information about the awsCredentials.     Amazon Web Services Secret Key Selector    Property Required Updatable Type Default Description     accessKeyId ✓ ✓ object  SecretKeySelector containing the AWS Access Key ID secret.    secretAccessKey ✓ ✓ object  SecretKeySelector containing the AWS Secret Access Key secret.     GSC - Google Cloud Storage configuration    Property Required Updatable Type Default Description     bucket ✓ ✓ string  GCS bucket name.    path  ✓ string  Optional path within the GCS bucket. Note that StackGres generates in any case a folder per StackGres cluster, using the SGCluster.metadata.name.    gcpCredentials ✓ ✓ object  Credentials to access GCS for writing and reading.     GCP Credentials    Property Required Updatable Type Default Description     secretKeySelectors ✓ ✓ object  A Kubernetes SecretKeySelector to reference the Secrets that contain the information about the Service Account to access GCS.     GCP Secret Key Selector    Property Required Updatable Type Default Description     serviceAccountJSON ✓ ✓ object  A service account key from GCP. In JSON format, as downloaded from the GCP Console.     AZURE - Azure Blob Storage configuration    Property Required Updatable Type Default Description     bucket ✓ ✓ string  Azure Blob Storage bucket name.    path  ✓ string  Optional path within the Azure Blobk bucket. Note that StackGres generates in any case a folder per StackGres cluster, using the SGCluster.metadata.name.    azureCredentials ✓ ✓ object  The credentials to access Azure Blob Storage for writing and reading.     Azure Credentials    Property Required Updatable Type Default Description     secretKeySelectors ✓ ✓ object  Kubernetes SecretKeySelectors to reference the Secrets that contain the information about the azureCredentials.     Azure Secret Key Selector    Property Required Updatable Type Default Description     storageAccount ✓ ✓ object  SecretKeySelector containing the name of the storage account.    accessKey ✓ ✓ object  SecretKeySelector containing the primary or secondary access key for the storage account.     "
},
{
	"uri": "https://stackgres.io/doc/0.9/api/",
	"title": "Operator API",
	"tags": [],
	"description": "",
	"content": "Chapter 8 Operator API Operator Rest API to manipulate stackgres\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/reference/crd/sgdistributedlogs/",
	"title": "SGDistributedLogs",
	"tags": [],
	"description": "Details about SGDistributedLogs configurations",
	"content": "Creating a distributed logs cluster The distributed logs CR represent a distributed logs cluster. When a cluster is configured to use a distributed logs cluster it will forward all logs from different sources to the distributed logs cluster. Under the hood, distributed log cluster use a SGCluster, therefore the distributed log cluster could be receive SQL queries in a postgres-util fashion but not with it.\nFor more information about distributed log usage please review the Distributed Log Cluster Administration Guide\n Kind: SGDistributedLogs\nlistKind: SGDistributedLogsList\nplural: sgdistributedlogs\nsingular: sgdistributedlogs\n Spec\n   Property Required Updatable Type Default Description     persistentVolume ✓  string  Pod\u0026rsquo;s persistent volume configuration    scheduling  ✓ object  Pod custom scheduling configuration.    metadata  ✓ object  Metadata information from cluster created resources.    nonProductionOptions  ✓ array       Persistent volume    Property Required Updatable Type Default Description     size ✓  string  Size of the PersistentVolume set for the pod of the cluster for distributed logs. This size is specified either in Mebibytes, Gibibytes or Tebibytes (multiples of 2^20, 2^30 or 2^40, respectively).    storageClass ✓  string default storage class Name of an existing StorageClass in the Kubernetes cluster, used to create the PersistentVolumes for the instances of the cluster.     Scheduling Holds scheduling configuration for StackGres pods to have.\n   Property Required Updatable Type Default Description     nodeSelector  ✓ object  Pod custom node selector.    tolerations  ✓ array  Pod custom node tolerations     Tolerations Holds scheduling configuration for StackGres pods to have.\n   Property Required Updatable Type Default Description     key  ✓ string  Pod custom node selector.    operator  ✓ string Equal Pod custom node tolerations    value  ✓ string  Pod custom node tolerations    effect  ✓ string match all taint effects Pod custom node tolerations     Metadata Holds custom metadata information for StackGres generated resources to have.\n   Property Required Updatable Type Default Description     annotations  ✓ object  Custom Kubernetes annotations to be passed to resources created and managed by StackGres.     Annotations Holds custom annotations for StackGres generated resources to have.\n   Property Required Updatable Type Default Description     allResources  ✓ object  Annotations to attach to any resource created or managed by StackGres.    pods  ✓ object  Annotations to attach to pods created or managed by StackGres.    services  ✓ object  Annotations to attach to services created or managed by StackGres.     apiVersion: stackgres.io/v1beta1 kind: SGDistributedLogs metadata: name: stackgres spec: pods: metadata: annotations: allResources: customAnnotations: customAnnotationValue Non Production options The following options should NOT be enabled in a production environment.\n   Property Required Updatable Type Default Description     disableClusterPodAntiAffinity  ✓ boolean false It is a best practice, on non-containerized environments, when running production workloads, to run each database server on a different server (virtual or physical), i.e., not to co-locate more than one database server per host.\nThe same best practice applies to databases on containers. By default, StackGres will not allow to run more than one StackGres or Distributed Logs pod on a given Kubernetes node. If set to true it will allow more than one StackGres pod per node.\n     Example:\napiVersion: stackgres.io/v1beta1 kind: SGDistributedLogs metadata: name: distributedlogs spec: persistentVolume: size: 10Gi "
},
{
	"uri": "https://stackgres.io/doc/0.9/runbooks/",
	"title": "Runbooks",
	"tags": [],
	"description": "",
	"content": "Chapter 9 Runbooks This chapter contains day two operations.\n PGBadger reports  Details about how to generate a pgbadger report from the distributed logs server.\n   Volume downsize  Steps about how to perform a volume downsize\n   Restore a backup  Details about how to restore a StackGres cluster backup.\n   "
},
{
	"uri": "https://stackgres.io/doc/0.9/administration/extensions/",
	"title": "Database Extensions",
	"tags": [],
	"description": "",
	"content": "All extensions can be created with a regular CREATE EXTENSION command. To create then, connect on the database through psql then run:\npostgres=# CREATE EXTENSION pg_stat_statements; CREATE EXTENSION  Check here for more details about how to connect using kubectl.\n Checking available extensions Besides the standard PostgreSQL extensions, StackGres ships with the following extensions:\n PostGIS TimescaleDB pgbouncer_fdw pgsphere pg_repack pg_healpix pg_q3c  Check the current version on the releases page.\nTo list all available extensions, use the view pg_avaiable_extensions, like below:\npostgres=# select * from pg_available_extensions ; name | default_version | installed_version | comment --------------------+-----------------+-------------------+----------------------------------------------------------------------  ltree_plpython3u | 1.0 | | transform between ltree and plpython3u jsonb_plpythonu | 1.0 | | transform between jsonb and plpythonu xml2 | 1.1 | | XPath querying and XSLT citext | 1.6 | | data type for case-insensitive character strings dict_int | 1.0 | | text search dictionary template for integers amcheck | 1.2 | | functions for verifying relation integrity plpgsql | 1.0 | 1.0 | PL/pgSQL procedural language hstore_plpython2u | 1.0 | | transform between hstore and plpython2u pg_trgm | 1.4 | | text similarity measurement and index searching based on trigrams moddatetime | 1.0 | | functions for tracking last modification time tsm_system_time | 1.0 | | TABLESAMPLE method which accepts time in milliseconds as a limit hstore_plpython3u | 1.0 | | transform between hstore and plpython3u hstore | 1.6 | | data type for storing sets of (key, value) pairs pageinspect | 1.7 | | inspect the contents of database pages at a low level hstore_plpythonu | 1.0 | | transform between hstore and plpythonu btree_gin | 1.3 | | support for indexing common datatypes in GIN pg_buffercache | 1.3 | | examine the shared buffer cache seg | 1.3 | | data type for representing line segments or floating-point intervals file_fdw | 1.0 | | foreign-data wrapper for flat file access ltree_plpythonu | 1.0 | | transform between ltree and plpythonu pgcrypto | 1.3 | | cryptographic functions earthdistance | 1.1 | | calculate great-circle distances on the surface of the Earth lo | 1.1 | | Large Object maintenance pgstattuple | 1.5 | | show tuple-level statistics postgres_fdw | 1.0 | | foreign-data wrapper for remote PostgreSQL servers bloom | 1.0 | | bloom access method - signature file based index jsonb_plpython3u | 1.0 | | transform between jsonb and plpython3u pg_stat_statements | 1.7 | 1.7 | track execution statistics of all SQL statements executed autoinc | 1.0 | | functions for autoincrementing fields fuzzystrmatch | 1.1 | | determine similarities and distance between strings adminpack | 2.0 | | administrative functions for PostgreSQL pg_visibility | 1.2 | | examine the visibility map (VM) and page-level visibility info uuid-ossp | 1.1 | | generate universally unique identifiers (UUIDs) jsonb_plpython2u | 1.0 | | transform between jsonb and plpython2u dict_xsyn | 1.0 | | text search dictionary template for extended synonym processing unaccent | 1.1 | | text search dictionary that removes accents intarray | 1.2 | | functions, operators, and index support for 1-D arrays of integers ltree | 1.1 | | data type for hierarchical tree-like structures isn | 1.2 | | data types for international product numbering standards tablefunc | 1.0 | | functions that manipulate whole tables, including crosstab intagg | 1.1 | | integer aggregator and enumerator (obsolete) dblink | 1.2 | | connect to other PostgreSQL databases from within a database insert_username | 1.0 | | functions for tracking who changed a table tsm_system_rows | 1.0 | | TABLESAMPLE method which accepts number of rows as a limit cube | 1.4 | | data type for multidimensional cubes pgrowlocks | 1.2 | | show row-level locking information ltree_plpython2u | 1.0 | | transform between ltree and plpython2u refint | 1.0 | | functions for implementing referential integrity (obsolete) plpython3u | 1.0 | | PL/Python3U untrusted procedural language tcn | 1.0 | | Triggered change notifications btree_gist | 1.5 | | support for indexing common datatypes in GiST pg_freespacemap | 1.2 | | examine the free space map (FSM) pg_prewarm | 1.2 | | prewarm relation data (53 rows)   Checking installed extensions Execute the metacommand \\dxi to list the installed extensions:\npostgres=# \\dxi List of installed extensions Name | Version | Schema | Description --------------------+---------+------------+----------------------------------------------------------- pg_stat_statements | 1.7 | public | track execution statistics of all SQL statements executed plpgsql Custom shared_preload_libraries for extensions Both timescaledb and pg_stat_statements need a custom configuration. To fix that is necessary to add a new configuration that contains the custom value on shared_preload_libraries, like the example below:\ncat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; | kubectl create -f - apiVersion: stackgres.io/v1beta1 kind: SGPostgresConfig metadata: name: custom-conf spec: postgresVersion: \u0026#34;12\u0026#34; postgresql.conf: shared_preload_libraries: pg_stat_statements,timescaledb --- apiVersion: stackgres.io/v1beta1 kind: SGCluster metadata: name: extensions spec: instances: 2 postgresVersion: \u0026#39;latest\u0026#39; pods: persistentVolume: size: \u0026#39;1Gi\u0026#39; configurations: sgPostgresConfig: custom-conf EOF Once the config is done and the cluster is ready, you can create the extension without errors:\npostgres=# CREATE EXTENSION timescaledb CASCADE; WARNING: WELCOME TO _____ _ _ ____________ |_ _(_) | | | _ \\ ___ \\ | | _ _ __ ___ ___ ___ ___ __ _| | ___| | | | |_/ / | | | | _ ` _ \\ / _ \\/ __|/ __/ _` | |/ _ \\ | | | ___ \\ | | | | | | | | | __/\\__ \\ (_| (_| | | __/ |/ /| |_/ / |_| |_|_| |_| |_|\\___||___/\\___\\__,_|_|\\___|___/ \\____/ Running version 1.7.1 For more information on TimescaleDB, please visit the following links: 1. Getting started: https://docs.timescale.com/getting-started 2. API reference documentation: https://docs.timescale.com/api 3. How TimescaleDB is designed: https://docs.timescale.com/introduction/architecture Note: TimescaleDB collects anonymous reports to better understand and assist our users. For more information and how to disable, please see our docs https://docs.timescaledb.com/using-timescaledb/telemetry. CREATE EXTENSION "
},
{
	"uri": "https://stackgres.io/doc/0.9/administration/patroni/management/",
	"title": "Patroni Management",
	"tags": [],
	"description": "",
	"content": "Once you hace a StackGres cluster installed you\u0026rsquo;ll have a Full HA PostgreSQL configuration and depending of the size of your cluster you´ll have something like this:\nThese represents the containers of the StackGres cluster and you can list them using kubectl command like:\nkubectl get pods -n default -l app=StackGresCluster,cluster=true  Note: Change -n param to point to your namespace, in this example we use default.\n And we\u0026rsquo;ll get an output like:\nNAME READY STATUS RESTARTS AGE stackgres-0 5/5 Running 0 163m stackgres-1 5/5 Running 0 163m stackgres-2 5/5 Running 0 162m Identifying the master and replica nodes One of the most important task is to be able to identify which node is the current master and which ones the replica nodes.\nWe have two differents ways to acomplish this. The first one is with the kubectl command using the pod labels:\nTo identify the master node:\nkubectl get pods -n default -l app=StackGresCluster -l role=master output:\nNAME READY STATUS RESTARTS AGE stackgres-0 5/5 Running 0 165m To identify the replica nodes:\nkubectl get pods -n default -l app=StackGresCluster,cluster=true -l role=replica output:\nNAME READY STATUS RESTARTS AGE stackgres-1 5/5 Running 0 165m stackgres-2 5/5 Running 0 165m The other way is to use the own patroni commands. But first we need to connect to the patroni container:\nkubectl exec -it stackgres-0 -c patroni -- bash Once you are connected to it run the patroni command:\npatronictl list That will result:\n+-----------+-------------+------------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-----------+-------------+------------------+--------+---------+----+-----------+ | stackgres | stackgres-0 | 10.244.0.11:5433 | Leader | running | 2 | | | stackgres | stackgres-1 | 10.244.0.12:5433 | | running | 2 | 0.0 | | stackgres | stackgres-2 | 10.244.0.13:5433 | | running | 2 | 0.0 | +-----------+-------------+------------------+--------+---------+----+-----------+ As you can see we get the cluster status from patroni node. We can appreciate some value information here:\n Who is the master node Who are the replica nodes The ip and port The State of each node The Lag in MB in case some of the node is not up to date.  How to perform a Switchover to a replica A switchover (or graceful switchover) is a planned role reversal between the primary and the one of the standby databases. This is used when there is a planned outage on the primary database or primary server and you do not want to have extended downtime on the primary database. The switchover allows you to switch the roles of the databases so that the standby databases now becomes a primary databases and all your users and applications can continue operations on the \u0026ldquo;new\u0026rdquo; primary node.\nTo perform this we will use the patronictl switchover command:\nAs we can see in the cluster status shown before the master node is the one called stackgres-0 with the leader role and we going to switch it to the node called stackgres-1 so we run:\nbash-4.4$ patronictl switchover Then we will be asked for the master node (note that the command already give us the master node name):\nMaster [stackgres-0]: then we will be asked for the cantidate we want to promote:\nCandidate ['stackgres-1', 'stackgres-2'] []: and when we want to make the change:\nWhen should the switchover take place (e.g. 2020-01-16T17:23 ) [now]: And as a final question and warning, this show us the current status and if we want to proceed with the change:\nCurrent cluster topology +-----------+-------------+------------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-----------+-------------+------------------+--------+---------+----+-----------+ | stackgres | stackgres-0 | 10.244.0.11:5433 | Leader | running | 2 | | | stackgres | stackgres-1 | 10.244.0.12:5433 | | running | 2 | 0.0 | | stackgres | stackgres-2 | 10.244.0.13:5433 | | running | 2 | 0.0 | +-----------+-------------+------------------+--------+---------+----+-----------+ Are you sure you want to switchover cluster stackgres, demoting current master stackgres-0? [y/N]: After accept the change patroni will output the operation status and the new cluster status:\n2020-01-16 16:26:13.03648 Successfully switched over to \u0026quot;stackgres-1\u0026quot; +-----------+-------------+------------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-----------+-------------+------------------+--------+---------+----+-----------+ | stackgres | stackgres-0 | 10.244.0.11:5433 | | stopped | | unknown | | stackgres | stackgres-1 | 10.244.0.12:5433 | Leader | running | 3 | | | stackgres | stackgres-2 | 10.244.0.13:5433 | | running | | unknown | +-----------+-------------+------------------+--------+---------+----+-----------+ The old master node stackgres-0 will be stopped and then re-joined to the cluster as a replica.\nbash-4.4$ patronictl list +-----------+-------------+------------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-----------+-------------+------------------+--------+---------+----+-----------+ | stackgres | stackgres-0 | 10.244.0.11:5433 | | running | 3 | 0.0 | | stackgres | stackgres-1 | 10.244.0.12:5433 | Leader | running | 3 | | | stackgres | stackgres-2 | 10.244.0.13:5433 | | running | 3 | 0.0 | +-----------+-------------+------------------+--------+---------+----+-----------+ Important note: We strongly recommend to not manipulate the cluster with any other patronictl to avoid data lost or damage the entire configuration. Use de command explained above only if you know what are you doing.\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/monitoring/metrics/",
	"title": "Monitoring metrics",
	"tags": [],
	"description": "Contains details about the metrics automatically stored on Prometheus.",
	"content": "This page contains details about the metrics automatically stored on Prometheus. It shows the metrics exposed by StackGres through the exporters. All these metrics are used to create all the monitoring dashboards with Prometheus and Grafana directly, also acessible in the StackGres UI.\n Envoy  Contains details about the metrics collected by the envoy proxy with the Postgres filter.\n postgres_exporter  Contains details about the metrics collected by the postgres_exporter.\n node_exporter  Contains details about the metrics collected by the node_exporter.\n "
},
{
	"uri": "https://stackgres.io/doc/0.9/administration/passwords/",
	"title": "Database passwords",
	"tags": [],
	"description": "Describes how to get the auto-generated database passwords.",
	"content": "All passwords are stored by the StackGres Operator in a secret located in the same StackGres Cluster\u0026rsquo;s namespace and by convention, using the same name.\nBy default, a Stackgres Cluster initialization creates 3 users:\n superuser replication authenticator  The passwords for this users are randomly generated and stored in the stackgres cluster secret in a key=value fashion. Being the key a string in the format \u0026lt;user\u0026gt;-password and the value it\u0026rsquo;s the password itself.\nAssuming that we have a Stackgres cluster named stackgres in the namespace demo, we can get the users passwords with following commands:\n  superuser:\nkubectl get secrets -n demo stackgres -o jsonpath=\u0026#39;{.data.superuser-password}\u0026#39; | base64 -d  Note: the superuser\u0026rsquo;s password is the same as the postgres password\n   replication:\nkubectl get secrets -n demo stackgres -o jsonpath=\u0026#39;{.data.replication-password}\u0026#39; | base64 -d   authenticator:\nkubectl get secrets -n demo stackgres -o jsonpath=\u0026#39;{.data.authenticator-password}\u0026#39; | base64 -d   "
},
{
	"uri": "https://stackgres.io/doc/0.9/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": "Is StackGres a modified version of Postgres? No. StackGres contains PostgreSQL, plus several other components (such as connection pooling or automatic high availability software) from the PostgreSQL ecosystem. All of them are vanilla versions, as found in their respective open source repositories, including PostgreSQL. Any application that runs against a PostgreSQL database should work as-is if you use StackGres.\nHow is StackGres licensed? StackGres source code is licensed under the OSI-approved open source license GNU Affero General Public License version 3 (AGPLv3). All the source code is available on this repository.\nIs there a “GPL-free” commercial license for StackGres? Yes. Contact us at stackgres at ongres doc com if you want a trial or commercial license that does not contain the GPL clauses.\nWill you ever switch from an open-source license to a source-available one? No, this won’t happen. That\u0026rsquo;s our promise. We respect others who switch to or are directly built as source-available software, but we don’t follow this approach. We love the concept of GitLab’s stewardship, and in the same spirit, we promise you that StackGres will always be open source software.\nWhat PostgreSQL versions are supported? As of now, PostgreSQL major version 11 and 12. Version 13 will be added soon.\nWhere can I run StackGres? StackGres has been designed to run on any Kubernetes-certified platform. Whether that\u0026rsquo;s a Kubernetes-as-a-Service offered by a cloud provider or running on-premise, StackGres should run as-is.\nHow is HA implemented? High Availability and automatic failover are based on Patroni, a well-reputed and trusted software for PostgreSQL. No external DCS (Distributed Consistent Storage) is required, as it relies on K8s APIs for this (which, in turn, uses etcd internally).\nIs there connection pooling? Yes, we use pgbouncer. Most Postgres DBaaS solutions don\u0026rsquo;t include connection pooling as part of their managed service. Yet, in most real-life scenarios, PostgreSQL should be fronted by a connection pooler. There are many reasons for this, but the main ones are excessive memory consumption and degraded performance under too many connections — where too many can be as low as several hundreds or even a few thousand. That\u0026rsquo;s why we include it in StackGres.\nWhat “OS” are container images based on? Why not Alpine? All StackGres container images are built on the Red Hat Universal Base Image (UBI) version 8, which is derived from RHEL 8. Red Hat Universal Base Images (UBI) are OCI-compliant container base operating system images with complementary runtime languages and packages that are freely redistributable. UBI lets developers create the image once and deploy anywhere using enterprise-grade packages. For more information read the official UBI-FAQ. Alpine images are even smaller than UBI. However, they have significant disadvantages. They:\n Use musl libc, which might trigger performance and/or compatibility problems with PostgreSQL and other components of its ecosystem, including third-party extensions. Don\u0026rsquo;t have a trusted and long-term roadmap as Red Hat does. Don\u0026rsquo;t have third-party support, whereas UBI images can be supported with an existing RHEL support contract.  Does StackGres have any affiliation with Red Hat? No. We just believe UBI are the best base images for enterprise-grade containers. This way, StackGres users can get support for the container images from the most popular Linux distribution.\nCan I get support for StackGres? Yes. Contact with us.\nWill StackGres support other databases that are not PostgreSQL? No. OnGres (“On Postgres”), the company behind StackGres, is a Postgres-only shop. That’s our expertise and there’s no plan to divert from this. Databases are a very complex world, and while they may \u0026ldquo;look and feel\u0026rdquo; the same from a user perspective, they are very different from an operational perspective. We have more than two decades of experience developing and running PostgreSQL databases. Plus we believe PostgreSQL is the best relational database, anyway!\nAny other question? If you think we should add answers to other questions, please file an issue on our repository!\n"
},
{
	"uri": "https://stackgres.io/doc/0.9/administration/uninstall/",
	"title": "Uninstall",
	"tags": [],
	"description": "",
	"content": "Uninstalling StackGres clusters Database clusters Assuming that your cluster is running on the default namespace, execute the commands below to find and delete the clusters:\n## List the available clusters ❯ kubectl get sgcluster -n default NAME AGE my-db-cluster 4m27s ## List the pods for the cluster ❯ kubectl get pods NAME READY STATUS RESTARTS AGE my-db-cluster-0 5/5 Running 1 2m29s my-db-cluster-1 5/5 Running 1 99s my-db-cluster-2 5/5 Running 0 74s ## Delete the cluster ❯ kubectl delete sgcluster my-db-cluster -n default sgcluster.stackgres.io \u0026#34;my-db-cluster\u0026#34; deleted ## Check if the pods were deleted ❯ kubectl get pods -n default No resources found in default namespace. Other objects The SGCluster depends on other objects to work properly, such as instance profiles, postgres configurations, connection pooling, backup configurations, backups, distributed logs, . Execute the commands below to find and delete those objects:\n## List all sg* objects: ❯ kubectl get sgbackupconfigs,sgbackups,sgdistributedlogs,sginstanceprofiles,sgpgconfigs,sgpoolconfigs -n default NAME AGE sgbackupconfig.stackgres.io/backup-config-minio-backend 162m NAME AGE sgbackup.stackgres.io/teste 14m NAME AGE sginstanceprofile.stackgres.io/instance-profile-medium 148m sginstanceprofile.stackgres.io/instance-profile-nano 162m NAME AGE sgpostgresconfig.stackgres.io/postgres-11-generated-from-default-1609855369232 162m sgpostgresconfig.stackgres.io/postgres-11-generated-from-default-1609856085474 150m sgpostgresconfig.stackgres.io/postgres-11-generated-from-default-1609856466466 143m sgpostgresconfig.stackgres.io/postgres-11-generated-from-default-1609856836573 137m sgpostgresconfig.stackgres.io/postgres-11-generated-from-default-1609857658946 124m sgpostgresconfig.stackgres.io/postgres-11-generated-from-default-1609864032670 17m sgpostgresconfig.stackgres.io/postgres-11-generated-from-default-1609864616518 8m6s sgpostgresconfig.stackgres.io/postgres-12-generated-from-default-1609864589301 8m33s NAME AGE sgpoolingconfig.stackgres.io/generated-from-default-1609855369294 162m sgpoolingconfig.stackgres.io/generated-from-default-1609856085523 150m sgpoolingconfig.stackgres.io/generated-from-default-1609856466511 143m sgpoolingconfig.stackgres.io/generated-from-default-1609856836622 137m sgpoolingconfig.stackgres.io/generated-from-default-1609857659076 124m sgpoolingconfig.stackgres.io/generated-from-default-1609864032716 17m sgpoolingconfig.stackgres.io/generated-from-default-1609864589347 8m33s sgpoolingconfig.stackgres.io/generated-from-default-1609864616550 8m6s ## To delete them all: ## IMPORTANT: this WILL remove the backups too! ## PROCEED WITH CARE. ❯ kubectl get sgbackupconfigs,sgbackups,sgclusters,sgdistributedlogs,sginstanceprofiles,sgpgconfigs,sgpoolconfigs -n default -o name | xargs kubectl delete sgbackupconfig.stackgres.io \u0026#34;backup-config-minio-backend\u0026#34; deleted sgbackup.stackgres.io \u0026#34;teste\u0026#34; deleted sginstanceprofile.stackgres.io \u0026#34;instance-profile-medium\u0026#34; deleted sginstanceprofile.stackgres.io \u0026#34;instance-profile-nano\u0026#34; deleted sgpostgresconfig.stackgres.io \u0026#34;postgres-11-generated-from-default-1609855369232\u0026#34; deleted sgpostgresconfig.stackgres.io \u0026#34;postgres-11-generated-from-default-1609856085474\u0026#34; deleted sgpostgresconfig.stackgres.io \u0026#34;postgres-11-generated-from-default-1609856466466\u0026#34; deleted sgpostgresconfig.stackgres.io \u0026#34;postgres-11-generated-from-default-1609856836573\u0026#34; deleted sgpostgresconfig.stackgres.io \u0026#34;postgres-11-generated-from-default-1609857658946\u0026#34; deleted sgpostgresconfig.stackgres.io \u0026#34;postgres-11-generated-from-default-1609864032670\u0026#34; deleted sgpostgresconfig.stackgres.io \u0026#34;postgres-11-generated-from-default-1609864616518\u0026#34; deleted sgpostgresconfig.stackgres.io \u0026#34;postgres-12-generated-from-default-1609864589301\u0026#34; deleted sgpoolingconfig.stackgres.io \u0026#34;generated-from-default-1609855369294\u0026#34; deleted sgpoolingconfig.stackgres.io \u0026#34;generated-from-default-1609856085523\u0026#34; deleted sgpoolingconfig.stackgres.io \u0026#34;generated-from-default-1609856466511\u0026#34; deleted sgpoolingconfig.stackgres.io \u0026#34;generated-from-default-1609856836622\u0026#34; deleted sgpoolingconfig.stackgres.io \u0026#34;generated-from-default-1609857659076\u0026#34; deleted sgpoolingconfig.stackgres.io \u0026#34;generated-from-default-1609864032716\u0026#34; deleted sgpoolingconfig.stackgres.io \u0026#34;generated-from-default-1609864589347\u0026#34; deleted sgpoolingconfig.stackgres.io \u0026#34;generated-from-default-1609864616550\u0026#34; deleted Prometheus config maps The missing part is the postgres-exporter configuration that is stored as a config map:\n## To delete all: ❯ kubectl get configmap -l app=StackGresCluster -o name -n default | xargs kubectl delete configmap \u0026#34;my-db-cluster-prometheus-postgres-exporter-config\u0026#34; deleted Operator uninstall Using Helm Execute the steps below to remove the helm install:\n## locate the namespace that the operator was installed ## our doc always points to `stackgres` ❯ helm list --all-namespaces | egrep stackgres-operator\\|NAMESPACE NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION stackgres-operator stackgres 1 2021-01-05 10:55:09.543509648 -0300 -03 deployed stackgres-operator-0.9.3 0.9.3 ## Remove the operator ❯ helm delete stackgres-operator --namespace stackgres release \u0026#34;stackgres-operator\u0026#34; uninstalled ## ensure that there isn\u0026#39;t any object left on the `stackgres` namespace ❯ kubectl get all -n stackgres No resources found in stackgres namespace. Manually This tutorial expects that the operator was installed on the stackgres namespace. Change it if you have this installed in a different namespace.\nOperator deployments Execute the commands below to find and remove the operator deployments:\n## list the deployments in the `stackgres` namespace ❯ kubectl get deployments -n stackgres NAME READY UP-TO-DATE AVAILABLE AGE stackgres-operator 1/1 1 1 171m stackgres-restapi 1/1 1 1 171m ## delete the deployments in the `stackgres` namespace ❯ kubectl get deployments -n stackgres -o name | xargs kubectl delete -n stackgres deployment.apps \u0026#34;stackgres-operator\u0026#34; deleted deployment.apps \u0026#34;stackgres-restapi\u0026#34; deleted CRDs Execute the commands below to find and remove the Custom Resource Definitions (CRDs):\n## list all *.stackgres.io CRDs ❯ kubectl get crds | egrep stackgres.io\\|NAME NAME CREATED AT sgbackupconfigs.stackgres.io 2021-01-05T13:55:07Z sgbackups.stackgres.io 2021-01-05T13:55:07Z sgclusters.stackgres.io 2021-01-05T13:55:07Z sgdistributedlogs.stackgres.io 2021-01-05T13:55:07Z sginstanceprofiles.stackgres.io 2021-01-05T13:55:07Z sgpgconfigs.stackgres.io 2021-01-05T13:55:07Z sgpoolconfigs.stackgres.io 2021-01-05T13:55:07Z ## delete the CRDs ❯ kubectl get crds -o name | egrep stackgres.io | xargs kubectl delete customresourcedefinition.apiextensions.k8s.io \u0026#34;sgbackupconfigs.stackgres.io\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;sgbackups.stackgres.io\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;sgclusters.stackgres.io\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;sgdistributedlogs.stackgres.io\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;sginstanceprofiles.stackgres.io\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;sgpgconfigs.stackgres.io\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;sgpoolconfigs.stackgres.io\u0026#34; deleted Cluster Role Bindings Execute the commands below to find and remove the Custom Resource Definitions (CRDs):\n## list all ❯ kubectl get clusterrolebinding | egrep stackgres\\-\\|NAME NAME AGE stackgres-operator 3h14m stackgres-restapi 3h14m stackgres-restapi-admin 3h14m ## delete them ❯ kubectl get clusterrolebinding -o name | grep stackgres- | xargs kubectl delete clusterrolebinding.rbac.authorization.k8s.io \u0026#34;stackgres-operator\u0026#34; deleted clusterrolebinding.rbac.authorization.k8s.io \u0026#34;stackgres-restapi\u0026#34; deleted clusterrolebinding.rbac.authorization.k8s.io \u0026#34;stackgres-restapi-admin\u0026#34; deleted Cluster Roles Execute the commands below to find and remove the Custom Resource Definitions (CRDs):\n## list all ❯ kubectl get clusterrole | egrep stackgres\\-\\|NAME NAME AGE stackgres-operator 3h21m stackgres-restapi 3h21m ## delete all ❯ kubectl get clusterrole -o name | grep stackgres- | xargs kubectl delete clusterrole.rbac.authorization.k8s.io \u0026#34;stackgres-operator\u0026#34; deleted clusterrole.rbac.authorization.k8s.io \u0026#34;stackgres-restapi\u0026#34; deleted Namespaces Remove the stackgres namespace:\nkubectl delete namespace stackgres Other objects The last items to remove are the MutatingWebhookConfiguration and ValidatingWebhookConfiguration:\n## to list all ❯ kubectl get mutatingwebhookconfiguration,validatingwebhookconfiguration | egrep stackgres-\\|NAME NAME CREATED AT mutatingwebhookconfiguration.admissionregistration.k8s.io/stackgres-operator 2021-01-05T13:55:22Z NAME CREATED AT validatingwebhookconfiguration.admissionregistration.k8s.io/stackgres-operator 2021-01-05T13:55:22Z ## To remove all ❯ kubectl get mutatingwebhookconfiguration,validatingwebhookconfiguration -o name | grep stackgres- | xargs kubectl delete mutatingwebhookconfiguration.admissionregistration.k8s.io \u0026#34;stackgres-operator\u0026#34; deleted validatingwebhookconfiguration.admissionregistration.k8s.io \u0026#34;stackgres-operator\u0026#34; deleted "
},
{
	"uri": "https://stackgres.io/doc/0.9/",
	"title": "StackGres Docs",
	"tags": [],
	"description": "",
	"content": "Overview  Enterprise Postgres made easy. On Kubernetes\n StackGres is a full stack PostgreSQL distribution for Kubernetes, packed into an easy deployment unit. With a carefully selected and tuned set of surrounding PostgreSQL components.\nAn enterprise-grade PostgreSQL stack needs several other ecosystem components and significant tuning. It\u0026rsquo;s not only PostgreSQL. It requires connection pooling, automatic failover and HA, monitoring, backups and DR, centralized logging… we have built them all: a Postgres Stack.\nPostgres is not just the database. It is also all the ecosystem around it. If Postgres would be the Linux kernel, we need a PostgreSQL Distribution, surrounding PostgreSQL, to complement it with the components that are required for a production deployment. This is what we call a PostgreSQL Stack. And the stack needs to be curated. There are often several software for the same functionality. And not all is of the same quality or maturity. There are many pros and cons, and they are often not easy to evaluate. It is better to have an opinionated selection of components, that can be packaged and configured to work together in a predictable and trusted way.\nThe Operator An Operator is a method of packaging, deploying and managing a Kubernetes application. Some applications, such as databases, required more hand-holding, and a cloud-native Postgres requires an operator to provide additional knowledge of how to maintain state and integrate all the components. The StackGres operator allow to deploy a StackGres cluster using a few custom resources created by the user.\nThe Stack Curently the stack of StackGres is composed of the following components:\n PostgreSQL: The world\u0026rsquo;s most advanced open source relational database Patroni: The HA solution that relies on kubernetes distributed consensus storage to WAL-G: WAL-G is an archival restoration tool for Postgres PgBouncer: Lightweight connection pooler for PostgreSQL PostgreSQL Server Exporter: Prometheus exporter for PostgreSQL server metrics. Envoy: open source edge and service proxy, designed for cloud-native applications  "
},
{
	"uri": "https://stackgres.io/doc/0.9/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://stackgres.io/doc/0.9/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]